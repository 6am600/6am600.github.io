[{"title":"HEXO部署方案","path":"/2025/11/08/hexodeploy/","content":"hexo是一个无后端的博客系统, 不像wordpress, typecho可以直接线上编写文章, 需要在本地编写md文件, 然后编译成html上传到服务器, 整个过程还是比较繁琐, 所以, 想寻找hexo写作流的最佳方案 本地Hexo环境搭建安装必要软件 安装Git从Git官网下载并安装Git：https://git-scm.com/ 安装Node.js从Node.js官网下载并安装：https://nodejs.org/安装后验证： node -vnpm -v 安装和初始化Hexo 安装Hexo CLI npm install -g hexo-cli 初始化Hexo项目 hexo init myblogcd myblognpm install 本地测试 hexo server 访问 http://localhost:4000 查看博客效果。 项目结构说明myblog/├── _config.yml # 站点配置文件├── source/ # 文章和页面│ └── _posts/ # 文章目录├── themes/ # 主题目录├── scaffolds/ # 模板文件└── public/ # 生成的静态文件 服务器环境配置（无apt工具）由于你的服务器没有apt工具，推测可能是CentOSRHEL系系统，使用yum包管理器。 安装必要软件 安装Node.js # 添加NodeSource仓库（以Node.js 14.x为例）curl -sL https://rpm.nodesource.com/setup_14.x | sudo -E bash -# 安装Node.jssudo yum install -y nodejs 安装Git sudo yum install -y git 安装Nginx sudo yum install -y nginx 配置服务器Git仓库 创建Git用户和仓库 sudo useradd gitsudo passwd git # 设置密码su - gitmkdir -p ~/blog.gitcd ~/blog.gitgit init --bare 创建网站根目录 sudo mkdir -p /var/www/hexosudo chown git:git /var/www/hexosudo chmod 755 /var/www/hexo 配置Git钩子在 /home/git/blog.git/hooks/ 目录创建 post-receive 文件： # 切换到git用户su - git# 进入hooks目录cd /home/git/blog.git/hooks# 创建post-receive文件cat post-receive EOF#!/bin/bashecho 开始部署Hexo博客...# 设置路径GIT_DIR=/home/git/blog.gitWORK_TREE=/var/www/hexo# 创建网站目录（如果不存在）mkdir -p $WORK_TREE# 检出文件到网站目录while read oldrev newrev refnamedo branch=$(git rev-parse --symbolic --abbrev-ref $refname) if [ $branch = master ]; then echo 正在部署master分支到网站目录... git --work-tree=$WORK_TREE --git-dir=$GIT_DIR checkout -f $branch # 检查是否部署成功 if [ $? -eq 0 ]; then echo ✅ 博客部署成功！ echo 网站文件已更新到: $WORK_TREE # 显示部署的文件列表 echo 部署的文件: ls -la $WORK_TREE/ | head -10 else echo ❌ 部署失败！ exit 1 fi fidoneEOF 赋予执行权限： chmod +x /home/git/blog.git/hooks/post-receive# 将钩子文件的所有者改为git用户sudo chown git:git /home/git/blog.git/hooks/post-receive# 检查修正后的权限ls -la /home/git/blog.git/hooks/post-receive# 将整个blog.git目录的所有权交给git用户sudo chown -R git:git /home/git/blog.git# 修复网站目录权限sudo chown -R git:git /var/www/hexosudo chmod -R 755 /var/www/hexo 配置Nginx编辑 /etc/nginx/nginx.conf 或 /etc/nginx/conf.d/hexo.conf： server listen 80; server_name your-domain.com; # 替换为你的域名或IP root /var/www/hexo; index index.html; location / try_files $uri $uri/ =404; 重启Nginx： sudo systemctl start nginxsudo systemctl enable nginx 配置Hexo多平台部署配置GitHub Pages仓库 创建GitHub仓库 仓库名格式：用户名.github.io 设置为public仓库 配置SSH密钥 ssh-keygen -t rsa -C your-email@example.com 将公钥id_rsa.pub内容添加到GitHub的SSH keys中。 配置Hexo部署设置修改Hexo项目中的 _config.yml 文件： # 部署配置[citation:6]deploy: type: git repo: # 服务器仓库 - git@your-server-ip:/home/git/blog.git # GitHub仓库 - git@github.com:yourusername/yourusername.github.io.git branch: master 这里注意, github 上的主分支可能是 main, 可以直接在配置文件修改, 也可以部署好后进入github settings中修改主分支, 网上资料也比较多 安装部署插件npm install hexo-deployer-git --save 完整的写作和部署流程日常写作流程 创建新文章 hexo new 文章标题 编辑文章在 source/_posts/ 目录下找到对应的Markdown文件进行编辑。 本地预览 hexo clean hexo generate hexo server 部署到双平台 hexo clean hexo generate hexo deploy 访问直接访问 用户名.github.io 或者自己的域名都可以访问博客","tags":["HEXO","博客"],"categories":["博客"]},{"title":"【每日早报】","path":"/2025/11/06/【每日早报】/","content":"每日早报","tags":["每日早报"],"categories":["每日早报"]},{"title":"修改键盘F9为Home键","path":"/2025/10/22/changekey/","content":"修改键盘F9键功能,将其映射为HOME键 由于键盘是75键, 本身不具备HOME键, 但是日常和工作对HOME键还是挺有需要的(别问为什么不买全键的, 个人不太喜欢数字区, 对75键构造独钟) 回归正题, 如何实现 我采用的是修改注册表方式 网上能搜到比较多方案, 除了注册表, 一般都需要下载额外软件, 键盘驱动或者是windows软件来将按键功能映射为其他 所以, 修改注册表, 省时省力 修改注册表1.打开注册表 # 按下键盘上 win + r 并输入regidit 2.定位到按键修改路径 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Keyboard Layout 3.在右侧区域新建二进制值 - 命名为 Scancode Map, 并填入下列值 00 00 00 00 00 00 00 0002 00 00 00 47 E0 43 0000 00 00 00 00 00 00 00 4.点击确定, 退出注册表, 重启电脑, 永久生效👍","tags":["方法","技巧"],"categories":["效率"]},{"title":"反编译 APK","path":"/2025/03/27/decodeapk/","content":"Android逆向反编译APK工具介绍如果只是想拿到apk中的图片资源，只需要将apk后缀改为zip然后解压缩，res目录中就包含了所有的资源文件 classes.dex 则包含了所有的代码，只是还无法查看 AndroidManifest.xml 文件打开会发现无法阅读，都是16进制数 此时就需要用到工具 —— ApkTool ApkTool下载ApkTool官网 安装 使用apktool d xxx.apk d 表示 decode 还可以加上一些附加参数来控制 decode 行为： -f ：如果目标文件夹已存在，则强制删除现有文件夹（默认如果目标文件夹已存在，则解码失败） -o ：指定解码目标文件夹的名称（默认使用 APK 文件的名字来命名目标文件夹） -s ：不反编译dex文件，也就是说 classes.dex 文件会被保留（默认会将 dex 文件解码成 smali 文件） -r ：不反编译资源文件，也就是说 resources.arsc 文件会被保留（默认会将 resources.arsc 解码成具体的资源文件） 反编译之后会得到以下内容： 1、AndroidManifest.xml：经过反编译还原后的 manifest 文件 2、original 文件夹：存放了未经反编译过、原始的 AndroidManifest.xml 文件 3、res 文件夹：存放了反编译出来的所有资源 4、smali 文件夹：存放了反编译出来的所有代码，只不过格式都是.smali类型的 xml文件已经可以看懂了，不过 smali 类型文件我们依然无法阅读 此时，需要用到另一个工具 —— dex2jar + jd-gui dex2jar功能将 dex 转换成 jar 形式文件 下载dex2jar官网 使用将下载的 dex2jar 压缩包解压后，可以看到以下内容 windows上使用dex2jar.bat即可 dex2jar.bat classes.dex路径 看到上述console则表示成功 代码都位于 classes-dex2jar.jar 中 现在需要用到另一款工具 jd-gui jd-gui下载jd-gui官网 根据需要下载对应包即可 使用 解压到本地，双击jd-gui.exe文件即可运行 用jd-gui打开之前解压出来的dex文件即可看到所有的源码 jadx-gui 一个更强大的工具，一款出色的 **反编译工具 **和 代码查看器，但不能直接编辑 APK 文件或内部代码 使用 Jadx-GUI 打开一个apk文件时，它会根据 Dalvik 字节码（DEX文件）反编译成可读的 Java 源代码，然而，这些源代码只是 Jadx 根据字节码猜测出来的，并不是原始的、可变翼德Java源文件，因此，无法直接在 Jadx-GUI 中修改这些反编译出来的 Java 代码。 下载 Jadx-GUI 使用起来也很简单，打开exe文件 然后点击打开文件打开项目或者将apk文件直接拖拽过来即可查看；","tags":["Decode","安卓"],"categories":["学习","逆向工程"]},{"title":"有用的小知识","path":"/2025/03/01/justrecord/","content":"银屑病一些好用的药物他克莫司 补钙人体越越钙，吸收率越高，一般20% ~ 30%，缺钙的话可能提高到60 ~ 70，不缺钙可能会下降到10%左右（这是在正常摄入钙的情况下） 柠檬酸钙（较贵）碳酸钙（同样可以）每天补充 700mg 以上 补充氨基酸可以促进钙的吸收 所以，有复合产品 —— 氨基酸螯合钙等 钙补充多了会便秘（吸收率有限），多做运动保证吸收的钙能有效利用 多补充维生素D！！！ 最让人舒服的11种颜色RGB值和十六进制值 序号 名称 RGB 十六进制 1 豆沙绿 (199, 237, 204) #C7EDCC 2 银河白 (255, 255, 255) #FFFFFF 3 杏仁黄 (250, 249, 222) #FAF9DE 4 秋叶褐 (255, 242, 226) #FFF2E2 5 胭脂红 (253, 230, 224) #FDE6E0 6 青草绿 (227, 237, 205) #E3EDCD 7 海天蓝 (220, 226, 241) #DCE2F1 8 葛巾紫 (233, 235, 254) #E9EBFE 9 极光灰 (234, 234, 239) #EAEAEF 10 苹果绿 (183, 232, 189) #B7E8BD 11 豆沙绿-略暗 (204, 232, 207) #CCE8CF","tags":["随手一记","健康"],"categories":["生活"]},{"title":"小技巧","path":"/2024/12/21/someskills/","content":"如何一次性删除所有以某关键词结尾的文件 其他类型删除以此类推 find . -type f -name *.ko -exec rm -rf \\; Git push报错ssh: connect to host github.com port 22: Connection refused fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个错误表明 Git 无法通过 SSH（端口 22）连接到 GitHub。可能的原因和解决方法如下： 1. 检查 SSH 连接运行以下命令测试 SSH 连接： ssh -T git@github.com 如果连接被拒绝，可能是网络或防火墙问题。 2. 改用 HTTPS 协议如果 SSH 被屏蔽，可以临时改用 HTTPS： git remote set-url origin https://github.com/用户名/仓库名.gitgit push （需要输入 GitHub 账号密码或个人访问令牌） 3. 尝试 SSH 端口 443如果 22 端口被屏蔽，GitHub 也支持通过 443 端口使用 SSH。编辑 ~/.ssh/config 文件： Host github.comUser xxxxqq.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 然后再次测试 SSH 连接。 4. 检查防火墙代理设置 确保本地防火墙或公司网络未屏蔽 SSH（端口 22443） 如果使用代理，需配置 Git 使用代理：git config --global http.proxy http://代理地址:端口git config --global https.proxy https://代理地址:端口 5. 验证 SSH 密钥确保你的 SSH 密钥已添加到 GitHub： cat ~/.ssh/id_rsa.pub 然后将内容粘贴到 GitHub Settings → SSH and GPG keys。 6. 检查仓库是否存在确认远程仓库地址正确且存在： git remote -v","tags":["技巧","命令"],"categories":["效率"]},{"title":"张量索引切片操作","path":"/2024/02/21/张量索引切片操作/","content":"大致按照使用频率递减给出 这里可以给出一个结论性的规律, 便于判断张量形状, 索引操作时, 有几个 : , 就有几个维度 1. 基本下标与切片（Python 风格）import torchx = torch.arange(24).reshape(2,3,4) # shape (2,3,4)# x[batch, row, col]x[0,0,0] # 标量 tensor(0)x[0, :, :] # 第一维=0，取出 shape (3,4)x[:, 1, :] # 所有batch, 第二个row - shape (2,4)x[..., 2] # 省略号，等价于 x[:, :, 2] - shape (2,3)x[0] # 等价于 x[0, :, :] - shape (3,4) 切片语法支持 start:stop:step（含 start，不含 stop），支持负数索引和步长。 例如 x[:, ::-1, :] 会在中间维度上反转顺序（返回 view 还是 copy 取决于实现；在 PyTorch 中 negative step 会返回 copy）。 2. 使用 None np.newaxis（增加维度）y = torch.tensor([1,2,3]) # shape (3,)y[None, :] # shape (1,3)y[:, None] # shape (3,1) 常用于把向量转为列行以便广播。 3. 布尔掩码（Boolean Masking）a = torch.tensor([0,5,2,7,3])mask = a 3 # tensor([False, True, False, True, False])a[mask] # tensor([5,7]) - 1D 输出，丢失原shape信息 masked_select(a, mask) 等价于 a[mask]。 当 mask 是多维且与 a 同 shape 时，结果是扁平的 1D 张量（按行主序提取元素）。 可用于筛样本、实现 padding 掩码筛选等。 4. 花式索引（整数数组索引 Advanced Indexing）M = torch.arange(12).reshape(3,4) # shape (3,4)rows = torch.tensor([2,0])cols = torch.tensor([1,3])M[rows] # 按行选择 - shape (2,4)M[:, cols] # 按列选择 - shape (3,2)# 对应元素选择（pairwise）M[rows, cols] # 取 (2,1) 和 (0,3) - shape (2,) 若使用多个 1D 整数索引（同维度），会进行逐元素配对索引，输出长度等于索引数组长度。 若混合切片和整数数组索引，规则稍复杂：整数索引会先被应用，结果维度位置会消失或变为新维度。 例（多维）： T = torch.arange(2*3*4).reshape(2,3,4)idx0 = torch.tensor([0,1]) # 用于第0维idx1 = torch.tensor([2,0]) # 用于第1维T[idx0, idx1] # 逐对索引 - shape (2,4)# 等价于: torch.stack([T[0,2], T[1,0]], dim=0) 5. 广播与索引（注意形状）A = torch.arange(6).reshape(2,3) # (2,3)idx = torch.tensor([0,2]) # (2,)A[torch.arange(2), idx] # (2,) - 每 batch 对应列取值# torch.arange(2) 为 [0,1]，与 idx 配对 - 取 (0,0) 和 (1,2) 常用于按-batch 选择每个样本对应的索引（如分类预测的 top-k 判断）。 6. gather 与 scatter（按索引收集写入，适用于高维批量操作）# gather 示例：从 src 中按 index 收集（需要指定 dim）src = torch.tensor([[10,11,12],[20,21,22]]) # shape (2,3)index = torch.tensor([[2,1,0],[0,2,1]]) # shape (2,3)torch.gather(src, dim=1, index=index)# - shape (2,3): [[12,11,10],[20,22,21]] gather 要求 index 与 src 在除了 dim 外的维度完全相同；返回与 index 同形状的张量。 常用于实现按位置取值（例如 beam-search、按预测索引从概率张量中取值）。 scatter_scatter 用于把值写入指定位置，可做 one-hot 化或累积（有 reduce 参数）。 7. index_select take（按维度选择）v = torch.tensor([10,20,30,40])torch.index_select(v, dim=0, index=torch.tensor([3,1])) # tensor([40,20])# 对于矩阵按行选：M = torch.arange(12).reshape(3,4)torch.index_select(M, dim=0, index=torch.tensor([2,0])) # shape (2,4) index_select 返回的顺序与索引一致；与 fancy indexing（M[idx]）相似，但有些后端实现行为细微不同（比如保留 contiguous 性）。 8. masked_fill, where（掩码赋值 条件选择）x = torch.tensor([1., -2., 3.])x.masked_fill(x 0, 0.) # 把负数置0torch.where(x0, x, torch.zeros_like(x)) # 条件选择，相当于 np.where where(cond, A, B) 返回与 AB 广播后的形状相同的张量。 9. unsqueeze squeeze 与 view/reshape（维度控制）a = torch.tensor([1,2,3]) # (3,)a.unsqueeze(0) # (1,3)a.unsqueeze(1) # (3,1)torch.squeeze(a.unsqueeze(0)) # 恢复 squeeze(dim) 只在指定维度为 1 时删除该维度。 reshapeview 会改变内存视图（view 要求连续 contiguous；reshape 在必要时会复制）。 10. Ellipsis ...（省略号）X = torch.randn(4,5,6,7)X[..., 0] # 等价 X[:, :, :, 0]X[0, ...] # 等价 X[0, :, :, :] 在不确定前面后面维度数时非常有用，特别是在写通用层时。 11. 多维返回与维度插入（保持丢失维度） 使用整数索引会减少维度（那一维被消除）； 使用切片或 None，或保持长度为 1 的索引会保留维度。 示例： t = torch.randn(2,3,4)t[0].shape # (3,4) -- 整数索引去掉第0维t[0:1].shape # (1,3,4) -- 切片保留第0维t[[0]].shape # (1,3,4) -- 用长度1的索引数组也保留 12. 视图（view）与 copy（内存contiguous）相关注意 大多数简单切片和整型索引会返回原张量的 view（共享内存），但有些操作会返回 copy（例如带负步长的切片、某些高级索引）。 is_contiguous() 可以检查是否连续。若对返回的张量执行 view() 可能会报错，需先 .contiguous()。 s = torch.arange(6).reshape(2,3)t = s[:, ::-1] # 可能是 copy（不连续）t.is_contiguous() # 可能 Falset.contiguous().view(-1) # 安全 在 in-place 操作（如 t += 1）时，如果 t 与原张量共享内存，可能会影响原张量；对 copy 则无影响。 13. 反向传播（autograd）相关 索引、切片会保留计算图信息（如果原张量 requires_grad=True），因此从张量中取出的部分仍可对原张量反向传播。 但是，用高级索引赋值（x[idx] = something）不记录梯度；需要使用 scatter 或构造新的张量再计算 loss。 detach() 可以切断梯度传播（例如 x = x.detach()）。 示例（反向传播影响）： x = torch.randn(3, requires_grad=True)y = x[1] * 2y.backward() # 会为 x[1] 累积梯度，但 x[0], x[2] 为 0x.grad # tensor([0., 2., 0.]) 14. 常见用途与模式（实战片段） 按 batch 取样（分类概率取预测值）： probs = torch.randn(32, 10) # logits or probspred = probs.argmax(dim=1) # (32,)# 如果想从 probs 中收集每个 batch 对应预测的概率：selected = probs[torch.arange(32), pred] # shape (32,) padding mask（seq 长短不一）： seq = torch.randint(0, 100, (4,7)) # batch, seq_lenmask = (seq != PAD_TOKEN) # True 表示有效# 通过 mask 做池化：masked_sum = (embeddings * mask.unsqueeze(-1)).sum(dim=1) one-hot： idx = torch.tensor([0,2,1])onehot = torch.nn.functional.one_hot(idx, num_classes=4) # shape (3,4) 按索引更新参数（embedding lookup 与更新）： embedding = torch.nn.Embedding(num_embeddings=1000, embedding_dim=64)out = embedding(idx_tensor) # embedding 内部就是高级索引/ gather 实现 15. 进阶：einsum 作为灵活替代（当索引和 reshape 太繁琐） einsum 可以在一次表达式中完成复杂的通道维度重新排列与约简，可替代多个 transpose + matmul 操作。 # 例：批量矩阵乘积 sum over k: c_ij = sum_k a_ik b_jktorch.einsum(ik,jk-ij, a, b) 16. 常见陷阱与建议 整数数组索引通常会返回 copy（非 view） —— 这会影响内存并且后续 in-place 修改不会影响原张量。 带负步长的切片常常产生 copy，要注意 is_contiguous()。 混合使用布尔掩码与维度不适配会报错，确保 mask 与被掩的张量形状一致或能广播。 不要对需要 autograd 的部分用原地替换（x[idx] …），会破坏计算图，使用 scatterscatter_add 或构造新张量。 索引返回的张量可能会改变梯度分配，只有被实际用到（参与 loss）的元素才会有梯度。 尽量用 gather 实现批量按位置索引（可保持形状且直观），而不是复杂的循环。 17. 一张速查小表（常用 API） 基本：x[i], x[:, j], x[..., k], x[start:stop:step] 维度：unsqueeze, squeeze, transpose, permute, reshape, view, contiguous 选择花式：index_select, take, gather, scatter, where, masked_select, masked_fill 布尔掩码：x[mask], torch.where, torch.nonzero 其他：one_hot, topk, argmax/argmin, torch.arange（用于构建 batch 索引） 如果你愿意，我可以： 把这些示例做成一个可以直接运行的 Jupyter notebook（包含针对常见错误的测试用例），或者 根据你常用的框架（PyTorch TensorFlow NumPy）把示例改写成你习惯的 API，或 针对你当前代码中的索引问题（把你代码贴来），帮你找 bug 并修正。 你想要哪种后续帮助？","tags":["人工智能","张量","运算","Pytorch"],"categories":["大模型"]},{"title":"好用的工具","path":"/2024/01/10/nicetools/","content":"VPNEFCloud —— 跑路了，八嘎 一元机场 —— 非常稳定，几年了还在，就是节点很不稳定 特价机场 —— 便宜好用 ICON阿里巴巴矢量图标库 FONT AWESOME Carbon 封面图生成https://nav.rdonly.com/laboratory/bgimage/backimage.html https://cover.ruom.top/ 在线图片压缩1️⃣ TinyPNG 支持 PNG JPG WebP 一次可上传 20 张（单张 ≤ 5MB） 平均可减少 60–80% 体积 保留高画质，肉眼几乎无损 操作步骤： 打开网站 拖入图片 下载压缩后的版本 👉 适合 博客封面、展示图、LOGO 等日常用途。 2️⃣ ILoveIMG 支持批量上传 同时压缩 JPG PNG GIF 提供在线编辑（裁剪、加水印等）","tags":["工具"],"categories":["效率"]},{"title":"追星Transformer","path":"/wiki/note/index.html","content":"Transformer模型1 Transformer背景模型被提出时间： 2017年提出，2018年google发表了BERT模型，使得Transformer架构流行起来，BERT在许多NLP任务上，取得了Soat的成就。 模型优势： 1、能够实现并行计算，提高模型训练效率2、更好的特征提取能力 2 Transformer的模型架构架构图展示： 2.1 整体架构主要组成部分 1、输入部分2、编码器部分3、解码器部分4、输出部分 2.2 输入部分word Embeddding + Positional Encoding词嵌入层+位置编码器层 2.3 输出部分1、Linear层2、softmax层 2.4 编码器部分结构图： 组成部分： 1、N个编码器层堆叠而成2、每个编码器有两个子层连接结构构成3、第一个子层连接结构：多头自注意力层+规范化层+残差连接层4、第二个子层连接结构：前馈全连接层+规范化层+残差连接层"},{"title":"权重衰减","path":"/wiki/note/weight_decay.html","content":"权重衰减原理通过L2正则化防止过拟合","tags":[null,null],"categories":[null]},{"title":"位置编码","path":"/wiki/note/posencoding.html","content":"首先抛出一个问题, 为什么Transformer 要有位置编码? – 对于任何一门语言, 单词在句子中的位置和顺序都极为重要, 语序正确, 才能表达出正确含义 I do not like the story of the movie, but I do like the cast.I do like the story of the movie, but I do not like the cast. 如上述两个句子, 仅是改变not位置, 表达的意思截然相反 – Transformer抛弃了RNN和CNN作为序列学习的基本模型, 我们知道, 循环神经网络本身就是一种顺序结构, 天然包含了词在序列中的位置信息, 当抛弃循环神经网络结构, 完全采用Attention取而代之, 这些词序信息就会丢失, 模型就没有办法直到每个词在句子中的相对位置和绝对位置, 因此, 有必要把词序信息加到词向量上帮助模型学习这些信息, 位置编码(Positional Encoding)就是用来解决这个问题的办法 一、什么是位置编码 在transformer的encoder和decoder中使用了Positional Encoding, 最终的输入就变为了: input word_embedding + positional_encoding word_embedding : 词嵌入, 将token的维度从vocab_size映射到d_model(原论文中, d_model为512) 最终的输入通过词嵌入后的向量和位置编码矩阵相加得到, 所以positional_encoding也是d_model维度的向量 二、位置编码构造方法2.1 用增长数值标记位置 第一个token标记1, 第二个token标记2… 以此类推 这种方法存在一些问题: 模型可能遇见比训练时所用序列更长的序列, 不利于模型泛化 模型无法理解数字的含义, 可能认为大数字比小数字权重更高, 而非顺序更靠后 无法表达相对位置信息 2.2 用[0, 1]范围标记位置为解决上述问题, 将数值限制在[0, 1]区间内, 对其等分切割, 假设有3个token, 则位置信息为[0, 0.5, 1] 这样产生的问题是, 序列长度不同, 相对距离也不同, 模型可能认为这是单词语义发生了变化 2.3 用二进制向量标记位置 由于位置信息最终会作用到word_embedding上, 比起用单一的数值, 更好的方式是使用和word_embedding同纬度的向量, 可以想到, 将数值转换成二进制形式 假设d_model 3 d_model一般比较大(512, 1024…), 基本可以把每个token的位置都编码出来 但这样也存在问题, 不同单词间的位置变化不连续, 难以推测相对位置信息 2.4 用sin和cos函数交替标记位置 这是论文中提出的方法 三角函数有界且连续, 可以满足目前的需要 $$PE_{(pos, 2i)} \\sin\\left(\\frac{pos}{10000^{2id_{model}}}\\right)$$ $$PE_{(pos, 2i+1)} \\cos\\left(\\frac{pos}{10000^{2id_{model}}}\\right)$$ 参数说明: 符号 含义 pos 当前位置（position index） i 维度索引（dimension index） d_model 模型维度（embedding size，例如512） PE 位置编码矩阵（shape: [seq_len, d_model]） 这里只给出计算公式, 数学推导网上优秀文章较多, 不再演示 三. 为什么可以直接将词向量和位置编码进行相加 这是一直困扰我的点, 仅是进行张量相加就可以达到添加位置信息的作用吗? 难道这样不会破坏词向量本身信息吗? 网上浏览了一些博主的文章和视频, 但是答案也并没那么明确, 有点大力出奇迹的意思 3.1、公式复习Transformer 的输入是：$$X E_{\\text{word}} + E_{\\text{pos}}$$其中： $E_{word}$：词向量（表示词语语义） $E_{pos}$：位置编码（表示词语在句子中的位置） 维度相同（例如都为 $d_{model}512$） 3.2、为什么相加就能引入位置信息？1. 向量加法在嵌入空间中表示“组合信息”在向量空间中，加法表示叠加不同的语义因素。 比如： “king” ≈ “man” + “royalty” “Paris” - “France” + “Italy” ≈ “Rome” 同理： 把词向量和位置向量相加，就相当于在语义中叠加“第几个词”的信息。 也就是说，新的向量同时携带了词义和位置两种特征。 2. 相加不会“覆盖”信息，而是“偏移”语义空间我们可以把每个词向量看成在一个高维语义空间中的点。 加上位置编码后，等价于把它 沿着“位置维度方向”平移了一点： $$\\text{new_embedding semantic_embedding + position_offset}$$ 所以模型看到的不是“词义被破坏”，而是“相同词在不同位置处于略微不同的方向”。 这使得 Transformer 能区分： “I love you”和“You love I” 的区别。 3. 注意力机制会自动学会利用这些位置差异在自注意力计算中，Q、K 向量都会从输入中线性变换得到： $$Q XW_Q, \\quad K XW_K$$因此，位置信息通过 $E_{pos}$ 影响了 Query-Key 相似度。 当两个词位置不同、位置编码不同，它们的注意力权重分布也不同 → 模型能够学习“前后关系”。 3.3、为什么不会破坏词向量原始信息？1. 相加是线性可逆的（在一定程度上）如果两个向量空间的分布相对独立，线性相加仍能被后续层（线性变换）分离： $$W(E_{word}+E_{pos})WE_{word}+WE_{pos}$$Transformer 的多层线性结构可以在后续层中重新分辨“语义部分”和“位置信息部分”。 2. 位置编码的幅度较小，不会淹没词向量在实现时，位置编码通常被归一化或初始化为较小值（例如在 [-1,1] 范围内）。 这样词向量的主导语义仍然保留，只是轻微偏移 → 注入位置信息。 3. 高维空间中的“信息容量”巨大在 512 维或更高的空间中，向量叠加后不会导致严重的信息重叠。 就像 RGB 图像里混合红色和绿色还能区分出“黄色”一样， 词义与位置信息叠加后仍可被网络区分和提取。","tags":[null,null],"categories":[null]}]