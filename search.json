[{"title":"GitHub 每周热门项目 (2025.11.17 - 2025.11.23)","path":"/2025/11/19/github-weekly/","content":"🚀 本文自动生成，数据来源于 OSSInsight 与 GitHub 官方 API，每周自动更新。 1. straight-tamagomisaka26 👤 作者: straight-tamago🌍 语言: NA⭐ Stars: 1565 iOS iPadOS 16.0 - 26.1, An ultimate customization tool, uilitizing the bug that makes TrollRestore possible. 2. karpathyreader3 👤 作者: karpathy🌍 语言: Python⭐ Stars: 879 Quick illustration of how one can easily read books together with LLMs. It’s great and I highly recommend it. 3. AnandChowdharycontinuous-claude 👤 作者: AnandChowdhary🌍 语言: Shell⭐ Stars: 395 🔂 Run Claude Code in a continuous loop, autonomously creating PRs, waiting for checks, and merging 4. HaleclipseClaudix 👤 作者: Haleclipse🌍 语言: TypeScript⭐ Stars: 380 Gorgeous Claude Code Extend for VS Code. 5. medusalixFreeMDU 👤 作者: medusalix🌍 语言: Rust⭐ Stars: 363 Open hardware and software tools for communicating with Miele appliances via their optical diagnostic interface 6. K-Dense-AIkarpathy 👤 作者: K-Dense-AI🌍 语言: Python⭐ Stars: 355 An agentic Machine Learning Engineer 7. buyukakyuzinstall-nothing 👤 作者: buyukakyuz🌍 语言: Rust⭐ Stars: 308 A terminal application that simulates installing things but doesn’t actually install anything 8. zigbookzigbook 👤 作者: zigbook🌍 语言: Zig⭐ Stars: 259 A comprehensive guide to the Zig programming language, a journey from fundamentals to advanced systems programming. 9. modelscopeAgentEvolver 👤 作者: modelscope🌍 语言: Python⭐ Stars: 259 AgentEvolver: Towards Efficient Self-Evolving Agent System 10. steipeteoracle 👤 作者: steipete🌍 语言: TypeScript⭐ Stars: 242 Ask the oracle when you’re stuck. Invoke GPT-5 Pro with a custom context and files."},{"title":"反编译 APK","path":"/2025/03/27/decodeapk/","content":"Android逆向反编译APK工具介绍如果只是想拿到apk中的图片资源，只需要将apk后缀改为zip然后解压缩，res目录中就包含了所有的资源文件 classes.dex 则包含了所有的代码，只是还无法查看 AndroidManifest.xml 文件打开会发现无法阅读，都是16进制数 此时就需要用到工具 —— ApkTool ApkTool下载ApkTool官网 安装 使用apktool d xxx.apk d 表示 decode 还可以加上一些附加参数来控制 decode 行为： -f ：如果目标文件夹已存在，则强制删除现有文件夹（默认如果目标文件夹已存在，则解码失败） -o ：指定解码目标文件夹的名称（默认使用 APK 文件的名字来命名目标文件夹） -s ：不反编译dex文件，也就是说 classes.dex 文件会被保留（默认会将 dex 文件解码成 smali 文件） -r ：不反编译资源文件，也就是说 resources.arsc 文件会被保留（默认会将 resources.arsc 解码成具体的资源文件） 反编译之后会得到以下内容： 1、AndroidManifest.xml：经过反编译还原后的 manifest 文件 2、original 文件夹：存放了未经反编译过、原始的 AndroidManifest.xml 文件 3、res 文件夹：存放了反编译出来的所有资源 4、smali 文件夹：存放了反编译出来的所有代码，只不过格式都是.smali类型的 xml文件已经可以看懂了，不过 smali 类型文件我们依然无法阅读 此时，需要用到另一个工具 —— dex2jar + jd-gui dex2jar功能将 dex 转换成 jar 形式文件 下载dex2jar官网 使用将下载的 dex2jar 压缩包解压后，可以看到以下内容 windows上使用dex2jar.bat即可 dex2jar.bat classes.dex路径 看到上述console则表示成功 代码都位于 classes-dex2jar.jar 中 现在需要用到另一款工具 jd-gui jd-gui下载jd-gui官网 根据需要下载对应包即可 使用 解压到本地，双击jd-gui.exe文件即可运行 用jd-gui打开之前解压出来的dex文件即可看到所有的源码 jadx-gui 一个更强大的工具，一款出色的 **反编译工具 **和 代码查看器，但不能直接编辑 APK 文件或内部代码 使用 Jadx-GUI 打开一个apk文件时，它会根据 Dalvik 字节码（DEX文件）反编译成可读的 Java 源代码，然而，这些源代码只是 Jadx 根据字节码猜测出来的，并不是原始的、可变翼德Java源文件，因此，无法直接在 Jadx-GUI 中修改这些反编译出来的 Java 代码。 下载 Jadx-GUI 使用起来也很简单，打开exe文件 然后点击打开文件打开项目或者将apk文件直接拖拽过来即可查看；","tags":["Decode","安卓"],"categories":["学习","逆向工程"]},{"title":"权重衰减","path":"/2025/03/01/weightdecay/","content":"","tags":["AdamW"],"categories":["优化"]},{"title":"命名实体识别","path":"/2024/08/01/ner/","content":"","tags":["NER","NLP"],"categories":["数据处理"]},{"title":"完整front-matter字段","path":"/2024/05/21/front_matter/","content":"Hexo 内置字段 参数 描述 默认值 layout 布局 config.default_layout title 标题 文章的文件名 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 sticky 置顶（数字越大越靠前） comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章的永久链接，永久链接应该以 / 或 .html 结尾 null excerpt 纯文本的页面摘要。使用 该插件 来格式化文本 disableNunjucks 启用时禁用 Nunjucks 标签 % % 和 标签插件 的渲染功能 false lang 设置语言以覆盖 自动检测 继承自 _config.yml published 文章是否发布 对于 _posts 下的文章为 true，对于 _draft 下的文章为 false 详见：https://hexo.io/zh-cn/docs/front-matter Stellar 特有字段 参数 描述 默认值 可用版本 wiki 该页面所属的 wiki 项目 menu_id 高亮的菜单按钮 id sidebar 侧边栏配置 1.0.0 ~ 1.26.8 leftbar 左侧边栏配置 1.27.0 ~ rightbar 右侧边栏配置 1.27.0 ~ comment_title 评论区标题 poster 文章封面，包含 topicheadlinecaptioncolor 子配置 banner 页面顶部横幅背景 banner_info 横幅信息，包含 avatartitlesubtitle 子配置 logo 左侧边栏顶部 logo 区域信息，包含 iconavatartitlesubtitle 子配置 indent 段落是否缩进 false topic 所属话题专栏 1.25.0 ~ author 该文章的作者 1.23.0 ~ type 页面类型 1.26.0 ~ references 参考资料 h1 页内标题 title 1.26.0 ~ breadcrumb 面包屑导航 true indexing 页面能否是否能被搜索 true 第三方插件 参数 描述 类型 mathjax 渲染文章公式 boolean katex 同上 boolean mermaid 渲染图表 boolean","tags":["HEXO","博客","Stellar"],"categories":["博客"]},{"title":"模型蒸馏","path":"/2024/04/26/textcnn/","content":"TextCNN中的卷积操作卷积神经网络的核心思想是 捕捉局部特征，对于文本来说，局部特征就是 由若干单词组成的滑动窗口，类似 N-gram。卷积神经网络的优势在于能够自动的对 N-gram特征进行组合和筛选，获取不同抽象层次的语义信息 在图像中，这些局部特征可以是 边缘、纹理、形状 在文本中，这些局部特征可以是： ​\t情感短语：“太中了！“，“俺不中嘞” ​\t关键词组合：“股市 下跌”，“疫情 爆发” ​\t…… 句子可以看成由词向量组成的二维矩阵 $$X∈R^{n×d}$$ n 表示句子长度 d 表示词向量维度 每一行就是一个词的embedding 卷积核可以在这个矩阵上滑动，自动学习“哪些词组合表示什么语义”，从而达到文本分类目的 TextCNN的核心结构1. Embedding 层输入句子经过 embedding lookup 变成矩阵，词向量可以是： 随机初始化 预训练词向量（word2vec、GloVe、fastText） Transformer 输出的上下文向量（更高级） 2. 多尺寸卷积核提取 n-gram 特征不同高度的卷积核对应不同 n-gram： 宽度 embedding 维度（固定） 高度 n-gram 大小，如 2、3、4 例如，3×d 的卷积核可以捕捉 “三词短语” 模式。 卷积运算公式如下： $$c_i f(W * X_{i:i+h-1} + b)$$ 得到长度为 $$n−h+1$$ 的 feature map。 3. 最大池化（max-over-time pooling）对每个 feature map 取最大值 含义：保留该卷积核在整个句子中最强的激活，代表最重要的 n-gram 模式。 4. 全连接层 + Softmax 分类多个卷积核的池化结果拼接成向量，再输入全连接层实现分类。 TextCNN的超参数调参 参数名称 参数值 输入词向量 word2vec filter大小 (3,4,5) 每个size下的filter个数 100 激活函数 ReLU 池化策略 1-max pooling dropout rate 0.5 L2正则化 3 输入词向量表征：词向量表征的选取(如选word2vec还是GloVe) 卷积核大小：一个合理的值范围在1~10。若语料中的句子较长，可以考虑使用更大的卷积核。另外，可以在寻找到了最佳的单个filter的大小后，尝试在该filter的尺寸值附近寻找其他合适值来进行组合。实践证明这样的组合效果往往比单个最佳filter表现更出色 feature map特征图个数：主要考虑的是当增加特征图个数时，训练时间也会加长，因此需要权衡好。当特征图数量增加到将性能降低时，可以加强正则化效果，如将dropout率提高过0.5 激活函数：ReLU和tanh是最佳候选者 池化策略：1-max pooling表现最佳 正则化项(dropoutL2)：相对于其他超参数来说，影响较小点 使用Pytorch简单实现import torchimport torch.nn as nnimport torch.nn.functional as Fimport os​class Model(nn.Module): def __init__(self, config): super(Model, self).__init__() self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1) # 词嵌入层 self.convs = nn.ModuleList( [nn.Conv2d(1, config.num_filters, (k, config.embed)) for k in config.filter_sizes] ) # 卷积层列表，包含不同卷积核大小的卷积层 self.dropout = nn.Dropout(config.dropout) # 随机失活层 self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes) # 全连接层​ def conv_and_pool(self, x, conv): # 卷积和池化操作 x = F.relu(conv(x)).squeeze(3) x = F.max_pool1d(x, x.size(2)).squeeze(2) return x​ def forward(self, x): # 前向传播 out = self.embedding(x[0]) out = out.unsqueeze(1) # 对每个卷积层进行卷积和池化操作，然后拼接在一起 out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1) out = self.dropout(out) # 随机失活 out = self.fc(out) # 全连接层 return out","tags":["模型","CNN","文本分类"],"categories":["深度学习"]},{"title":"模型蒸馏","path":"/2024/04/26/modeldistill/","content":"","tags":["优化","Distill"],"categories":["深度学习","模型压缩"]},{"title":"有用的小知识","path":"/2024/03/01/justrecord/","content":"银屑病一些好用的药物他克莫司 补钙人体越越钙，吸收率越高，一般20% ~ 30%，缺钙的话可能提高到60 ~ 70，不缺钙可能会下降到10%左右（这是在正常摄入钙的情况下） 柠檬酸钙（较贵）碳酸钙（同样可以）每天补充 700mg 以上 补充氨基酸可以促进钙的吸收 所以，有复合产品 —— 氨基酸螯合钙等 钙补充多了会便秘（吸收率有限），多做运动保证吸收的钙能有效利用 多补充维生素D！！！ 最让人舒服的11种颜色RGB值和十六进制值 序号 名称 RGB 十六进制 1 豆沙绿 (199, 237, 204) #C7EDCC 2 银河白 (255, 255, 255) #FFFFFF 3 杏仁黄 (250, 249, 222) #FAF9DE 4 秋叶褐 (255, 242, 226) #FFF2E2 5 胭脂红 (253, 230, 224) #FDE6E0 6 青草绿 (227, 237, 205) #E3EDCD 7 海天蓝 (220, 226, 241) #DCE2F1 8 葛巾紫 (233, 235, 254) #E9EBFE 9 极光灰 (234, 234, 239) #EAEAEF 10 苹果绿 (183, 232, 189) #B7E8BD 11 豆沙绿-略暗 (204, 232, 207) #CCE8CF","tags":["随手一记","健康"],"categories":["生活"]},{"title":"张量索引切片操作","path":"/2024/02/21/tensor/","content":"大致按照使用频率递减给出 这里可以给出一个结论性的规律, 便于判断张量形状, 索引操作时, 有几个 : , 就有几个维度 1. 基本下标与切片（Python 风格）import torchx = torch.arange(24).reshape(2,3,4) # shape (2,3,4)# x[batch, row, col]x[0,0,0] # 标量 tensor(0)x[0, :, :] # 第一维=0，取出 shape (3,4)x[:, 1, :] # 所有batch, 第二个row - shape (2,4)x[..., 2] # 省略号，等价于 x[:, :, 2] - shape (2,3)x[0] # 等价于 x[0, :, :] - shape (3,4) 切片语法支持 start:stop:step（含 start，不含 stop），支持负数索引和步长。 例如 x[:, ::-1, :] 会在中间维度上反转顺序（返回 view 还是 copy 取决于实现；在 PyTorch 中 negative step 会返回 copy）。 2. 使用 None np.newaxis（增加维度）y = torch.tensor([1,2,3]) # shape (3,)y[None, :] # shape (1,3)y[:, None] # shape (3,1) 常用于把向量转为列行以便广播。 3. 布尔掩码（Boolean Masking）a = torch.tensor([0,5,2,7,3])mask = a 3 # tensor([False, True, False, True, False])a[mask] # tensor([5,7]) - 1D 输出，丢失原shape信息 masked_select(a, mask) 等价于 a[mask]。 当 mask 是多维且与 a 同 shape 时，结果是扁平的 1D 张量（按行主序提取元素）。 可用于筛样本、实现 padding 掩码筛选等。 4. 花式索引（整数数组索引 Advanced Indexing）M = torch.arange(12).reshape(3,4) # shape (3,4)rows = torch.tensor([2,0])cols = torch.tensor([1,3])M[rows] # 按行选择 - shape (2,4)M[:, cols] # 按列选择 - shape (3,2)# 对应元素选择（pairwise）M[rows, cols] # 取 (2,1) 和 (0,3) - shape (2,) 若使用多个 1D 整数索引（同维度），会进行逐元素配对索引，输出长度等于索引数组长度。 若混合切片和整数数组索引，规则稍复杂：整数索引会先被应用，结果维度位置会消失或变为新维度。 例（多维）： T = torch.arange(2*3*4).reshape(2,3,4)idx0 = torch.tensor([0,1]) # 用于第0维idx1 = torch.tensor([2,0]) # 用于第1维T[idx0, idx1] # 逐对索引 - shape (2,4)# 等价于: torch.stack([T[0,2], T[1,0]], dim=0) 5. 广播与索引（注意形状）A = torch.arange(6).reshape(2,3) # (2,3)idx = torch.tensor([0,2]) # (2,)A[torch.arange(2), idx] # (2,) - 每 batch 对应列取值# torch.arange(2) 为 [0,1]，与 idx 配对 - 取 (0,0) 和 (1,2) 常用于按-batch 选择每个样本对应的索引（如分类预测的 top-k 判断）。 6. gather 与 scatter（按索引收集写入，适用于高维批量操作）# gather 示例：从 src 中按 index 收集（需要指定 dim）src = torch.tensor([[10,11,12],[20,21,22]]) # shape (2,3)index = torch.tensor([[2,1,0],[0,2,1]]) # shape (2,3)torch.gather(src, dim=1, index=index)# - shape (2,3): [[12,11,10],[20,22,21]] gather 要求 index 与 src 在除了 dim 外的维度完全相同；返回与 index 同形状的张量。 常用于实现按位置取值（例如 beam-search、按预测索引从概率张量中取值）。 scatter_scatter 用于把值写入指定位置，可做 one-hot 化或累积（有 reduce 参数）。 7. index_select take（按维度选择）v = torch.tensor([10,20,30,40])torch.index_select(v, dim=0, index=torch.tensor([3,1])) # tensor([40,20])# 对于矩阵按行选：M = torch.arange(12).reshape(3,4)torch.index_select(M, dim=0, index=torch.tensor([2,0])) # shape (2,4) index_select 返回的顺序与索引一致；与 fancy indexing（M[idx]）相似，但有些后端实现行为细微不同（比如保留 contiguous 性）。 8. masked_fill, where（掩码赋值 条件选择）x = torch.tensor([1., -2., 3.])x.masked_fill(x 0, 0.) # 把负数置0torch.where(x0, x, torch.zeros_like(x)) # 条件选择，相当于 np.where where(cond, A, B) 返回与 AB 广播后的形状相同的张量。 9. unsqueeze squeeze 与 view/reshape（维度控制）a = torch.tensor([1,2,3]) # (3,)a.unsqueeze(0) # (1,3)a.unsqueeze(1) # (3,1)torch.squeeze(a.unsqueeze(0)) # 恢复 squeeze(dim) 只在指定维度为 1 时删除该维度。 reshapeview 会改变内存视图（view 要求连续 contiguous；reshape 在必要时会复制）。 10. Ellipsis ...（省略号）X = torch.randn(4,5,6,7)X[..., 0] # 等价 X[:, :, :, 0]X[0, ...] # 等价 X[0, :, :, :] 在不确定前面后面维度数时非常有用，特别是在写通用层时。 11. 多维返回与维度插入（保持丢失维度） 使用整数索引会减少维度（那一维被消除）； 使用切片或 None，或保持长度为 1 的索引会保留维度。 示例： t = torch.randn(2,3,4)t[0].shape # (3,4) -- 整数索引去掉第0维t[0:1].shape # (1,3,4) -- 切片保留第0维t[[0]].shape # (1,3,4) -- 用长度1的索引数组也保留 12. 视图（view）与 copy（内存contiguous）相关注意 大多数简单切片和整型索引会返回原张量的 view（共享内存），但有些操作会返回 copy（例如带负步长的切片、某些高级索引）。 is_contiguous() 可以检查是否连续。若对返回的张量执行 view() 可能会报错，需先 .contiguous()。 s = torch.arange(6).reshape(2,3)t = s[:, ::-1] # 可能是 copy（不连续）t.is_contiguous() # 可能 Falset.contiguous().view(-1) # 安全 在 in-place 操作（如 t += 1）时，如果 t 与原张量共享内存，可能会影响原张量；对 copy 则无影响。 13. 反向传播（autograd）相关 索引、切片会保留计算图信息（如果原张量 requires_grad=True），因此从张量中取出的部分仍可对原张量反向传播。 但是，用高级索引赋值（x[idx] = something）不记录梯度；需要使用 scatter 或构造新的张量再计算 loss。 detach() 可以切断梯度传播（例如 x = x.detach()）。 示例（反向传播影响）： x = torch.randn(3, requires_grad=True)y = x[1] * 2y.backward() # 会为 x[1] 累积梯度，但 x[0], x[2] 为 0x.grad # tensor([0., 2., 0.]) 14. 常见用途与模式（实战片段） 按 batch 取样（分类概率取预测值）： probs = torch.randn(32, 10) # logits or probspred = probs.argmax(dim=1) # (32,)# 如果想从 probs 中收集每个 batch 对应预测的概率：selected = probs[torch.arange(32), pred] # shape (32,) padding mask（seq 长短不一）： seq = torch.randint(0, 100, (4,7)) # batch, seq_lenmask = (seq != PAD_TOKEN) # True 表示有效# 通过 mask 做池化：masked_sum = (embeddings * mask.unsqueeze(-1)).sum(dim=1) one-hot： idx = torch.tensor([0,2,1])onehot = torch.nn.functional.one_hot(idx, num_classes=4) # shape (3,4) 按索引更新参数（embedding lookup 与更新）： embedding = torch.nn.Embedding(num_embeddings=1000, embedding_dim=64)out = embedding(idx_tensor) # embedding 内部就是高级索引/ gather 实现 15. 进阶：einsum 作为灵活替代（当索引和 reshape 太繁琐） einsum 可以在一次表达式中完成复杂的通道维度重新排列与约简，可替代多个 transpose + matmul 操作。 # 例：批量矩阵乘积 sum over k: c_ij = sum_k a_ik b_jktorch.einsum(ik,jk-ij, a, b) 16. 常见陷阱与建议 整数数组索引通常会返回 copy（非 view） —— 这会影响内存并且后续 in-place 修改不会影响原张量。 带负步长的切片常常产生 copy，要注意 is_contiguous()。 混合使用布尔掩码与维度不适配会报错，确保 mask 与被掩的张量形状一致或能广播。 不要对需要 autograd 的部分用原地替换（x[idx] …），会破坏计算图，使用 scatterscatter_add 或构造新张量。 索引返回的张量可能会改变梯度分配，只有被实际用到（参与 loss）的元素才会有梯度。 尽量用 gather 实现批量按位置索引（可保持形状且直观），而不是复杂的循环。 17. 一张速查小表（常用 API） 基本：x[i], x[:, j], x[..., k], x[start:stop:step] 维度：unsqueeze, squeeze, transpose, permute, reshape, view, contiguous 选择花式：index_select, take, gather, scatter, where, masked_select, masked_fill 布尔掩码：x[mask], torch.where, torch.nonzero 其他：one_hot, topk, argmax/argmin, torch.arange（用于构建 batch 索引） 如果你愿意，我可以： 把这些示例做成一个可以直接运行的 Jupyter notebook（包含针对常见错误的测试用例），或者 根据你常用的框架（PyTorch TensorFlow NumPy）把示例改写成你习惯的 API，或 针对你当前代码中的索引问题（把你代码贴来），帮你找 bug 并修正。 你想要哪种后续帮助？","tags":["张量","运算","Pytorch"],"categories":["深度学习"]},{"title":"好用的工具","path":"/2024/01/10/nicetools/","content":"VPNEFCloudhttps://www.efc123.com/shop 一元机场https://xn--4gq62f52gdss.ink 特价机场 便宜好用https://xn--6nq44rc0n82k.com ICON阿里巴巴矢量图标库https://www.iconfont.cn FONT AWESOMEhttps://fontawesome.com Carbonhttps://yesicon.app/carbon/skill-level 封面图生成BackImagehttps://nav.rdonly.com/laboratory/bgimage/backimage.html Coverhttps://cover.ruom.top 在线图片压缩TinyPNGhttps://tinypng.com 支持 PNG JPG WebP 一次可上传 20 张（单张 ≤ 5MB） 平均可减少 60–80% 体积 保留高画质，肉眼几乎无损 操作步骤： 打开网站 拖入图片 下载压缩后的版本 👉 适合 博客封面、展示图、LOGO 等日常用途。 ILoveIMGhttps://www.iloveimg.com/compress-image 支持批量上传 同时压缩 JPG PNG GIF 提供在线编辑（裁剪、加水印等） 高质量壁纸网站WallRoomhttps://wallroom.io Wallhavenhttps://wallhaven.cc Unsplashhttps://unsplash.com","tags":["工具"],"categories":["效率"]},{"title":"Teacher Forcing","path":"/2024/01/01/teacher_forcing/","content":"","tags":["优化","架构","模型"],"categories":["深度学习"]},{"title":"小技巧","path":"/2023/12/21/someskills/","content":"如何一次性删除所有以某关键词结尾的文件 其他类型删除以此类推 find . -type f -name *.ko -exec rm -rf \\; Git push报错ssh: connect to host github.com port 22: Connection refused fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个错误表明 Git 无法通过 SSH（端口 22）连接到 GitHub。可能的原因和解决方法如下： 1. 检查 SSH 连接运行以下命令测试 SSH 连接： ssh -T git@github.com 如果连接被拒绝，可能是网络或防火墙问题。 2. 改用 HTTPS 协议如果 SSH 被屏蔽，可以临时改用 HTTPS： git remote set-url origin https://github.com/用户名/仓库名.gitgit push （需要输入 GitHub 账号密码或个人访问令牌） 3. 尝试 SSH 端口 443如果 22 端口被屏蔽，GitHub 也支持通过 443 端口使用 SSH。编辑 ~/.ssh/config 文件： Host github.comUser xxxxqq.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 然后再次测试 SSH 连接。 4. 检查防火墙代理设置 确保本地防火墙或公司网络未屏蔽 SSH（端口 22443） 如果使用代理，需配置 Git 使用代理：git config --global http.proxy http://代理地址:端口git config --global https.proxy https://代理地址:端口 5. 验证 SSH 密钥确保你的 SSH 密钥已添加到 GitHub： cat ~/.ssh/id_rsa.pub 然后将内容粘贴到 GitHub Settings → SSH and GPG keys。 6. 检查仓库是否存在确认远程仓库地址正确且存在： git remote -v","tags":["技巧","命令"],"categories":["效率"]},{"title":"一键更换所有图片格式","path":"/2023/11/10/formatimage/","content":"如题, 要求是把 旧格式的图片引用： ![](imageurl) 批量替换为 新主题格式： % image imageurl % Python实现 这种方式需要电脑中有Python环境, 虽然配置环境也比较简单, 但是还是需要一些门槛 注意修改脚本中的文件夹路径为自己的即可 import osimport re# 要处理的文件夹路径，例如 Hexo 的 source/_postsROOT_DIR = ./source/_posts# 匹配旧格式：![](URL)pattern = re.compile(r!\\[\\]\\((https://[^)]*)\\))def replace_in_file(file_path): with open(file_path, r, encoding=utf-8) as f: content = f.read() # 替换为新格式：% image URL % new_content = pattern.sub(r% image \\1 %, content) if new_content != content: with open(file_path, w, encoding=utf-8) as f: f.write(new_content) print(Updated:, file_path)def walk_dir(root): for root_dir, _, files in os.walk(root): for filename in files: if filename.endswith(.md): full_path = os.path.join(root_dir, filename) replace_in_file(full_path)if __name__ == __main__: walk_dir(ROOT_DIR) print(Done.) VSCode一键替换 VSCode还是好用啊, 相比于写脚本来实现, VSCode的全局搜索支持正则表达式, 可以一键替换文件夹中所有内容 搜索：!\\[\\]\\((https:\\/\\/[^)]*)\\) 替换：% image $1 %"},{"title":"修改键盘F9为Home键","path":"/2023/10/22/changekey/","content":"修改键盘F9键功能,将其映射为HOME键 由于键盘是75键, 本身不具备HOME键, 但是日常和工作对HOME键还是挺有需要的(别问为什么不买全键的, 个人不太喜欢数字区, 对75键构造独钟) 回归正题, 如何实现 我采用的是修改注册表方式 网上能搜到比较多方案, 除了注册表, 一般都需要下载额外软件, 键盘驱动或者是windows软件来将按键功能映射为其他 所以, 修改注册表, 省时省力 修改注册表1.打开注册表 # 按下键盘上 win + r 并输入regidit 2.定位到按键修改路径 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Keyboard Layout 3.在右侧区域新建二进制值 - 命名为 Scancode Map, 并填入下列值 00 00 00 00 00 00 00 0002 00 00 00 47 E0 43 0000 00 00 00 00 00 00 00 4.点击确定, 退出注册表, 重启电脑, 永久生效👍","tags":["方法","技巧"],"categories":["效率"]},{"title":"序列到序列模型","path":"/2023/06/29/encoder-decoder/","content":"encoder-decoder模型同时使用编码器和解码器. 它将每个task视作序列到序列的转换生成（比如，文本到文本，文本到图像或者图像到文本的多模态任务）.Encoder-decoder模型通常用于需要内容理解和生成的任务，比如机器翻译 代表模型T5T5模型结构与原始的Transformer基本一致,除了做了以下几点改动 采用了一种简化版的Layer Normalization，去除了Layer Norm的bias，将Layer Norm放在残差连接外面 位置编码：T5使用了一种简化版的相对位置编码，每个位置编码都是一个标量，被加到logits上用于计算注意力权重。各层共享位置编码，但是同一层内，不同的注意力头的位置编码都是独立学习的 ​ 自监督预训练T5 在预训练阶段采用了类似于 BERT 和 GPT 的大规模自监督学习策略，但与这两个模型的设计不同的是，T5 使用了“文本到文本”的格式来处理任务。 预训练任务包括两种类型：Causal Language Modeling（因果语言建模）和 填空任务（Masked Language Modeling） 多任务微调除了使用大规模数据进行无监督预训练，T5模型还可以利用不同任务的标注数据进行有监督的多任务预训练，例如SQuAD问答和机器翻译等任务 数据集和参数作者对公开爬取的网页数据集Common Crawl进行了过滤，去掉一些重复的、低质量的，看着像代码的 文本等，并且最后只保留英文文本，得到数据集C4: the Colossal Clean Crawled Corpus 参数 取值 transformer层数 24 特征维度 768 transformer head数 12 总参数量 2.2亿 大模型主流架构Decoder-only LLM之所以主要都用Decoder-only架构，除了训练效率和工程实现上的优势外，在理论上是因为Encoder的双向注意力会存在低秩问题，这可能会削弱模型表达能力，就生成任务而言，引入双向注意力并无实质好处 而Encoder-Decoder架构之所以能够在某些场景下表现更好，大概只是因为它多了一倍参数。所以，在同等参数量、同等推理成本下，Decoder-only架构就是最优选择了","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"自回归模型","path":"/2023/06/27/autoregress/","content":"自回归模型 Autoregressive model，AR 本篇主要围绕GPT作为代表了解AR 2018年6月, OpenAI公司发表了论文“Improving Language Understanding by Generative Pre-training”《用生成式预训练提高模型的语言理解力》, 推出了具有1.17亿个参数的GPT（Generative Pre-training , 生成式预训练）模型 特点：decoder-only 基本原理：从左往右学习的模型，只能利用上文或者下文 AR模型通常用于生成式任务，在长文本的生成能力很强，比如NLG领域的任务：摘要、翻译、抽象问答 GPT模型架构 从上图可以很清楚的看到GPT采用的是单向Transformer模型, 例如给定一个句子[u1, u2, …, un], GPT在预测单词ui的时候只会利用[u1, u2, …, u(i-1)]的信息, 而BERT会同时利用上下文的信息[u1, u2, …, u(i-1),u(i+1), …, un] GPT采用了Transformer的decoder模块，但是做了修改 GPT移除了第二个encoder-decoder attention子层，只保留Masked Multi-head Attention子层和Feed Forward子层 对比经典Transformer采用6层decoder，GPT采用了12层 GPT训练过程 GPT训练过程包括两个阶段：1.无监督的预训练语言模型 2.有监督的下游任务fine-tuning 无监督的预训练语言模型给定句子U [u1, u2, …, un], GPT训练语言模型时的目标是最大化下面的似然函数 $$L_1(U) \\sum_i \\log P(u_i \\mid u_{i-k}, \\cdots, u_{i-1}; \\Theta)$$ 上述公式具体来说是要预测每个词 $$u_i$$的概率，这个概率是基于它前面$$u_{i-k}$$到$$u_{i-1}$$个词，以及模型$$\\Theta$$，这里的k表示上文的窗口大小，理论上k取值越大，模型所能获取的信息越充足，能力越强 GPT是一个单向语言模型,模型对输入U 进行特征嵌入得到 transformer 第一层的输h0，再经过多层 transformer 特征编码，使用最后一层的输出即可得到当前预测的概率分布，计算过程如下 $$h_0UW_e+W_p$$ $$W_p$$表示单词位置编码 → [max_seq_len, embedding_dim]，$$W_e$$可以看作是单位矩阵 → [vocab_size, embedding_dim] 得到输入张量h0后, 要将h0传入GPT的Decoder Block中, 依次得到ht $$h_t \\text{transformer_block}(h_{l-1}) \\quad l \\in [1,t]$$ 最后通过得到的ht来预测下一个单词 $$P(u) \\text{softmax}(h_t W^T_e)$$ 有监督的下游任务fine-tuningGPT预训练后, 会针对具体的下游任务对模型进行微调. 微调采用的是有监督学习, 训练样本包括单词序列[x1, x2, …, xn]和label y. GPT微调的目标任务是根据单词序列[x1, x2, …, xn]预测标签y $$P(y|x^1,…,x^m) \\text{softmax}(h_l^m W_y)$$ 其中Wy表示预测输出的矩阵参数, 微调任务的目标是最大化下面的函数 $$L_2 \\sum_{(x,y)} \\log P(y|x^1, \\cdots, x^m)$$ 综合两个阶段的目标任务函数, 可知GPT的最终优化函数为： $$L_3L_2+λL_1$$ 整体训练架构图： 数据集和参数GPT使用了BooksCorpus数据集,文本大小约5GB，包含7400w+的句子。这个数据集由7000本独立的、不同风格类型的书籍组成 参数 取值 transformer层数 12 特征维度 768 transformer head数 12 总参数量 1.17亿","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"自编码模型","path":"/2023/06/26/autoencoder/","content":"自编码模型 AutoEncoder model，AE 本篇主要围绕BERT作为代表了解AE 代表模型BERT BERT是2018年10月由Google AI研究院提出的一种预训练模型（Bidirectional Encoder Representation from Transformer） 特点：encoder-only 基本原理：在输入中随机MASK掉一部分单词，根据上下文预测这个词（不难联想到BERT的主要预训练任务MLM） AE模型通常用于内容理解任务（NLU），比如NLU中的分类任务：情感分析、提取式问答 BERT架构 BERT架构整体可以分为三个模块： 第一层Embedding模块 中间层Transformer模块 第三层预微调模块 Embedding模块该模块由三种embedding共同组成 Token Embeddings：词嵌入张量，第一个单词是CLS标志，可用于之后的分类任务 Segment Embeddings：句子分段入张量，为了服务后续的NSP预训练任务 Positional Embeddings：位置编码张量 整个Embedding模块的输出张量是这三个张量的直接加和结果 双向Transformer模块依然可以看到上面那张图，BERT只使用了Transformer中的encoder部分，完全舍弃decoder部分 两大预训练任务也集中体现在训练Transformer模块中 预微调模块 经过中间层Transformer的处理后，Bert的最后一层根据下游任务需求做调整即可 比如对于sequence-level的分类任务，直接取第一个[CLS] token的 last_hidden_state，再加一层全连接层后进行softmax来预测最终的标签 BERT的预训练任务MLM（Masked Language Model） 带mask的语言模型训练 在原始训练文本中, 随机的抽取15%的token作为参与MASK任务的对象 80%的概率下, 用[MASK]标记替换该token 在10%的概率下, 用一个随机的单词替换token 在10%的概率下, 保持该token不变. NSP（Next Sentence Prediction） 下一句话预测任务 输入句子对(A, B), 模型来预测句子B是不是句子A的真实的下一句话. 所有参与任务训练的语句都被选中作为句子A 其中50%的B是原始文本中真实跟随A的下一句话(标记为IsNext, 代表正样本) 其中50%的B是原始文本中随机抽取的一句话(标记为NotNext, 代表负样本) 数据集与参数**数据集：**BooksCorpus (800M words) + English Wikipedia (2,500M words) 模型超参数： 参数 取值 transformer层数 12 特征维度 768 transformer head数 12 总参数量 1.15亿","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"语言模型开篇","path":"/2023/06/20/lmopening/","content":"大语言模型介绍 这里先给出大语言模型的进化树 不难看出，从左到右，分支模型架构大致可以分为 encoder-only，encoder-decoder，decoder-only **大语言模型（Large Language Model）：**一种人工智能模型，旨在理解和生成人类语言，大语言模型可以处理杜仲NLP任务 → 文本分类、文旦、翻译、对话等 对于大预言模型的的大，主要体现在其参数量，一般认为，包含千亿级别以上参数的语言模型（目前定义参数量超过10B的模型为大语言模型）；当然，除了参数量，庞大的语料库也是其一大特点。 语言模型发展阶段第一阶段设计一系列的自监督训练目标（MLM、NSP等），设计新颖的模型架构（Transformer）, 遵循Pre-training和Fine-tuning范式. 典型代表是BERT、GPT、XLNet等;第二阶段逐步扩大模型参数和训练语料规模，探索不同类型的架构。典型代表是BART、T5、GPT-3等;第三阶段走向AIGC（Artificial Intelligent Generated Content）时代, 模型参数规模步入千万亿, 模型架构为自回归架构, 大模型走向对话式、生成式、多模态时代，更加注重与人类交互进行对齐，实现可靠、安全、无毒的模型. 典型代表是InstructionGPT、ChatGPT、Bard、GPT-4等. 语言模型技术发展第一阶段基于规则和统计的语言模型 → N-gram模型第二阶段神经网络语言模型第三阶段预训练语言模型第四阶段大语言模型 N-gram模型 基于规则和统计的模型的典型或者可以说就是N-gram模型 这里引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关 如果一个词的出现与它周围的词是独立的，那么我们称之为 unigram，也就是一元语言模型。 $$P(S)P(W1)⋅P(W2)⋅…⋅P(Wn)$$ 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 bigram。 $$P(S)P(W1)⋅P(W2∣W1)⋅P(W3∣W2)⋯P(Wn∣Wn−1)$$ 如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 trigram。 $$P(S)P(W1)⋅P(W2∣W1)⋅P(W3∣W2,W1)⋯P(Wn∣Wn−1,Wn−2)$$ 一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关，而这些概率参数都是可以通过大规模语料库来计算，比如三元概率： $$P(Wi|Wi-2)Count(Wi-2Wi-1Wi)Count(Wi-2Wi-1)$$ 简单理解上述公式，在语料库中，出现 我爱你 的次数比上 我爱 出现的次数 N-gram模型特点 采用极大似然估计，参数易训练 完全包含了前n-1个词的全部信息 可解释性强，直观易理解 上面说的是其优点，但是： 只能建模前n-1个词 随着n增大，参数空间指数增长 数据稀疏，会出现OOV问题（out of vocabulary） 泛化能力差 神经网络语言模型 基于N-gram语言模型以上的问题，以及随着神经网络技术的发展，人们开始尝试使用神经网络来建立语言模型 模型的输入: wt-n+1, …, wt-2, wt-1就是前n-1个词. 现在需要根据这已知的n-1个词预测下一个词 wt，C(w)表示单词w所对应的词向量 网络的第一层: 是将C(wt-n+1),..,C(wt-2), C(wt-1)这n-1个向量首尾拼接起来形成一个(n-1)*m大小的向量 网络的第二层: 定义一个全连接层, 通过全连接层后结果再使用tanh 激活函数进行处理 网络的第三层: 输出一共V个节点 (V代表语料的词汇总数), 本质上为一个全连接层. 每个输出节点y_i表示下一个词语为i的未归一化logits值. 最后使用 softmax 激活函数将输出值y进行归一化。得到最大概率值，就是我们需要预测的结果 相比于N-gram有更好的泛化能力，降低了数据稀疏带来的问题；但是对于长序列建模能力有限，可能会出现梯度消失问题。 基于Transformer的预训练语言模型 基于Transformer的预训练模型：包括GPT、BERT、T5等.这些模型能够从大规模通用文本数据中学习大量的语言表示，并将这些知识运用到下游任务中，获得较好的效果 预训练模型的使用方式： 预训练：在大规模数据集上事先训练神经网络模型，使其学习到通用的特征表示和知识 微调：在具体的下游任务中使用预训练好的模型进行迁移学习，以获取更好的泛化效果 预训练模型的特点： 优点：更强大的泛化能力，丰富的语义表示，可以有效防止过拟合 缺点：计算资源需求大，可解释性差等 大语言模型随着预训练模型参数的指数级提升，其语言模型性能也会线性上升。2020年，OpenAI发布了参数量高达1750亿的GPT-3，首次展示了大语言模型的性能。 大语言模型的特点： 优点：像“人类”一样智能，具备了能与人类沟通聊天的能力，甚至具备了使用插件进行自动信息检索的能力 缺点：参数量大，算力要求高、训练时间长、可能生成部分有害的、有偏见的内容等等 语言模型评估指标常见的有以下几种 Accuracy （准确率）: 模型预测正确的样本数量占总样本量的比重 Precision（精确率）: 在被识别为正类别的样本中，为正类别的比例 Recall（召回率）: 在所有正类别样本中，被正确识别为正类别的比例 BLEU 分数：评估一种语言翻译成另一种语言的文本质量的指标. 它将“质量”的好坏定义为与人类翻译结果的一致性程度. 取值范围是[0, 1], 越接近1, 表明翻译质量越好 ROUGE 指标：在机器翻译、自动摘要、问答生成等领域常见的评估指标. ROUGE 通过将模型生成的摘要或者回答与参考答案（一般是人工生成的）进行比较计算，得到对应的得分 PPL：用来度量一个概率分布或概率模型预测样本的好坏程度. PPL越小，标明模型越好 前三个相信都不陌生，机器学习就在使用了，涉及到的就是混淆矩阵中值得简单运算 这里主要介绍后面三个 BLEU BLEU 根据n-gram可以划分成多种评价指标，其中n-gram指的是连续的单词个数为n，实践中，通常是取N1~4，然后对进行加权平均 基本步骤： 分别计算candidate句和reference句的N-grams模型，然后统计其匹配的个数，计算匹配度 公式：candidate和reference中匹配的 n−gram 的个数 candidate中n−gram 的个数 candidate 候选值，即模型预测值 reference 参考值，即真实值 举个简单例子： 使用1-gram进行匹配： candidate: {it, is, a, nice, day, today} reference: {today, is, a, nice, day} 结果:其中{today, is, a, nice, day}匹配，所以匹配度为56 使用2-gram进行匹配： candidate: {it is, is a, a nice, nice day, day today} reference: {today is, is a, a nice, nice day} 结果:其中{is a, a nice, nice day}匹配，所以匹配度为35 以此类推，还是比较好理解的 不难发现，匹配个数越多，BLEU值越大，说明候选句子越好 但是，也会有特殊情况，比如： candidate: the the the the reference: The cat is standing on the ground 如果按照1-gram的方法进行匹配，则匹配度为1，显然是不合理的 **解决办法就是：**首先，计算一个单词在任意一个参考句子出现的最大次数, 然后用每个（非重复）单词在参考句子中出现的最大 次数来修剪–单词在候选句子的出现次数. 如下所示的公式 $$count_kmin(c_k,s_k)$$ 其中k表示在候选句子（candidate）中出现的第k个词语, ck则代表在候选句子中这个词语出现的次数，而sk则代表 在参考文本（reference）中这个词语出现的次数 代码实现先安装工具包 pip install nltk from nltk.translate.bleu_score import sentence_bleudef cumulative_bleu(reference, candidate): bleu_1_gram = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)) bleu_2_gram = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)) bleu_3_gram = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)) bleu_4_gram = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)) return bleu_1_gram, bleu_2_gram, bleu_3_gram, bleu_4_gram# 生成文本generated_text = [This, is, some, generated, text]# 参考文本列表reference_texts = [[This, is, a, reference, text]]# 计算 Bleu 指标c_bleu = cumulative_bleu(reference_texts, generated_text)# 打印结果print(The Bleu score is:, c_bleu)#The Bleu score is: (0.6, 0.387, 1.5e-102, 9.2e-155) ROUGE ROUGE指标与BLEU指标非常类似，均可用来衡量生成结果和标准结果的匹配程度，不同的是ROUGE基于召回率，BLEU更看重准确率。 ROUGE也分为四种方法：ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S 这里仅介绍ROUGE-N 还是举个简单例子说明： 使用ROUGE-1进行匹配： candidate: {it, is, a, nice, day, today} reference: {today, is, a, nice, day} 结果:其中{today, is, a, nice, day}匹配，所以匹配度为551,这说明生成的内容完全覆盖了参考文本中的所有单词，质量较高 代码实现先安装工具包pip install rouge import Rouge# 生成文本generated_text = This is some generated text.# 参考文本列表reference_texts = [This is another generated reference text.]# 计算 ROUGE 指标rouge = Rouge()scores = rouge.get_scores(generated_text, reference_texts[0])# 打印结果print(ROUGE-1 precision:, scores[0][rouge-1][p])print(ROUGE-1 recall:, scores[0][rouge-1][r])print(ROUGE-1 F1 score:, scores[0][rouge-1][f])# ROUGE-1 precision: 0.8# ROUGE-1 recall: 0.6666666666666666# ROUGE-1 F1 score: 0.7272727223140496 PPL 困惑都（perplexity） PPL用来度量一个概率分布或概率模型预测样本的好坏程度 给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好 公式： 两种形式 $${PPL}(S) ;; \\frac{1}{\\sqrt[N]{P(w_1w_2…w_N)}}$$ $${PPL}(S) ;; 2^{-\\frac{1}{N}\\sum_{i1}^{N}\\log_2 P(w_i)}$$ 从公式可以看出，句子概率越大，语言模型越好，困惑度越小 代码实现import math# 定义语料库sentences = [ [I, have, a, pen], [He, has, a, book], [She, has, a, cat]]# 定义语言模型unigram = I: 1/12, have: 1/12, a: 3/12, pen: 1/12,He: 1/12, has: 2/12,book: 1/12,She: 1/12, cat: 1/12# 计算困惑度perplexity = 0for sentence in sentences: sentence_prob = 1 for word in sentence: sentence_prob *= unigram[word] temp = -math.log(sentence_prob, 2)/len(sentence) perplexity+=2**tempperplexity = perplexity/len(sentences)print(困惑度为：, perplexity)# 困惑度为： 8.15 LLM架构类别这一块内容比较多，分开几章讲，这里简单介绍 LLM本身基于Transformer架构，原始Transformer为不同领域模型提供了灵感和启发 基于原始Transformer衍生出一系列模型，一些模型仅使用encoder，一些模型仅使用decoder，有些模型同时使用encoder和decoder LLM大致可以分为三种：自编码模型（encoder）、自回归模型（decoder）和序列到序列模型（encoder-decoder）","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"HEXO部署方案","path":"/2023/04/21/hexodeploy/","content":"hexo是一个无后端的博客系统, 不像wordpress, typecho可以直接线上编写文章, 需要在本地编写md文件, 然后编译成html上传到服务器, 整个过程还是比较繁琐, 所以, 想寻找hexo写作流的最佳方案 本地Hexo环境搭建安装必要软件 安装Git从Git官网下载并安装Git：https://git-scm.com/ 安装Node.js从Node.js官网下载并安装：https://nodejs.org/安装后验证： node -vnpm -v 安装和初始化Hexo 安装Hexo CLI npm install -g hexo-cli 初始化Hexo项目 hexo init myblogcd myblognpm install 本地测试 hexo server 访问 http://localhost:4000 查看博客效果。 项目结构说明myblog/├── _config.yml # 站点配置文件├── source/ # 文章和页面│ └── _posts/ # 文章目录├── themes/ # 主题目录├── scaffolds/ # 模板文件└── public/ # 生成的静态文件 服务器环境配置（无apt工具）由于你的服务器没有apt工具，推测可能是CentOSRHEL系系统，使用yum包管理器。 安装必要软件 安装Node.js # 添加NodeSource仓库（以Node.js 14.x为例）curl -sL https://rpm.nodesource.com/setup_14.x | sudo -E bash -# 安装Node.jssudo yum install -y nodejs 安装Git sudo yum install -y git 安装Nginx sudo yum install -y nginx 配置服务器Git仓库 创建Git用户和仓库 sudo useradd gitsudo passwd git # 设置密码su - gitmkdir -p ~/blog.gitcd ~/blog.gitgit init --bare 创建网站根目录 sudo mkdir -p /var/www/hexosudo chown git:git /var/www/hexosudo chmod 755 /var/www/hexo 配置Git钩子在 /home/git/blog.git/hooks/ 目录创建 post-receive 文件： # 切换到git用户su - git# 进入hooks目录cd /home/git/blog.git/hooks# 创建post-receive文件cat post-receive EOF#!/bin/bashecho 开始部署Hexo博客...# 设置路径GIT_DIR=/home/git/blog.gitWORK_TREE=/var/www/hexo# 创建网站目录（如果不存在）mkdir -p $WORK_TREE# 检出文件到网站目录while read oldrev newrev refnamedo branch=$(git rev-parse --symbolic --abbrev-ref $refname) if [ $branch = master ]; then echo 正在部署master分支到网站目录... git --work-tree=$WORK_TREE --git-dir=$GIT_DIR checkout -f $branch # 检查是否部署成功 if [ $? -eq 0 ]; then echo ✅ 博客部署成功！ echo 网站文件已更新到: $WORK_TREE # 显示部署的文件列表 echo 部署的文件: ls -la $WORK_TREE/ | head -10 else echo ❌ 部署失败！ exit 1 fi fidoneEOF 赋予执行权限： chmod +x /home/git/blog.git/hooks/post-receive# 将钩子文件的所有者改为git用户sudo chown git:git /home/git/blog.git/hooks/post-receive# 检查修正后的权限ls -la /home/git/blog.git/hooks/post-receive# 将整个blog.git目录的所有权交给git用户sudo chown -R git:git /home/git/blog.git# 修复网站目录权限sudo chown -R git:git /var/www/hexosudo chmod -R 755 /var/www/hexo 配置Nginx编辑 /etc/nginx/nginx.conf 或 /etc/nginx/conf.d/hexo.conf： server listen 80; server_name your-domain.com; # 替换为你的域名或IP root /var/www/hexo; index index.html; location / try_files $uri $uri/ =404; 重启Nginx： sudo systemctl start nginxsudo systemctl enable nginx 配置Hexo多平台部署配置GitHub Pages仓库 创建GitHub仓库 仓库名格式：用户名.github.io 设置为public仓库 配置SSH密钥 ssh-keygen -t rsa -C your-email@example.com 将公钥id_rsa.pub内容添加到GitHub的SSH keys中。 配置Hexo部署设置修改Hexo项目中的 _config.yml 文件： # 部署配置[citation:6]deploy: type: git repo: # 服务器仓库 - git@your-server-ip:/home/git/blog.git # GitHub仓库 - git@github.com:yourusername/yourusername.github.io.git branch: master 这里注意, github 上的主分支可能是 main, 可以直接在配置文件修改, 也可以部署好后进入github settings中修改主分支, 网上资料也比较多 安装部署插件npm install hexo-deployer-git --save 完整的写作和部署流程日常写作流程 创建新文章 hexo new 文章标题 编辑文章在 source/_posts/ 目录下找到对应的Markdown文件进行编辑。 本地预览 hexo clean hexo generate hexo server 部署到双平台 hexo clean hexo generate hexo deploy 访问直接访问 用户名.github.io 或者自己的域名都可以访问博客","tags":["HEXO","博客"],"categories":["博客"]},{"title":"卸载python要先安装python？","path":"/2023/01/01/uninstallpy/","content":"最近卸载旧版python时发现这么一个错误 大致能看出来是权限问题或者文件被锁定，Config.MSI 目录是 Windows Installer 用于存放安装回滚脚本（.rbf.rbs）的隐藏系统文件夹。安装程序完成后会自动删除这些文件，但如果安装中途失败或缺乏权限，文件可能残留未被清理。此外，如果相关的 Python 安装程序或其它程序（如 Windows Installer 服务自身）正在使用这些 *.rbf 文件，则直接删除时会提示“拒绝访问”。因此常见原因包括： Config.MSI 文件夹默认隐藏且标记为受保护系统文件，需要管理员权限才能修改。 之前的安装卸载未正常完成，回滚文件遗留在 Config.MSI 中未被删除。 目标文件正被系统或其它进程占用（例如 Windows Installer 进程还在运行），导致无法删除。 用户权限不足（未以管理员身份运行），无法修改或删除该文件。 权限不足不让删除的话，那手动删除是否安全呢网上答案众说纷纭，但结论是没啥影响 GPT给出的答案是：Config.MSI 文件夹中的 .rbf 文件只是安装时的回滚脚本备份，不会影响正常系统运行。多位技术专家指出，如果这些文件在卸载后未被自动删除，是可以手动删除或忽略的[superuser.com](https://superuser.com/questions/8012/what-is-the-hidden-system-config-msi-folder#:~:text=,files from your hard drive)[cnblogs.com](https://www.cnblogs.com/zjoch/archive/2010/05/05/1728106.html#:~:text=在Windows的安装过程中将产生返回脚本（rollback scriptS）以实现返回功能，在这些文件中包含了已执行的操作序列，文件和注册表的更新信息以及其他操作信息。脚本文件包括,则包含在操作系统所在的驱动器中。)。例如，superuser 上的回答引用技术博客指出：“配置文件夹包含安装时的备份脚本，成功安装后应自动删除…如果安装程序没有清除它们，你可以安全删除该文件夹和文件”[superuser.com](https://superuser.com/questions/8012/what-is-the-hidden-system-config-msi-folder#:~:text=,files from your hard drive)。中文技术博客也说明：Windows 安装过程中产生的 Config.MSI 文件夹可以自行删除以释放空间[cnblogs.com](https://www.cnblogs.com/zjoch/archive/2010/05/05/1728106.html#:~:text=在Windows的安装过程中将产生返回脚本（rollback scriptS）以实现返回功能，在这些文件中包含了已执行的操作序列，文件和注册表的更新信息以及其他操作信息。脚本文件包括,则包含在操作系统所在的驱动器中。)。因此，一般情况下*直接删除 Config.MSI 文件夹及其中的 *.rbf 文件是安全的，这不会破坏系统，只是移除了无用的回滚备份。 OK，既然这样，那就用管理员权限给它删掉吧 隐藏文件展示这类文件默认不会展示 方法一：通过控制面板进入“文件夹选项”这是 Win11 中最稳定、最通用的方式。 按下 Win + R 输入： control folders 然后回车 👉 会直接打开 “文件夹选项”窗口（老界面） 切换到 查看(View) 选项卡 在高级设置中向下滚动，找到： “隐藏受保护的操作系统文件（推荐）” 取消勾选 → 系统会弹出警告 → 点击 确定 即可。 方法二：通过控制面板图形化入口如果你想通过界面一步步点进去： 打开 控制面板 可以按 Win 键然后输入 控制面板 搜索 点击 外观和个性化 点击 文件资源管理器选项 👉 会打开“文件夹选项” 切换到 查看(View) 标签 找到并取消： 隐藏受保护的操作系统文件（推荐） 操作完后应该就可以再D盘看到Config.MSI文件夹 删除Config.MSI管理员权限打开CMD，运行下面命令 takeown /F D:\\Config.MSI /R /D Yicacls D:\\Config.MSI /grant Administrators:F /T 以上命令将把 D:\\Config.MSI 文件夹及子文件的所有权授给管理员组，让你拥有完全控制权 然后执行 rd /S /Q D:\\Config.MSI 操作完后查看D盘就没有Config.MSI文件夹，此时再去尝试卸载旧版本python，如果删掉了，恭喜 如果没删掉，按照下面操作来 通过安装包卸载 确实很反直觉啊，python的卸载不能在控制面板中直接删除，而是需要通过安装包，没错，也就是exe文件来删除，这类文件一般安装好程序后我都不会再保留，所以还需要去官网下载对应版本 比如我的版本是3.8.6 右键exe文件以管理员身份运行，会看到三个选项，选择uninstall卸载即可","tags":["程序","卸载"],"categories":["Python"]},{"title":"Self Attention","path":"/wiki/note/attention.html","content":"为什么需要注意力机制 Transformer出现之前, 对于NLP任务的主力模型是: RNN LSTMGRU 这些模型都承担着一个艰难任务： 在一个长序列中，理解词与词之间的关系，尤其是远距离的关系。 但它们有一个共同的致命限制： 不能让每个词看到全局 信息被压得太狠，丢失严重 训练难以并行，效率低下 注意力机制正是为了解决这些根本性问题而诞生的。 为什么RNNLSTM处理不好全局依赖单向传递信息RNNLSTM 的信息流是链式的： x1 → x2 → x3 → x4 → ... 这意味着： x5 想知道 x1 的信息，要经过 x2 → x3 → x4 的层层传递 一旦中间丢失，就再也补不回来 越远的关系，越容易消失 所以像这种句子对它们非常困难： “我昨天看了一部电影，电影主角是….剧情非常的跌宕起伏,….它非常好看。” “它” 与 “电影” 的距离太远，RNN 容易“忘”。 梯度消失梯度爆炸链式结构导致反向传播也需要一层层回传。 传统 RNN 的隐藏状态更新公式： $$httanh(Wht−1+Uxt)$$ 反向传播时梯度会不断乘以权重矩阵 WWW 和 tanh 的导数 $$Dnσ′(z1)w1⋅σ′(z2)w2⋅⋯⋅σ′(zn)wn$$ 不难发现, 梯度的计算呈连乘形式 当序列很长时： 梯度不断变小 → 消失 或不断变大 → 爆炸 这让模型学不到长期关系。 当然,LSTMGRU 在梯度消失爆炸问题已经做了优化, 通过门控机制控制信息流, 使一部分状态以近似线性方式在序列中传播，从而避免梯度在时间维度上的指数级衰减或爆炸，解决传统 RNN 的长期依赖问题 串行计算RNN系列模型的特点: 当前时间步必须等上一时间步走完才能开始 因此, 训练速度很慢 而 Attention 可以并行计算, 一次矩阵乘法全部计算出来, 并行计算, 充分利用GPU 为什么不用CNNCNN 虽然可以并行，但存在一个无法突破的问题： CNN 的感受野是局部的。 想让 CNN 跨越长距离依赖，需要不断叠加卷积层扩展感受野。 5层感受野可能只有几十个词 要覆盖整个句子需要几十层甚至更多 训练难度非常大。 隐藏状态是对信息的强压缩RNN 把之前所有词的信息压进一个固定维度的张量： h_t = f(x_t, h_t-1) 无论你前面说了 100 个词，还是 1000 个词，最终信息都必须塞进一个固定长度的 h。 上图是seq2seq架构处理翻译任务, seq2seq架构包括三部分, encoder(编码器), decoder(解码器), 中间语义张量c 对于图中的案例, 编码器首先处理中文输入 “欢迎 来 北京”, 通过GRU模型获得每个时间步的输出张量, 最后将它们拼接成一个中间语义张量c, 接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言 decoder只能依赖固定长度的中间语义张量c 也就是说, 无论句子多长, 多复杂, 都要压缩成一个固定维度张量 结果就是: 信息丢失严重 长序列问题效果差 注意力机制如何解决这些痛点每个词直接看到所有词在注意力里，一个词无需通过第 2、3、4、5 个词才能知道第 1 个词的信息。 它可以直接算： 相关性 = Query(当前词) · Key(所有词) 这是 完全平等的全局视野。 不存在梯度消失问题因为注意力不依赖链式结构，反向传播只会经过几层矩阵运算，梯度非常稳定。 完全可并行化（压倒 RNN 的关键优势）RNN 每一步都依赖上一部 ⇒ 只能串行 Attention 所有词之间的关系都可同时计算 ⇒ 一次矩阵乘法 速度差几十倍甚至上百倍。 这是 Transformer 能训练巨型模型的根本原因。 注意力机制如何计算 这里简要概括, 详细过程推荐知乎猛猿的一篇文章, 这里给出链接 Transformer学习笔记二：Self-Attention（自注意力机制）https://zhuanlan.zhihu.com/p/455399791 Transformer 使用的是缩放点积注意力(Scaled Dot-Prodcut Attention) 对于每个token, 产生三个矩阵 Query, Key, Value(下文简称 Q, K, V) Q（Query）：我想找什么？ K（Key）：我有哪些信息？ V（Value）：对应的信息内容 $$\\text{Attention}(Q, K, V) \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V$$ (1) Q 与 K 做点积：计算“匹配程度”score = Q • Kᵀ 相似度越高 → 代表 Query 认为这个 Key 更相关。 (2) 缩放（除以 √d）防止数值过大score = score / √d (3) Softmax → 转成概率分布（注意力权重）attention_weights = softmax(score) (4) 用权重加权求和 Value，得到最终输出output = attention_weights × V 自注意力机制对于一般情况下, Query来自decoder, Key, Value来自encoder 自注意力则是一种特殊情况, Query, Key, Value均来自同一个序列, 即 $$QXW^Q,KXW^K,VXW^V$$","tags":[null,null],"categories":[null]},{"title":"追星Transformer","path":"/wiki/note/index.html","content":"O1 搭建知识库 正常 72% KR1 添加《初始Transformer》 对Transformer的大致了解 已完成 100% KR2 添加《Self Attention》 关于Transformer中的注意力机制，自注意力、多头注意力… 正常 90% KR3 添加《Position Encoding》 关于Transformer中的位置编码实现 已完成 100% KR4 关于Transformer模型家族，Bert、GPT等 未完成 0%"},{"title":"Positional Encoding","path":"/wiki/note/posencoding.html","content":"首先抛出一个问题, 为什么Transformer 要有位置编码? – 对于任何一门语言, 单词在句子中的位置和顺序都极为重要, 语序正确, 才能表达出正确含义 I do not like the story of the movie, but I do like the cast.I do like the story of the movie, but I do not like the cast. 如上述两个句子, 仅是改变not位置, 表达的意思截然相反 – Transformer抛弃了RNN和CNN作为序列学习的基本模型, 我们知道, 循环神经网络本身就是一种顺序结构, 天然包含了词在序列中的位置信息, 当抛弃循环神经网络结构, 完全采用Attention取而代之, 这些词序信息就会丢失, 模型就没有办法直到每个词在句子中的相对位置和绝对位置, 因此, 有必要把词序信息加到词向量上帮助模型学习这些信息, 位置编码(Positional Encoding)就是用来解决这个问题的办法 这里给出huggingface上的的一篇文章, 对于位置编码的讲解非常清晰透彻 设计位置编码https://huggingface.co/blog/zh/designing-positional-encoding 一、什么是位置编码 在transformer的encoder和decoder中使用了Positional Encoding, 最终的输入就变为了: input word_embedding + positional_encoding word_embedding : 词嵌入, 将token的维度从vocab_size映射到d_model(原论文中, d_model为512) 最终的输入通过词嵌入后的向量和位置编码矩阵相加得到, 所以positional_encoding也是d_model维度的向量 二、位置编码构造方法2.1 用增长数值标记位置 第一个token标记1, 第二个token标记2… 以此类推 这种方法存在一些问题: 模型可能遇见比训练时所用序列更长的序列, 不利于模型泛化 模型无法理解数字的含义, 可能认为大数字比小数字权重更高, 而非顺序更靠后 无法表达相对位置信息 2.2 用[0, 1]范围标记位置为解决上述问题, 将数值限制在[0, 1]区间内, 对其等分切割, 假设有3个token, 则位置信息为[0, 0.5, 1] 这样产生的问题是, 序列长度不同, 相对距离也不同, 模型可能认为这是单词语义发生了变化 2.3 用二进制向量标记位置 由于位置信息最终会作用到word_embedding上, 比起用单一的数值, 更好的方式是使用和word_embedding同纬度的向量, 可以想到, 将数值转换成二进制形式 假设d_model 3 d_model一般比较大(512, 1024…), 基本可以把每个token的位置都编码出来 但这样也存在问题, 不同单词间的位置变化不连续, 难以推测相对位置信息 2.4 用sin和cos函数交替标记位置 这是论文中提出的方法 三角函数有界且连续, 可以满足目前的需要 $$PE_{(pos, 2i)} \\sin\\left(\\frac{pos}{10000^{2id_{model}}}\\right)$$ $$PE_{(pos, 2i+1)} \\cos\\left(\\frac{pos}{10000^{2id_{model}}}\\right)$$ 参数说明: 符号 含义 pos 当前位置（position index） i 维度索引（dimension index） d_model 模型维度（embedding size，例如512） PE 位置编码矩阵（shape: [seq_len, d_model]） 这里只给出计算公式, 数学推导网上优秀文章较多, 不再演示 三. 为什么可以直接将词向量和位置编码进行相加 这是一直困扰我的点, 仅是进行张量相加就可以达到添加位置信息的作用吗? 难道这样不会破坏词向量本身信息吗? 网上浏览了一些博主的文章和视频, 但是答案也并没那么明确, 有点大力出奇迹的意思 3.1、公式复习Transformer 的输入是：$$X E_{\\text{word}} + E_{\\text{pos}}$$其中： $E_{word}$：词向量（表示词语语义） $E_{pos}$：位置编码（表示词语在句子中的位置） 维度相同（例如都为 $d_{model}512$） 3.2、为什么相加就能引入位置信息？1. 向量加法在嵌入空间中表示“组合信息”在向量空间中，加法表示叠加不同的语义因素。 比如： “king” ≈ “man” + “royalty” “Paris” - “France” + “Italy” ≈ “Rome” 同理： 把词向量和位置向量相加，就相当于在语义中叠加“第几个词”的信息。 也就是说，新的向量同时携带了词义和位置两种特征。 2. 相加不会“覆盖”信息，而是“偏移”语义空间我们可以把每个词向量看成在一个高维语义空间中的点。 加上位置编码后，等价于把它 沿着“位置维度方向”平移了一点： $$\\text{new_embedding semantic_embedding + position_offset}$$ 所以模型看到的不是“词义被破坏”，而是“相同词在不同位置处于略微不同的方向”。 这使得 Transformer 能区分： “I love you”和“You love I” 的区别。 3. 注意力机制会自动学会利用这些位置差异在自注意力计算中，Q、K 向量都会从输入中线性变换得到： $$Q XW_Q, \\quad K XW_K$$因此，位置信息通过 $E_{pos}$ 影响了 Query-Key 相似度。 当两个词位置不同、位置编码不同，它们的注意力权重分布也不同 → 模型能够学习“前后关系”。 3.3、为什么不会破坏词向量原始信息？1. 相加是线性可逆的（在一定程度上）如果两个向量空间的分布相对独立，线性相加仍能被后续层（线性变换）分离： $$W(E_{word}+E_{pos})WE_{word}+WE_{pos}$$Transformer 的多层线性结构可以在后续层中重新分辨“语义部分”和“位置信息部分”。 2. 位置编码的幅度较小，不会淹没词向量在实现时，位置编码通常被归一化或初始化为较小值（例如在 [-1,1] 范围内）。 这样词向量的主导语义仍然保留，只是轻微偏移 → 注入位置信息。 3. 高维空间中的“信息容量”巨大在 512 维或更高的空间中，向量叠加后不会导致严重的信息重叠。 就像 RGB 图像里混合红色和绿色还能区分出“黄色”一样， 词义与位置信息叠加后仍可被网络区分和提取。","tags":[null,null],"categories":[null]},{"title":"编码器","path":"/wiki/note/transformer-encoder.html","content":"","tags":[null,null],"categories":[null]},{"title":"知识学爆","path":"/wiki/tutorial/index.html","content":"知识回顾工具这么多，命令这么繁，一段时间不用就会忘记，这很正常，需要用时来翻翻。扳布"},{"title":"doccano数据标注平台","path":"/wiki/tutorial/doccano.html","content":"Doccano平台介绍 Doccano是documment anotation的缩写，是一个开源的文本标注工具，我们可以用它为NLP任务的语料库进行打标。它支持情感分析，命名实体识别，文本摘要等任务。 它的操作非常便捷，在小型语料库上，只要数小时就能完成全部的打标工作。下面介绍一下如何安装、配置和使用doccano。 一、doccano的安装 doccano的安装要在虚拟环境下进行！ 进入你用到的虚拟环境中, 安装doccano pip install doccano 然后，在终端里输入 # 初始化数据库doccano init# 创建一个super user。这里要把pass改成你需要的密码。当然，用户名也可以改成别的。doccano createuser --username admin --password 1234 二、启动doccano首先，在终端中运行下面的代码来启动WebServer # 启动webserverdoccano webserver --port 8000 然后，打开另一个终端，运行下面的代码启动—任务队列: # 启动任务队列doccano task 三、运行doccano与创建新的文本打标项目 首先，打开浏览器（最好是Chrome），在地址栏中输入http://127.0.0.1:8000/并回车。 然后，我们点击中间的蓝色按钮“快速开始”。此时，我们会跳转到登陆的界面。这里，我们需要用之前创建的超级用户登陆。 完成登陆后，我们会来到“项目”的界面。我们可以点击左上角的“创建”按钮来创建新的项目；也可以点击“删除”按钮来删除已经创建的项目。 我们点击左上角的“创建”按钮，创建一个新的项目。 对项目进行基本的说明，在填写完要求的信息后，点击创建，我们就创建了一个新的NLP标注项目。在创建完成后，会自动跳转到项目的主页。 进入数据标注平台 最左侧是一系列可以选择的页面。“主页”这个标签下面是doccano提供的一系列教程，其他的页面可以对项目进行设置。 稍后，我们将在上图所示的界面中的完成文本打标项目的各项设置。我们会依次点击左侧的各个标签，依次进行设置。 四、添加语料库 我们直接从“数据集”这个标签开始看。 在“数据集”这个页面，我们可以将准备好的文本添加到项目中，为将来的打标做准备。 我们首先点击左上角的“操作”→“导入数据集“。 此时，我们会来到”上传数据”的界面。 如上图所示，doccano总共支持4种格式的文本，他们的区别如下： Textfile：要求上传的文件为txt格式，并且在打标的时候，一整个txt文件在打标的时候显示为一页内容； Textline：要求上传的文件为txt格式，并且在打标的时候，该txt文件的一行文字会在打标的时候显示为一页内容； JSONL：是JSON Lines的简写，每行是一个有效的JSON值。 CoNLL：是“中文依存语料库”，是根据句子的依存结构而建立的树库。其中，依存结构描述的是句子中词与词之间直接的句法关系。具体介绍看汉语树库。 注意： doccano官方推荐的文档编码格式为UTF-8。 在使用JSONL格式的时候，文字数据本身要符合JSON格式的规范。 数据集中不要包含空行。 这里我们以Textline格式举例。点击“TextLine格式”。然后在跳转到的界面里，设置File Format和Encoding。然后点击下图中的“Drop files here…”来上传文件，最后，点击导入即可。 此时，再点击“数据集”的标签，我们就可以看到一条一条的文本已经被添加到项目中了。将来我们将对这些文本进行打标。 五、添加标签 在这一部分，我们讲解如何往项目中添加在打标时可选的标签。 在NER任务中，我们可能会添加People、Location、Company等；在文本分类任务中，我们可能会添加Positive、Negative等标签作为打标时的可选标签。 注意，这里只是添加将来可供选择的标签，是项目配置的过程，而不是进行文本标注。 点击左侧的“标签”按钮，就来到了添加标签的界面。 继续点击“操作”按钮，并在下拉菜单中点击“创建标签”按钮。 在弹出的“创建标签”窗口里面，在标签名一栏写上标签的名字，键值代表快捷键。例如在NER的例子中，可以写People、Location、Company等。例如，我们给People设置的快捷键是p。将来在打标的时候，右手用鼠标选中段落中的文字（例如“白居易”），左手在键盘按下快捷键p，就可以把被选中的文字打标成“People”。 再往下，我们可以给标签自定义颜色。 全部设置好以后，点击右下角的“保存”按钮。 此时，一个标签就添加完成了。我们以同样的方法添加其他所需要的标签。 六、添加成员 在为机器学习的语料库打标的时候，由于语料库一般比较大，如果让一个人给所有的文本打标的话，那到地老天荒都完不成。因此，我们需要多个人协同完成语料库的打标工作。 回忆一下，此时我们的项目还只有一个成员，也就是在初始配置doccano的时候创建的超级用户admin。因此，为了让其他人参与到打标项目中来，我们首先需要为其他成员创建账户。 我们打开网页http://127.0.0.1:8000/admin/，来到数据库的管理系统页面Django administration，并用超级用户的账号密码登陆该管理系统。 此时，我们再返回项目的设置页面。点击左侧的“成员”标签，点击页面上的“添加”按钮，会弹出“添加成员”窗口。 其中，在“用户搜索接口”的下拉菜单里面可以找到我们刚添加的用户“小明”。 注意，在这里只能找到已经创建到的用户，而不能创建新的用户。如果要新建用户，必须要到前面Django administration界面。 同时，我们还可以设置不同的成员的角色，不同的角色对应着不同的权限。如下图，我们把小明设置为“标注员”。其他角色还有项目管理员和审查员。 七、添加标注指南 我们可以事前给标注员和审查员准备一些标注指南，便于项目成员理解我们标注的要求和注意点。 例如，在判断文本正负面倾向的文本分类任务中，我们要具体说明判断正负面的标准，例如满足哪些要求，我们就可以认为一个本文是正面的。 因为一万个读者眼里有一万个哈姆雷特，不同人对文本的理解和判断正负面的尺度是不一样的。我们只有把标准写具体、写明确了，让人不用动脑筋都能做出符合我们要求的判断，我们才能得到一个尺度统一的数据集。数据集上的打标尺度统一，是机器学习获得好的效果的前提。 添加指南的界面如下图所示。 八、开始给文本打标 准备工作忙活了老半天，终于可以进入正题了—-给文本打标。 需要注意的是，上面的前期设置里面并不是所有的都是必须的。在最精简的情况下，我们可以在仅添加了数据集与标签后，就开始给文本打标。 这里，我们用标注员小明的账号登陆打标系统做演示。同样是打开http://127.0.0.1:8000/地址，输入小明的账号密码登陆。 和之前不一样的是，由于小明的角色是“标注员”，因此他只有打标的权限，没有对项目进行各项设置的权限，所以在左侧列表没有管理员用户的各项设置项目。 这里我们直接点击左上角的“开始标注”进行打标。 以NER任务为例，在打标的界面下，我们选中句子中的实体，会自动弹出一个下拉菜单，我们可以从这个下拉菜单中选择相应的实体类型People，也可以直接在键盘上按下p键。 这是添加标签之后的样子。 九、导出打标结果 当我们要导出标注结果的时候，我们重新用管理员用户登陆，在“数据集”页面下，点击“操作”→“导出数据集”。 在弹出的窗口中，根据我们的需要进行设置后，点击Export，即可导出标注结果。 保存好的文本是字典的格式。 如下图所示，保存了句子的ID、句子原文、实体的在句子中的位置、实体的类型。","tags":[null,null,null],"categories":[null]},{"title":"认识Transformer","path":"/wiki/note/knew-transformer.html","content":"Transformer：深度学习时代的分水岭过去十年，深度学习几乎重塑了整个人工智能领域。从图像识别到语音识别，再到自然语言处理（NLP），我们见证了一个又一个模型的进化。而在 NLP 领域，真正改变游戏规则的模型，毫无疑问是 Transformer。 这篇文章将带你从零理解 Transformer： 它为什么出现？解决了什么问题？内部结构如何运作？为什么它能成为 GPT、BERT 等大模型的基础？ 一、Transformer 的出现：时代的需要在 Transformer 出现之前，NLP 的主流模型是： RNN（循环神经网络） LSTM GRU（改进版 RNN） CNN 文本卷积模型 虽然这些模型在当时取得了不错的效果，但也遭遇了多个根本性瓶颈： 1. 序列依赖太强，难以并行RNN 每一步都依赖前一步： h(t) - h(t+1) - h(t+2) ... 这意味着： 训练无法并行化 速度慢 对长序列不友好（梯度消失爆炸问题） 2. 长距离依赖难以建模例如句子： “我昨天看了一部电影，我觉得它非常好看。” 这里“它”指代“电影”，人类很自然，但 RNN 需要跨越很多词才能关联，效果不稳定。 3. CNN 虽可并行，但感受野有限必须堆叠很多层卷积才能覆盖长距离上下文，效率仍然不高。 二、Transformer：新的范式从此诞生2017 年，Google 在论文 《Attention Is All You Need》 中提出： “序列建模根本不需要 RNN，也不需要 CNN，只需要 Attention（注意力）。” 这是一个革命性的观点。 Transformer 完全抛弃 RNN 的递归结构，转而只使用： → Self-Attention（自注意力机制）→ Position Encoding（位置编码）→ 多头注意力、多层堆叠（Multi-Head, Stacked Layers）最核心的思想是： “在序列中，每个词都能直接和其他所有词建立联系，并根据相关性动态分配权重。” 这彻底解决了 RNN 的所有历史问题。 三、Transformer 为什么如此强大？总结它的优势可以一句话概括： 更快、更准、能看全局。 下面逐点解释。 1. 训练可以完全并行化（突破性进步）自注意力机制让所有词之间的关系可以一次性计算： 所有词 ↔ 所有词 不再像 RNN 一步步传递。 这带来： 训练速度巨大提升 能利用 GPU 的矩阵计算优势 能训练更大的模型 2. 天然具备“全局视野”自注意力机制计算每个词对所有词的重要性，例如： Attention(它, 电影) → 高权重Attention(它, 昨天) → 低权重 模型能自动找到长距离依赖关系。 3. 表达能力更强Transformer 使用 多头注意力（Multi-Head Attention）： 一个头关注语义信息 一个头关注句法结构 一个头关注代词关系 …… 模型可以从多个角度“看”句子，让表示更加丰富。 4. 更容易扩展、堆叠和并行Transformer 模块化结构清晰，可以堆叠几十层甚至上百层。 这为后续的模型奠定了基础： BERT（2018） GPT 系列（2018 – 2024） ViT 图像 Transformer（2021） ChatGPT、Claude、Gemini 等顶级大模型 都源自 Transformer。 四、Transformer 内部结构概览 Transformer 包含 Encoder（编码器） 和 Decoder（解码器） 两个部分，但现代大多数模型只使用其中一部分： 模型 使用结构 BERT Encoder-only GPT Decoder-only T5 FLAN Encoder-Decoder 原始架构中，一个 Transformer 层包含： 1. Multi-Head Self-Attention（多头自注意力）核心计算公式： Attention(Q, K, V) = softmax(QKᵀ / sqrt(d_k)) V 实现“词与词之间相关性”的计算。 2. Feed Forward Network（位置前馈网络）对每个词的表示做非线性变换，提升表达能力。 3. Add Norm（残差连接 + LayerNorm）为了解决深层网络训练困难问题。 4. Position Encoding（位置编码）因为 Transformer 不再使用 RNNCNN，序列没有位置顺序，因此需要位置编码告诉模型： 谁是第一个词 谁是第二个词 谁更靠后 位置编码通常用正弦余弦函数构造。 五、Transformer 之后：新时代的大模型浪潮Transformer 诞生后，NLP 领域彻底变天。 2018：BERT 出现 彻底改变 NLP 预训练范式 情感分析、命名实体识别等任务效果暴涨 2018~2020：GPT 系列 GPT-1 → GPT-2 → GPT-3 语言生成能力大幅提升 LLM（大语言模型）时代开启 2021：Vision Transformer（ViT）Transformer 模型跨界视觉，效果强于 CNN 2022~2025：ChatGPT 时代Transformer 模型成为 AGI 的基础框架 世界开始感受 AI 的力量","tags":[null],"categories":[null]}]