[{"title":"GitHub 每周热门项目 (2025.11.10 - 2025.11.16)","path":"/2025/11/13/github-weekly/","content":"🚀 本文自动生成，数据来源于 OSSInsight 与 GitHub 官方 API，每周自动更新。 1. cryptocj520crypto-trading-open 👤 作者: cryptocj520🌍 语言: Python⭐ Stars: 977 crypto-trading-open 2. fgit-rewrite-commits 👤 作者: f🌍 语言: TypeScript⭐ Stars: 650 AI-powered git commit message rewriter using GPT 3. crowmd2K-N8NWORKFLOWS 👤 作者: crowmd🌍 语言: NA⭐ Stars: 448 暂无简介 4. Doriandarkokimi-writer 👤 作者: Doriandarko🌍 语言: Python⭐ Stars: 432 AI writing agent powered by kimi-k2-thinking - autonomously creates novels and stories with deep reasoning 5. wesbossticker-dream 👤 作者: wesbos🌍 语言: TypeScript⭐ Stars: 409 voice activated sticker dreamer and printer. 6. daodao97code-switch 👤 作者: daodao97🌍 语言: Vue⭐ Stars: 357 Claude Code Codex 多供应商代理与管理工具 7. boku7venom 👤 作者: boku7🌍 语言: Python⭐ Stars: 314 Venom C2 is a dependency‑free Python3 Command Control framework for redteam persistence 8. Meph1s-tStartrail-Gal 👤 作者: Meph1s-t🌍 语言: Ren’Py⭐ Stars: 255 暂无简介 9. princepainterComfyUI-PainterI2V 👤 作者: princepainter🌍 语言: Python⭐ Stars: 235 An enhanced Wan2.2 Image-to-Video node specifically designed to fix the slow-motion issue in 4-step LoRAs (like lightx2v). 10. fastgsFastGS 👤 作者: fastgs🌍 语言: NA⭐ Stars: 217 Offical code for “FastGS: Training 3D Gaussian Splatting in 100 Seconds”"},{"title":"反编译 APK","path":"/2025/03/27/decodeapk/","content":"Android逆向反编译APK工具介绍如果只是想拿到apk中的图片资源，只需要将apk后缀改为zip然后解压缩，res目录中就包含了所有的资源文件 classes.dex 则包含了所有的代码，只是还无法查看 AndroidManifest.xml 文件打开会发现无法阅读，都是16进制数 此时就需要用到工具 —— ApkTool ApkTool下载ApkTool官网 安装 使用apktool d xxx.apk d 表示 decode 还可以加上一些附加参数来控制 decode 行为： -f ：如果目标文件夹已存在，则强制删除现有文件夹（默认如果目标文件夹已存在，则解码失败） -o ：指定解码目标文件夹的名称（默认使用 APK 文件的名字来命名目标文件夹） -s ：不反编译dex文件，也就是说 classes.dex 文件会被保留（默认会将 dex 文件解码成 smali 文件） -r ：不反编译资源文件，也就是说 resources.arsc 文件会被保留（默认会将 resources.arsc 解码成具体的资源文件） 反编译之后会得到以下内容： 1、AndroidManifest.xml：经过反编译还原后的 manifest 文件 2、original 文件夹：存放了未经反编译过、原始的 AndroidManifest.xml 文件 3、res 文件夹：存放了反编译出来的所有资源 4、smali 文件夹：存放了反编译出来的所有代码，只不过格式都是.smali类型的 xml文件已经可以看懂了，不过 smali 类型文件我们依然无法阅读 此时，需要用到另一个工具 —— dex2jar + jd-gui dex2jar功能将 dex 转换成 jar 形式文件 下载dex2jar官网 使用将下载的 dex2jar 压缩包解压后，可以看到以下内容 windows上使用dex2jar.bat即可 dex2jar.bat classes.dex路径 看到上述console则表示成功 代码都位于 classes-dex2jar.jar 中 现在需要用到另一款工具 jd-gui jd-gui下载jd-gui官网 根据需要下载对应包即可 使用 解压到本地，双击jd-gui.exe文件即可运行 用jd-gui打开之前解压出来的dex文件即可看到所有的源码 jadx-gui 一个更强大的工具，一款出色的 **反编译工具 **和 代码查看器，但不能直接编辑 APK 文件或内部代码 使用 Jadx-GUI 打开一个apk文件时，它会根据 Dalvik 字节码（DEX文件）反编译成可读的 Java 源代码，然而，这些源代码只是 Jadx 根据字节码猜测出来的，并不是原始的、可变翼德Java源文件，因此，无法直接在 Jadx-GUI 中修改这些反编译出来的 Java 代码。 下载 Jadx-GUI 使用起来也很简单，打开exe文件 然后点击打开文件打开项目或者将apk文件直接拖拽过来即可查看；","tags":["Decode","安卓"],"categories":["学习","逆向工程"]},{"title":"完整front-matter字段","path":"/2024/05/21/front_matter/","content":"Hexo 内置字段 参数 描述 默认值 layout 布局 config.default_layout title 标题 文章的文件名 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 sticky 置顶（数字越大越靠前） comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章的永久链接，永久链接应该以 / 或 .html 结尾 null excerpt 纯文本的页面摘要。使用 该插件 来格式化文本 disableNunjucks 启用时禁用 Nunjucks 标签 % % 和 标签插件 的渲染功能 false lang 设置语言以覆盖 自动检测 继承自 _config.yml published 文章是否发布 对于 _posts 下的文章为 true，对于 _draft 下的文章为 false 详见：https://hexo.io/zh-cn/docs/front-matter Stellar 特有字段 参数 描述 默认值 可用版本 wiki 该页面所属的 wiki 项目 menu_id 高亮的菜单按钮 id sidebar 侧边栏配置 1.0.0 ~ 1.26.8 leftbar 左侧边栏配置 1.27.0 ~ rightbar 右侧边栏配置 1.27.0 ~ comment_title 评论区标题 poster 文章封面，包含 topicheadlinecaptioncolor 子配置 banner 页面顶部横幅背景 banner_info 横幅信息，包含 avatartitlesubtitle 子配置 logo 左侧边栏顶部 logo 区域信息，包含 iconavatartitlesubtitle 子配置 indent 段落是否缩进 false topic 所属话题专栏 1.25.0 ~ author 该文章的作者 1.23.0 ~ type 页面类型 1.26.0 ~ references 参考资料 h1 页内标题 title 1.26.0 ~ breadcrumb 面包屑导航 true indexing 页面能否是否能被搜索 true 第三方插件 参数 描述 类型 mathjax 渲染文章公式 boolean katex 同上 boolean mermaid 渲染图表 boolean","tags":["HEXO","博客","Stellar"],"categories":["博客"]},{"title":"有用的小知识","path":"/2024/03/01/justrecord/","content":"银屑病一些好用的药物他克莫司 补钙人体越越钙，吸收率越高，一般20% ~ 30%，缺钙的话可能提高到60 ~ 70，不缺钙可能会下降到10%左右（这是在正常摄入钙的情况下） 柠檬酸钙（较贵）碳酸钙（同样可以）每天补充 700mg 以上 补充氨基酸可以促进钙的吸收 所以，有复合产品 —— 氨基酸螯合钙等 钙补充多了会便秘（吸收率有限），多做运动保证吸收的钙能有效利用 多补充维生素D！！！ 最让人舒服的11种颜色RGB值和十六进制值 序号 名称 RGB 十六进制 1 豆沙绿 (199, 237, 204) #C7EDCC 2 银河白 (255, 255, 255) #FFFFFF 3 杏仁黄 (250, 249, 222) #FAF9DE 4 秋叶褐 (255, 242, 226) #FFF2E2 5 胭脂红 (253, 230, 224) #FDE6E0 6 青草绿 (227, 237, 205) #E3EDCD 7 海天蓝 (220, 226, 241) #DCE2F1 8 葛巾紫 (233, 235, 254) #E9EBFE 9 极光灰 (234, 234, 239) #EAEAEF 10 苹果绿 (183, 232, 189) #B7E8BD 11 豆沙绿-略暗 (204, 232, 207) #CCE8CF","tags":["随手一记","健康"],"categories":["生活"]},{"title":"张量索引切片操作","path":"/2024/02/21/tensor/","content":"大致按照使用频率递减给出 这里可以给出一个结论性的规律, 便于判断张量形状, 索引操作时, 有几个 : , 就有几个维度 1. 基本下标与切片（Python 风格）import torchx = torch.arange(24).reshape(2,3,4) # shape (2,3,4)# x[batch, row, col]x[0,0,0] # 标量 tensor(0)x[0, :, :] # 第一维=0，取出 shape (3,4)x[:, 1, :] # 所有batch, 第二个row - shape (2,4)x[..., 2] # 省略号，等价于 x[:, :, 2] - shape (2,3)x[0] # 等价于 x[0, :, :] - shape (3,4) 切片语法支持 start:stop:step（含 start，不含 stop），支持负数索引和步长。 例如 x[:, ::-1, :] 会在中间维度上反转顺序（返回 view 还是 copy 取决于实现；在 PyTorch 中 negative step 会返回 copy）。 2. 使用 None np.newaxis（增加维度）y = torch.tensor([1,2,3]) # shape (3,)y[None, :] # shape (1,3)y[:, None] # shape (3,1) 常用于把向量转为列行以便广播。 3. 布尔掩码（Boolean Masking）a = torch.tensor([0,5,2,7,3])mask = a 3 # tensor([False, True, False, True, False])a[mask] # tensor([5,7]) - 1D 输出，丢失原shape信息 masked_select(a, mask) 等价于 a[mask]。 当 mask 是多维且与 a 同 shape 时，结果是扁平的 1D 张量（按行主序提取元素）。 可用于筛样本、实现 padding 掩码筛选等。 4. 花式索引（整数数组索引 Advanced Indexing）M = torch.arange(12).reshape(3,4) # shape (3,4)rows = torch.tensor([2,0])cols = torch.tensor([1,3])M[rows] # 按行选择 - shape (2,4)M[:, cols] # 按列选择 - shape (3,2)# 对应元素选择（pairwise）M[rows, cols] # 取 (2,1) 和 (0,3) - shape (2,) 若使用多个 1D 整数索引（同维度），会进行逐元素配对索引，输出长度等于索引数组长度。 若混合切片和整数数组索引，规则稍复杂：整数索引会先被应用，结果维度位置会消失或变为新维度。 例（多维）： T = torch.arange(2*3*4).reshape(2,3,4)idx0 = torch.tensor([0,1]) # 用于第0维idx1 = torch.tensor([2,0]) # 用于第1维T[idx0, idx1] # 逐对索引 - shape (2,4)# 等价于: torch.stack([T[0,2], T[1,0]], dim=0) 5. 广播与索引（注意形状）A = torch.arange(6).reshape(2,3) # (2,3)idx = torch.tensor([0,2]) # (2,)A[torch.arange(2), idx] # (2,) - 每 batch 对应列取值# torch.arange(2) 为 [0,1]，与 idx 配对 - 取 (0,0) 和 (1,2) 常用于按-batch 选择每个样本对应的索引（如分类预测的 top-k 判断）。 6. gather 与 scatter（按索引收集写入，适用于高维批量操作）# gather 示例：从 src 中按 index 收集（需要指定 dim）src = torch.tensor([[10,11,12],[20,21,22]]) # shape (2,3)index = torch.tensor([[2,1,0],[0,2,1]]) # shape (2,3)torch.gather(src, dim=1, index=index)# - shape (2,3): [[12,11,10],[20,22,21]] gather 要求 index 与 src 在除了 dim 外的维度完全相同；返回与 index 同形状的张量。 常用于实现按位置取值（例如 beam-search、按预测索引从概率张量中取值）。 scatter_scatter 用于把值写入指定位置，可做 one-hot 化或累积（有 reduce 参数）。 7. index_select take（按维度选择）v = torch.tensor([10,20,30,40])torch.index_select(v, dim=0, index=torch.tensor([3,1])) # tensor([40,20])# 对于矩阵按行选：M = torch.arange(12).reshape(3,4)torch.index_select(M, dim=0, index=torch.tensor([2,0])) # shape (2,4) index_select 返回的顺序与索引一致；与 fancy indexing（M[idx]）相似，但有些后端实现行为细微不同（比如保留 contiguous 性）。 8. masked_fill, where（掩码赋值 条件选择）x = torch.tensor([1., -2., 3.])x.masked_fill(x 0, 0.) # 把负数置0torch.where(x0, x, torch.zeros_like(x)) # 条件选择，相当于 np.where where(cond, A, B) 返回与 AB 广播后的形状相同的张量。 9. unsqueeze squeeze 与 view/reshape（维度控制）a = torch.tensor([1,2,3]) # (3,)a.unsqueeze(0) # (1,3)a.unsqueeze(1) # (3,1)torch.squeeze(a.unsqueeze(0)) # 恢复 squeeze(dim) 只在指定维度为 1 时删除该维度。 reshapeview 会改变内存视图（view 要求连续 contiguous；reshape 在必要时会复制）。 10. Ellipsis ...（省略号）X = torch.randn(4,5,6,7)X[..., 0] # 等价 X[:, :, :, 0]X[0, ...] # 等价 X[0, :, :, :] 在不确定前面后面维度数时非常有用，特别是在写通用层时。 11. 多维返回与维度插入（保持丢失维度） 使用整数索引会减少维度（那一维被消除）； 使用切片或 None，或保持长度为 1 的索引会保留维度。 示例： t = torch.randn(2,3,4)t[0].shape # (3,4) -- 整数索引去掉第0维t[0:1].shape # (1,3,4) -- 切片保留第0维t[[0]].shape # (1,3,4) -- 用长度1的索引数组也保留 12. 视图（view）与 copy（内存contiguous）相关注意 大多数简单切片和整型索引会返回原张量的 view（共享内存），但有些操作会返回 copy（例如带负步长的切片、某些高级索引）。 is_contiguous() 可以检查是否连续。若对返回的张量执行 view() 可能会报错，需先 .contiguous()。 s = torch.arange(6).reshape(2,3)t = s[:, ::-1] # 可能是 copy（不连续）t.is_contiguous() # 可能 Falset.contiguous().view(-1) # 安全 在 in-place 操作（如 t += 1）时，如果 t 与原张量共享内存，可能会影响原张量；对 copy 则无影响。 13. 反向传播（autograd）相关 索引、切片会保留计算图信息（如果原张量 requires_grad=True），因此从张量中取出的部分仍可对原张量反向传播。 但是，用高级索引赋值（x[idx] = something）不记录梯度；需要使用 scatter 或构造新的张量再计算 loss。 detach() 可以切断梯度传播（例如 x = x.detach()）。 示例（反向传播影响）： x = torch.randn(3, requires_grad=True)y = x[1] * 2y.backward() # 会为 x[1] 累积梯度，但 x[0], x[2] 为 0x.grad # tensor([0., 2., 0.]) 14. 常见用途与模式（实战片段） 按 batch 取样（分类概率取预测值）： probs = torch.randn(32, 10) # logits or probspred = probs.argmax(dim=1) # (32,)# 如果想从 probs 中收集每个 batch 对应预测的概率：selected = probs[torch.arange(32), pred] # shape (32,) padding mask（seq 长短不一）： seq = torch.randint(0, 100, (4,7)) # batch, seq_lenmask = (seq != PAD_TOKEN) # True 表示有效# 通过 mask 做池化：masked_sum = (embeddings * mask.unsqueeze(-1)).sum(dim=1) one-hot： idx = torch.tensor([0,2,1])onehot = torch.nn.functional.one_hot(idx, num_classes=4) # shape (3,4) 按索引更新参数（embedding lookup 与更新）： embedding = torch.nn.Embedding(num_embeddings=1000, embedding_dim=64)out = embedding(idx_tensor) # embedding 内部就是高级索引/ gather 实现 15. 进阶：einsum 作为灵活替代（当索引和 reshape 太繁琐） einsum 可以在一次表达式中完成复杂的通道维度重新排列与约简，可替代多个 transpose + matmul 操作。 # 例：批量矩阵乘积 sum over k: c_ij = sum_k a_ik b_jktorch.einsum(ik,jk-ij, a, b) 16. 常见陷阱与建议 整数数组索引通常会返回 copy（非 view） —— 这会影响内存并且后续 in-place 修改不会影响原张量。 带负步长的切片常常产生 copy，要注意 is_contiguous()。 混合使用布尔掩码与维度不适配会报错，确保 mask 与被掩的张量形状一致或能广播。 不要对需要 autograd 的部分用原地替换（x[idx] …），会破坏计算图，使用 scatterscatter_add 或构造新张量。 索引返回的张量可能会改变梯度分配，只有被实际用到（参与 loss）的元素才会有梯度。 尽量用 gather 实现批量按位置索引（可保持形状且直观），而不是复杂的循环。 17. 一张速查小表（常用 API） 基本：x[i], x[:, j], x[..., k], x[start:stop:step] 维度：unsqueeze, squeeze, transpose, permute, reshape, view, contiguous 选择花式：index_select, take, gather, scatter, where, masked_select, masked_fill 布尔掩码：x[mask], torch.where, torch.nonzero 其他：one_hot, topk, argmax/argmin, torch.arange（用于构建 batch 索引） 如果你愿意，我可以： 把这些示例做成一个可以直接运行的 Jupyter notebook（包含针对常见错误的测试用例），或者 根据你常用的框架（PyTorch TensorFlow NumPy）把示例改写成你习惯的 API，或 针对你当前代码中的索引问题（把你代码贴来），帮你找 bug 并修正。 你想要哪种后续帮助？","tags":["张量","运算","Pytorch"],"categories":["深度学习"]},{"title":"好用的工具","path":"/2024/01/10/nicetools/","content":"VPNEFCloudhttps://www.efc123.com/shop 一元机场https://xn--4gq62f52gdss.ink 特价机场 便宜好用https://xn--6nq44rc0n82k.com ICON阿里巴巴矢量图标库https://www.iconfont.cn FONT AWESOMEhttps://fontawesome.com Carbonhttps://yesicon.app/carbon/skill-level 封面图生成BackImagehttps://nav.rdonly.com/laboratory/bgimage/backimage.html Coverhttps://cover.ruom.top 在线图片压缩TinyPNGhttps://tinypng.com 支持 PNG JPG WebP 一次可上传 20 张（单张 ≤ 5MB） 平均可减少 60–80% 体积 保留高画质，肉眼几乎无损 操作步骤： 打开网站 拖入图片 下载压缩后的版本 👉 适合 博客封面、展示图、LOGO 等日常用途。 ILoveIMGhttps://www.iloveimg.com/compress-image 支持批量上传 同时压缩 JPG PNG GIF 提供在线编辑（裁剪、加水印等） 高质量壁纸网站WallRoomhttps://wallroom.io Wallhavenhttps://wallhaven.cc","tags":["工具"],"categories":["效率"]},{"title":"小技巧","path":"/2023/12/21/someskills/","content":"如何一次性删除所有以某关键词结尾的文件 其他类型删除以此类推 find . -type f -name *.ko -exec rm -rf \\; Git push报错ssh: connect to host github.com port 22: Connection refused fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个错误表明 Git 无法通过 SSH（端口 22）连接到 GitHub。可能的原因和解决方法如下： 1. 检查 SSH 连接运行以下命令测试 SSH 连接： ssh -T git@github.com 如果连接被拒绝，可能是网络或防火墙问题。 2. 改用 HTTPS 协议如果 SSH 被屏蔽，可以临时改用 HTTPS： git remote set-url origin https://github.com/用户名/仓库名.gitgit push （需要输入 GitHub 账号密码或个人访问令牌） 3. 尝试 SSH 端口 443如果 22 端口被屏蔽，GitHub 也支持通过 443 端口使用 SSH。编辑 ~/.ssh/config 文件： Host github.comUser xxxxqq.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 然后再次测试 SSH 连接。 4. 检查防火墙代理设置 确保本地防火墙或公司网络未屏蔽 SSH（端口 22443） 如果使用代理，需配置 Git 使用代理：git config --global http.proxy http://代理地址:端口git config --global https.proxy https://代理地址:端口 5. 验证 SSH 密钥确保你的 SSH 密钥已添加到 GitHub： cat ~/.ssh/id_rsa.pub 然后将内容粘贴到 GitHub Settings → SSH and GPG keys。 6. 检查仓库是否存在确认远程仓库地址正确且存在： git remote -v","tags":["技巧","命令"],"categories":["效率"]},{"title":"一键更换所有图片格式","path":"/2023/11/10/formatimage/","content":"如题, 要求是把 旧格式的图片引用： ![](imageurl) 批量替换为 新主题格式： % image imageurl % Python实现 这种方式需要电脑中有Python环境, 虽然配置环境也比较简单, 但是还是需要一些门槛 注意修改脚本中的文件夹路径为自己的即可 import osimport re# 要处理的文件夹路径，例如 Hexo 的 source/_postsROOT_DIR = ./source/_posts# 匹配旧格式：![](URL)pattern = re.compile(r!\\[\\]\\((https://[^)]*)\\))def replace_in_file(file_path): with open(file_path, r, encoding=utf-8) as f: content = f.read() # 替换为新格式：% image URL % new_content = pattern.sub(r% image \\1 %, content) if new_content != content: with open(file_path, w, encoding=utf-8) as f: f.write(new_content) print(Updated:, file_path)def walk_dir(root): for root_dir, _, files in os.walk(root): for filename in files: if filename.endswith(.md): full_path = os.path.join(root_dir, filename) replace_in_file(full_path)if __name__ == __main__: walk_dir(ROOT_DIR) print(Done.) VSCode一键替换 VSCode还是好用啊, 相比于写脚本来实现, VSCode的全局搜索支持正则表达式, 可以一键替换文件夹中所有内容 搜索：!\\[\\]\\((https:\\/\\/[^)]*)\\) 替换：% image $1 %"},{"title":"修改键盘F9为Home键","path":"/2023/10/22/changekey/","content":"修改键盘F9键功能,将其映射为HOME键 由于键盘是75键, 本身不具备HOME键, 但是日常和工作对HOME键还是挺有需要的(别问为什么不买全键的, 个人不太喜欢数字区, 对75键构造独钟) 回归正题, 如何实现 我采用的是修改注册表方式 网上能搜到比较多方案, 除了注册表, 一般都需要下载额外软件, 键盘驱动或者是windows软件来将按键功能映射为其他 所以, 修改注册表, 省时省力 修改注册表1.打开注册表 # 按下键盘上 win + r 并输入regidit 2.定位到按键修改路径 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Keyboard Layout 3.在右侧区域新建二进制值 - 命名为 Scancode Map, 并填入下列值 00 00 00 00 00 00 00 0002 00 00 00 47 E0 43 0000 00 00 00 00 00 00 00 4.点击确定, 退出注册表, 重启电脑, 永久生效👍","tags":["方法","技巧"],"categories":["效率"]},{"title":"HEXO部署方案","path":"/2023/04/21/hexodeploy/","content":"hexo是一个无后端的博客系统, 不像wordpress, typecho可以直接线上编写文章, 需要在本地编写md文件, 然后编译成html上传到服务器, 整个过程还是比较繁琐, 所以, 想寻找hexo写作流的最佳方案 本地Hexo环境搭建安装必要软件 安装Git从Git官网下载并安装Git：https://git-scm.com/ 安装Node.js从Node.js官网下载并安装：https://nodejs.org/安装后验证： node -vnpm -v 安装和初始化Hexo 安装Hexo CLI npm install -g hexo-cli 初始化Hexo项目 hexo init myblogcd myblognpm install 本地测试 hexo server 访问 http://localhost:4000 查看博客效果。 项目结构说明myblog/├── _config.yml # 站点配置文件├── source/ # 文章和页面│ └── _posts/ # 文章目录├── themes/ # 主题目录├── scaffolds/ # 模板文件└── public/ # 生成的静态文件 服务器环境配置（无apt工具）由于你的服务器没有apt工具，推测可能是CentOSRHEL系系统，使用yum包管理器。 安装必要软件 安装Node.js # 添加NodeSource仓库（以Node.js 14.x为例）curl -sL https://rpm.nodesource.com/setup_14.x | sudo -E bash -# 安装Node.jssudo yum install -y nodejs 安装Git sudo yum install -y git 安装Nginx sudo yum install -y nginx 配置服务器Git仓库 创建Git用户和仓库 sudo useradd gitsudo passwd git # 设置密码su - gitmkdir -p ~/blog.gitcd ~/blog.gitgit init --bare 创建网站根目录 sudo mkdir -p /var/www/hexosudo chown git:git /var/www/hexosudo chmod 755 /var/www/hexo 配置Git钩子在 /home/git/blog.git/hooks/ 目录创建 post-receive 文件： # 切换到git用户su - git# 进入hooks目录cd /home/git/blog.git/hooks# 创建post-receive文件cat post-receive EOF#!/bin/bashecho 开始部署Hexo博客...# 设置路径GIT_DIR=/home/git/blog.gitWORK_TREE=/var/www/hexo# 创建网站目录（如果不存在）mkdir -p $WORK_TREE# 检出文件到网站目录while read oldrev newrev refnamedo branch=$(git rev-parse --symbolic --abbrev-ref $refname) if [ $branch = master ]; then echo 正在部署master分支到网站目录... git --work-tree=$WORK_TREE --git-dir=$GIT_DIR checkout -f $branch # 检查是否部署成功 if [ $? -eq 0 ]; then echo ✅ 博客部署成功！ echo 网站文件已更新到: $WORK_TREE # 显示部署的文件列表 echo 部署的文件: ls -la $WORK_TREE/ | head -10 else echo ❌ 部署失败！ exit 1 fi fidoneEOF 赋予执行权限： chmod +x /home/git/blog.git/hooks/post-receive# 将钩子文件的所有者改为git用户sudo chown git:git /home/git/blog.git/hooks/post-receive# 检查修正后的权限ls -la /home/git/blog.git/hooks/post-receive# 将整个blog.git目录的所有权交给git用户sudo chown -R git:git /home/git/blog.git# 修复网站目录权限sudo chown -R git:git /var/www/hexosudo chmod -R 755 /var/www/hexo 配置Nginx编辑 /etc/nginx/nginx.conf 或 /etc/nginx/conf.d/hexo.conf： server listen 80; server_name your-domain.com; # 替换为你的域名或IP root /var/www/hexo; index index.html; location / try_files $uri $uri/ =404; 重启Nginx： sudo systemctl start nginxsudo systemctl enable nginx 配置Hexo多平台部署配置GitHub Pages仓库 创建GitHub仓库 仓库名格式：用户名.github.io 设置为public仓库 配置SSH密钥 ssh-keygen -t rsa -C your-email@example.com 将公钥id_rsa.pub内容添加到GitHub的SSH keys中。 配置Hexo部署设置修改Hexo项目中的 _config.yml 文件： # 部署配置[citation:6]deploy: type: git repo: # 服务器仓库 - git@your-server-ip:/home/git/blog.git # GitHub仓库 - git@github.com:yourusername/yourusername.github.io.git branch: master 这里注意, github 上的主分支可能是 main, 可以直接在配置文件修改, 也可以部署好后进入github settings中修改主分支, 网上资料也比较多 安装部署插件npm install hexo-deployer-git --save 完整的写作和部署流程日常写作流程 创建新文章 hexo new 文章标题 编辑文章在 source/_posts/ 目录下找到对应的Markdown文件进行编辑。 本地预览 hexo clean hexo generate hexo server 部署到双平台 hexo clean hexo generate hexo deploy 访问直接访问 用户名.github.io 或者自己的域名都可以访问博客","tags":["HEXO","博客"],"categories":["博客"]},{"title":"Self Attention","path":"/wiki/note/attention.html","content":"为什么需要注意力机制 Transformer出现之前, 对于NLP任务的主力模型是: RNN LSTMGRU 这些模型都承担着一个艰难任务： 在一个长序列中，理解词与词之间的关系，尤其是远距离的关系。 但它们有一个共同的致命限制： 不能让每个词看到全局 信息被压得太狠，丢失严重 训练难以并行，效率低下 注意力机制正是为了解决这些根本性问题而诞生的。 为什么RNNLSTM处理不好全局依赖单向传递信息RNNLSTM 的信息流是链式的： x1 → x2 → x3 → x4 → ... 这意味着： x5 想知道 x1 的信息，要经过 x2 → x3 → x4 的层层传递 一旦中间丢失，就再也补不回来 越远的关系，越容易消失 所以像这种句子对它们非常困难： “我昨天看了一部电影，电影主角是….剧情非常的跌宕起伏,….它非常好看。” “它” 与 “电影” 的距离太远，RNN 容易“忘”。 梯度消失梯度爆炸链式结构导致反向传播也需要一层层回传。 传统 RNN 的隐藏状态更新公式： $$httanh(Wht−1+Uxt)$$ 反向传播时梯度会不断乘以权重矩阵 WWW 和 tanh 的导数 $$Dnσ′(z1)w1⋅σ′(z2)w2⋅⋯⋅σ′(zn)wn$$ 不难发现, 梯度的计算呈连乘形式 当序列很长时： 梯度不断变小 → 消失 或不断变大 → 爆炸 这让模型学不到长期关系。 当然,LSTMGRU 在梯度消失爆炸问题已经做了优化, 通过门控机制控制信息流, 使一部分状态以近似线性方式在序列中传播，从而避免梯度在时间维度上的指数级衰减或爆炸，解决传统 RNN 的长期依赖问题 串行计算RNN系列模型的特点: 当前时间步必须等上一时间步走完才能开始 因此, 训练速度很慢 而 Attention 可以并行计算, 一次矩阵乘法全部计算出来, 并行计算, 充分利用GPU 为什么不用CNNCNN 虽然可以并行，但存在一个无法突破的问题： CNN 的感受野是局部的。 想让 CNN 跨越长距离依赖，需要不断叠加卷积层扩展感受野。 5层感受野可能只有几十个词 要覆盖整个句子需要几十层甚至更多 训练难度非常大。 隐藏状态是对信息的强压缩RNN 把之前所有词的信息压进一个固定维度的张量： h_t = f(x_t, h_t-1) 无论你前面说了 100 个词，还是 1000 个词，最终信息都必须塞进一个固定长度的 h。 上图是seq2seq架构处理翻译任务, seq2seq架构包括三部分, encoder(编码器), decoder(解码器), 中间语义张量c 对于图中的案例, 编码器首先处理中文输入 “欢迎 来 北京”, 通过GRU模型获得每个时间步的输出张量, 最后将它们拼接成一个中间语义张量c, 接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言 decoder只能依赖固定长度的中间语义张量c 也就是说, 无论句子多长, 多复杂, 都要压缩成一个固定维度张量 结果就是: 信息丢失严重 长序列问题效果差 注意力机制如何解决这些痛点每个词直接看到所有词在注意力里，一个词无需通过第 2、3、4、5 个词才能知道第 1 个词的信息。 它可以直接算： 相关性 = Query(当前词) · Key(所有词) 这是 完全平等的全局视野。 不存在梯度消失问题因为注意力不依赖链式结构，反向传播只会经过几层矩阵运算，梯度非常稳定。 完全可并行化（压倒 RNN 的关键优势）RNN 每一步都依赖上一部 ⇒ 只能串行 Attention 所有词之间的关系都可同时计算 ⇒ 一次矩阵乘法 速度差几十倍甚至上百倍。 这是 Transformer 能训练巨型模型的根本原因。 注意力机制如何计算 这里简要概括, 详细过程推荐知乎猛猿的一篇文章, 这里给出链接 Transformer学习笔记二：Self-Attention（自注意力机制）https://zhuanlan.zhihu.com/p/455399791 Transformer 使用的是缩放点积注意力(Scaled Dot-Prodcut Attention) 对于每个token, 产生三个矩阵 Query, Key, Value(下文简称 Q, K, V) Q（Query）：我想找什么？ K（Key）：我有哪些信息？ V（Value）：对应的信息内容 $$\\text{Attention}(Q, K, V) \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V$$ (1) Q 与 K 做点积：计算“匹配程度”score = Q • Kᵀ 相似度越高 → 代表 Query 认为这个 Key 更相关。 (2) 缩放（除以 √d）防止数值过大score = score / √d (3) Softmax → 转成概率分布（注意力权重）attention_weights = softmax(score) (4) 用权重加权求和 Value，得到最终输出output = attention_weights × V 自注意力机制对于一般情况下, Query来自decoder, Key, Value来自encoder 自注意力则是一种特殊情况, Query, Key, Value均来自同一个序列, 即 $$QXW^Q,KXW^K,VXW^V$$","tags":[null,null],"categories":[null]},{"title":"追星Transformer","path":"/wiki/note/index.html","content":"O1 搭建知识库 正常 72% KR1 添加《初始Transformer》 对Transformer的大致了解 已完成 100% KR2 添加《Self Attention》 关于Transformer中的注意力机制，自注意力、多头注意力… 正常 90% KR3 添加《Position Encoding》 关于Transformer中的位置编码实现 已完成 100% KR4 关于Transformer模型家族，Bert、GPT等 未完成 0%"},{"title":"Positional Encoding","path":"/wiki/note/posencoding.html","content":"首先抛出一个问题, 为什么Transformer 要有位置编码? – 对于任何一门语言, 单词在句子中的位置和顺序都极为重要, 语序正确, 才能表达出正确含义 I do not like the story of the movie, but I do like the cast.I do like the story of the movie, but I do not like the cast. 如上述两个句子, 仅是改变not位置, 表达的意思截然相反 – Transformer抛弃了RNN和CNN作为序列学习的基本模型, 我们知道, 循环神经网络本身就是一种顺序结构, 天然包含了词在序列中的位置信息, 当抛弃循环神经网络结构, 完全采用Attention取而代之, 这些词序信息就会丢失, 模型就没有办法直到每个词在句子中的相对位置和绝对位置, 因此, 有必要把词序信息加到词向量上帮助模型学习这些信息, 位置编码(Positional Encoding)就是用来解决这个问题的办法 这里给出huggingface上的的一篇文章, 对于位置编码的讲解非常清晰透彻 设计位置编码https://huggingface.co/blog/zh/designing-positional-encoding 一、什么是位置编码 在transformer的encoder和decoder中使用了Positional Encoding, 最终的输入就变为了: input word_embedding + positional_encoding word_embedding : 词嵌入, 将token的维度从vocab_size映射到d_model(原论文中, d_model为512) 最终的输入通过词嵌入后的向量和位置编码矩阵相加得到, 所以positional_encoding也是d_model维度的向量 二、位置编码构造方法2.1 用增长数值标记位置 第一个token标记1, 第二个token标记2… 以此类推 这种方法存在一些问题: 模型可能遇见比训练时所用序列更长的序列, 不利于模型泛化 模型无法理解数字的含义, 可能认为大数字比小数字权重更高, 而非顺序更靠后 无法表达相对位置信息 2.2 用[0, 1]范围标记位置为解决上述问题, 将数值限制在[0, 1]区间内, 对其等分切割, 假设有3个token, 则位置信息为[0, 0.5, 1] 这样产生的问题是, 序列长度不同, 相对距离也不同, 模型可能认为这是单词语义发生了变化 2.3 用二进制向量标记位置 由于位置信息最终会作用到word_embedding上, 比起用单一的数值, 更好的方式是使用和word_embedding同纬度的向量, 可以想到, 将数值转换成二进制形式 假设d_model 3 d_model一般比较大(512, 1024…), 基本可以把每个token的位置都编码出来 但这样也存在问题, 不同单词间的位置变化不连续, 难以推测相对位置信息 2.4 用sin和cos函数交替标记位置 这是论文中提出的方法 三角函数有界且连续, 可以满足目前的需要 $$PE_{(pos, 2i)} \\sin\\left(\\frac{pos}{10000^{2id_{model}}}\\right)$$ $$PE_{(pos, 2i+1)} \\cos\\left(\\frac{pos}{10000^{2id_{model}}}\\right)$$ 参数说明: 符号 含义 pos 当前位置（position index） i 维度索引（dimension index） d_model 模型维度（embedding size，例如512） PE 位置编码矩阵（shape: [seq_len, d_model]） 这里只给出计算公式, 数学推导网上优秀文章较多, 不再演示 三. 为什么可以直接将词向量和位置编码进行相加 这是一直困扰我的点, 仅是进行张量相加就可以达到添加位置信息的作用吗? 难道这样不会破坏词向量本身信息吗? 网上浏览了一些博主的文章和视频, 但是答案也并没那么明确, 有点大力出奇迹的意思 3.1、公式复习Transformer 的输入是：$$X E_{\\text{word}} + E_{\\text{pos}}$$其中： $E_{word}$：词向量（表示词语语义） $E_{pos}$：位置编码（表示词语在句子中的位置） 维度相同（例如都为 $d_{model}512$） 3.2、为什么相加就能引入位置信息？1. 向量加法在嵌入空间中表示“组合信息”在向量空间中，加法表示叠加不同的语义因素。 比如： “king” ≈ “man” + “royalty” “Paris” - “France” + “Italy” ≈ “Rome” 同理： 把词向量和位置向量相加，就相当于在语义中叠加“第几个词”的信息。 也就是说，新的向量同时携带了词义和位置两种特征。 2. 相加不会“覆盖”信息，而是“偏移”语义空间我们可以把每个词向量看成在一个高维语义空间中的点。 加上位置编码后，等价于把它 沿着“位置维度方向”平移了一点： $$\\text{new_embedding semantic_embedding + position_offset}$$ 所以模型看到的不是“词义被破坏”，而是“相同词在不同位置处于略微不同的方向”。 这使得 Transformer 能区分： “I love you”和“You love I” 的区别。 3. 注意力机制会自动学会利用这些位置差异在自注意力计算中，Q、K 向量都会从输入中线性变换得到： $$Q XW_Q, \\quad K XW_K$$因此，位置信息通过 $E_{pos}$ 影响了 Query-Key 相似度。 当两个词位置不同、位置编码不同，它们的注意力权重分布也不同 → 模型能够学习“前后关系”。 3.3、为什么不会破坏词向量原始信息？1. 相加是线性可逆的（在一定程度上）如果两个向量空间的分布相对独立，线性相加仍能被后续层（线性变换）分离： $$W(E_{word}+E_{pos})WE_{word}+WE_{pos}$$Transformer 的多层线性结构可以在后续层中重新分辨“语义部分”和“位置信息部分”。 2. 位置编码的幅度较小，不会淹没词向量在实现时，位置编码通常被归一化或初始化为较小值（例如在 [-1,1] 范围内）。 这样词向量的主导语义仍然保留，只是轻微偏移 → 注入位置信息。 3. 高维空间中的“信息容量”巨大在 512 维或更高的空间中，向量叠加后不会导致严重的信息重叠。 就像 RGB 图像里混合红色和绿色还能区分出“黄色”一样， 词义与位置信息叠加后仍可被网络区分和提取。","tags":[null,null],"categories":[null]},{"title":"编码器","path":"/wiki/note/transformer-encoder.html","content":"","tags":[null,null],"categories":[null]},{"title":"知识学爆","path":"/wiki/tutorial/index.html","content":"知识回顾工具这么多，命令这么繁，一段时间不用就会忘记，这很正常，需要用时来翻翻。扳布"},{"title":"doccano数据标注平台","path":"/wiki/tutorial/doccano.html","content":"Doccano平台介绍 Doccano是documment anotation的缩写，是一个开源的文本标注工具，我们可以用它为NLP任务的语料库进行打标。它支持情感分析，命名实体识别，文本摘要等任务。 它的操作非常便捷，在小型语料库上，只要数小时就能完成全部的打标工作。下面介绍一下如何安装、配置和使用doccano。 一、doccano的安装 doccano的安装要在虚拟环境下进行！ 进入你用到的虚拟环境中, 安装doccano pip install doccano 然后，在终端里输入 # 初始化数据库doccano init# 创建一个super user。这里要把pass改成你需要的密码。当然，用户名也可以改成别的。doccano createuser --username admin --password 1234 二、启动doccano首先，在终端中运行下面的代码来启动WebServer # 启动webserverdoccano webserver --port 8000 然后，打开另一个终端，运行下面的代码启动—任务队列: # 启动任务队列doccano task 三、运行doccano与创建新的文本打标项目 首先，打开浏览器（最好是Chrome），在地址栏中输入http://127.0.0.1:8000/并回车。 然后，我们点击中间的蓝色按钮“快速开始”。此时，我们会跳转到登陆的界面。这里，我们需要用之前创建的超级用户登陆。 完成登陆后，我们会来到“项目”的界面。我们可以点击左上角的“创建”按钮来创建新的项目；也可以点击“删除”按钮来删除已经创建的项目。 我们点击左上角的“创建”按钮，创建一个新的项目。 对项目进行基本的说明，在填写完要求的信息后，点击创建，我们就创建了一个新的NLP标注项目。在创建完成后，会自动跳转到项目的主页。 进入数据标注平台 最左侧是一系列可以选择的页面。“主页”这个标签下面是doccano提供的一系列教程，其他的页面可以对项目进行设置。 稍后，我们将在上图所示的界面中的完成文本打标项目的各项设置。我们会依次点击左侧的各个标签，依次进行设置。 四、添加语料库 我们直接从“数据集”这个标签开始看。 在“数据集”这个页面，我们可以将准备好的文本添加到项目中，为将来的打标做准备。 我们首先点击左上角的“操作”→“导入数据集“。 此时，我们会来到”上传数据”的界面。 如上图所示，doccano总共支持4种格式的文本，他们的区别如下： Textfile：要求上传的文件为txt格式，并且在打标的时候，一整个txt文件在打标的时候显示为一页内容； Textline：要求上传的文件为txt格式，并且在打标的时候，该txt文件的一行文字会在打标的时候显示为一页内容； JSONL：是JSON Lines的简写，每行是一个有效的JSON值。 CoNLL：是“中文依存语料库”，是根据句子的依存结构而建立的树库。其中，依存结构描述的是句子中词与词之间直接的句法关系。具体介绍看汉语树库。 注意： doccano官方推荐的文档编码格式为UTF-8。 在使用JSONL格式的时候，文字数据本身要符合JSON格式的规范。 数据集中不要包含空行。 这里我们以Textline格式举例。点击“TextLine格式”。然后在跳转到的界面里，设置File Format和Encoding。然后点击下图中的“Drop files here…”来上传文件，最后，点击导入即可。 此时，再点击“数据集”的标签，我们就可以看到一条一条的文本已经被添加到项目中了。将来我们将对这些文本进行打标。 五、添加标签 在这一部分，我们讲解如何往项目中添加在打标时可选的标签。 在NER任务中，我们可能会添加People、Location、Company等；在文本分类任务中，我们可能会添加Positive、Negative等标签作为打标时的可选标签。 注意，这里只是添加将来可供选择的标签，是项目配置的过程，而不是进行文本标注。 点击左侧的“标签”按钮，就来到了添加标签的界面。 继续点击“操作”按钮，并在下拉菜单中点击“创建标签”按钮。 在弹出的“创建标签”窗口里面，在标签名一栏写上标签的名字，键值代表快捷键。例如在NER的例子中，可以写People、Location、Company等。例如，我们给People设置的快捷键是p。将来在打标的时候，右手用鼠标选中段落中的文字（例如“白居易”），左手在键盘按下快捷键p，就可以把被选中的文字打标成“People”。 再往下，我们可以给标签自定义颜色。 全部设置好以后，点击右下角的“保存”按钮。 此时，一个标签就添加完成了。我们以同样的方法添加其他所需要的标签。 六、添加成员 在为机器学习的语料库打标的时候，由于语料库一般比较大，如果让一个人给所有的文本打标的话，那到地老天荒都完不成。因此，我们需要多个人协同完成语料库的打标工作。 回忆一下，此时我们的项目还只有一个成员，也就是在初始配置doccano的时候创建的超级用户admin。因此，为了让其他人参与到打标项目中来，我们首先需要为其他成员创建账户。 我们打开网页http://127.0.0.1:8000/admin/，来到数据库的管理系统页面Django administration，并用超级用户的账号密码登陆该管理系统。 此时，我们再返回项目的设置页面。点击左侧的“成员”标签，点击页面上的“添加”按钮，会弹出“添加成员”窗口。 其中，在“用户搜索接口”的下拉菜单里面可以找到我们刚添加的用户“小明”。 注意，在这里只能找到已经创建到的用户，而不能创建新的用户。如果要新建用户，必须要到前面Django administration界面。 同时，我们还可以设置不同的成员的角色，不同的角色对应着不同的权限。如下图，我们把小明设置为“标注员”。其他角色还有项目管理员和审查员。 七、添加标注指南 我们可以事前给标注员和审查员准备一些标注指南，便于项目成员理解我们标注的要求和注意点。 例如，在判断文本正负面倾向的文本分类任务中，我们要具体说明判断正负面的标准，例如满足哪些要求，我们就可以认为一个本文是正面的。 因为一万个读者眼里有一万个哈姆雷特，不同人对文本的理解和判断正负面的尺度是不一样的。我们只有把标准写具体、写明确了，让人不用动脑筋都能做出符合我们要求的判断，我们才能得到一个尺度统一的数据集。数据集上的打标尺度统一，是机器学习获得好的效果的前提。 添加指南的界面如下图所示。 八、开始给文本打标 准备工作忙活了老半天，终于可以进入正题了—-给文本打标。 需要注意的是，上面的前期设置里面并不是所有的都是必须的。在最精简的情况下，我们可以在仅添加了数据集与标签后，就开始给文本打标。 这里，我们用标注员小明的账号登陆打标系统做演示。同样是打开http://127.0.0.1:8000/地址，输入小明的账号密码登陆。 和之前不一样的是，由于小明的角色是“标注员”，因此他只有打标的权限，没有对项目进行各项设置的权限，所以在左侧列表没有管理员用户的各项设置项目。 这里我们直接点击左上角的“开始标注”进行打标。 以NER任务为例，在打标的界面下，我们选中句子中的实体，会自动弹出一个下拉菜单，我们可以从这个下拉菜单中选择相应的实体类型People，也可以直接在键盘上按下p键。 这是添加标签之后的样子。 九、导出打标结果 当我们要导出标注结果的时候，我们重新用管理员用户登陆，在“数据集”页面下，点击“操作”→“导出数据集”。 在弹出的窗口中，根据我们的需要进行设置后，点击Export，即可导出标注结果。 保存好的文本是字典的格式。 如下图所示，保存了句子的ID、句子原文、实体的在句子中的位置、实体的类型。","tags":[null,null,null],"categories":[null]},{"title":"认识Transformer","path":"/wiki/note/knew-transformer.html","content":"Transformer：深度学习时代的分水岭过去十年，深度学习几乎重塑了整个人工智能领域。从图像识别到语音识别，再到自然语言处理（NLP），我们见证了一个又一个模型的进化。而在 NLP 领域，真正改变游戏规则的模型，毫无疑问是 Transformer。 这篇文章将带你从零理解 Transformer： 它为什么出现？解决了什么问题？内部结构如何运作？为什么它能成为 GPT、BERT 等大模型的基础？ 一、Transformer 的出现：时代的需要在 Transformer 出现之前，NLP 的主流模型是： RNN（循环神经网络） LSTM GRU（改进版 RNN） CNN 文本卷积模型 虽然这些模型在当时取得了不错的效果，但也遭遇了多个根本性瓶颈： 1. 序列依赖太强，难以并行RNN 每一步都依赖前一步： h(t) - h(t+1) - h(t+2) ... 这意味着： 训练无法并行化 速度慢 对长序列不友好（梯度消失爆炸问题） 2. 长距离依赖难以建模例如句子： “我昨天看了一部电影，我觉得它非常好看。” 这里“它”指代“电影”，人类很自然，但 RNN 需要跨越很多词才能关联，效果不稳定。 3. CNN 虽可并行，但感受野有限必须堆叠很多层卷积才能覆盖长距离上下文，效率仍然不高。 二、Transformer：新的范式从此诞生2017 年，Google 在论文 《Attention Is All You Need》 中提出： “序列建模根本不需要 RNN，也不需要 CNN，只需要 Attention（注意力）。” 这是一个革命性的观点。 Transformer 完全抛弃 RNN 的递归结构，转而只使用： → Self-Attention（自注意力机制）→ Position Encoding（位置编码）→ 多头注意力、多层堆叠（Multi-Head, Stacked Layers）最核心的思想是： “在序列中，每个词都能直接和其他所有词建立联系，并根据相关性动态分配权重。” 这彻底解决了 RNN 的所有历史问题。 三、Transformer 为什么如此强大？总结它的优势可以一句话概括： 更快、更准、能看全局。 下面逐点解释。 1. 训练可以完全并行化（突破性进步）自注意力机制让所有词之间的关系可以一次性计算： 所有词 ↔ 所有词 不再像 RNN 一步步传递。 这带来： 训练速度巨大提升 能利用 GPU 的矩阵计算优势 能训练更大的模型 2. 天然具备“全局视野”自注意力机制计算每个词对所有词的重要性，例如： Attention(它, 电影) → 高权重Attention(它, 昨天) → 低权重 模型能自动找到长距离依赖关系。 3. 表达能力更强Transformer 使用 多头注意力（Multi-Head Attention）： 一个头关注语义信息 一个头关注句法结构 一个头关注代词关系 …… 模型可以从多个角度“看”句子，让表示更加丰富。 4. 更容易扩展、堆叠和并行Transformer 模块化结构清晰，可以堆叠几十层甚至上百层。 这为后续的模型奠定了基础： BERT（2018） GPT 系列（2018 – 2024） ViT 图像 Transformer（2021） ChatGPT、Claude、Gemini 等顶级大模型 都源自 Transformer。 四、Transformer 内部结构概览 Transformer 包含 Encoder（编码器） 和 Decoder（解码器） 两个部分，但现代大多数模型只使用其中一部分： 模型 使用结构 BERT Encoder-only GPT Decoder-only T5 FLAN Encoder-Decoder 原始架构中，一个 Transformer 层包含： 1. Multi-Head Self-Attention（多头自注意力）核心计算公式： Attention(Q, K, V) = softmax(QKᵀ / sqrt(d_k)) V 实现“词与词之间相关性”的计算。 2. Feed Forward Network（位置前馈网络）对每个词的表示做非线性变换，提升表达能力。 3. Add Norm（残差连接 + LayerNorm）为了解决深层网络训练困难问题。 4. Position Encoding（位置编码）因为 Transformer 不再使用 RNNCNN，序列没有位置顺序，因此需要位置编码告诉模型： 谁是第一个词 谁是第二个词 谁更靠后 位置编码通常用正弦余弦函数构造。 五、Transformer 之后：新时代的大模型浪潮Transformer 诞生后，NLP 领域彻底变天。 2018：BERT 出现 彻底改变 NLP 预训练范式 情感分析、命名实体识别等任务效果暴涨 2018~2020：GPT 系列 GPT-1 → GPT-2 → GPT-3 语言生成能力大幅提升 LLM（大语言模型）时代开启 2021：Vision Transformer（ViT）Transformer 模型跨界视觉，效果强于 CNN 2022~2025：ChatGPT 时代Transformer 模型成为 AGI 的基础框架 世界开始感受 AI 的力量","tags":[null],"categories":[null]}]