[{"title":"Agent Skills","path":"/2026/01/03/Skill/","content":"实践 以下只是一个最简单的skill使用，还有一些额外功能比如script、reference等未进行扩展，有兴趣可以自行搜索 第一步：在正确的目录创建Skill文件夹Claude Code会在特定目录寻找技能。你需要在这个目录下工作。 打开你系统的“技能根目录”： 在Windows资源管理器的地址栏，直接输入以下路径并回车： %USERPROFILE%\\.claude\\skills\\ 系统会定位到类似 C:\\Users\\[你的用户名]\\.claude\\skills\\ 的目录。 注意，我是windows系统，所以一般会在这个目录，mac可以直接按照claude官方文档操作 创建你的技能文件夹： 在 skills （如果不存在的话创建一个skill）文件夹内，新建一个文件夹，并以你的技能名命名（建议使用小写字母和连字符，例如 meeting-summary）。 这是你的技能目录，所有相关文件都将放在这里。 第二步：创建核心文件 SKILL.md这是技能的灵魂，是一个包含 YAML头部（元数据） 和 Markdown正文（指令） 的文本文件。 在 meeting-summary 文件夹内，新建一个文本文档。 将其重命名为 SKILL.md （注意后缀是 .md）。 用记事本或VS Code等编辑器打开它，并复制并修改以下模板： ---name: meeting-summarizerdescription: 快速会议总结助手。当我收到会议记录、对话转录或聊天文本时，自动生成包含关键决策、行动项和要点的结构化总结。version: 1.0.0---# 快速会议总结助手## 我的专长我是一个会议内容分析专家。你只需把会议记录粘贴给我，我就能**自动提取核心信息**，生成清晰可执行的总结，包含：1. **关键决策** - 会上达成了哪些共识和决定2. **行动项** - 谁需要做什么，何时完成（我会用表格整理）3. **讨论要点** - 主要讨论了哪些话题4. **参与人员** - 自动识别参会者（如果文本中包含）## 我的工作方式当你把会议文本发给我时，我会：### 第一步：分析文本结构- 识别不同的发言人和对话轮次- 标记出决策性语句（如我们决定、同意、通过）- 找出任务分配表述（如由XX负责、需要完成）### 第二步：提取关键信息我会重点关注以下内容：- **任务描述**：需要完成的具体工作- **责任人**：明确提到的负责人- **时间节点**：截止日期、完成时间- **决策点**：达成的共识和结论- **待解决问题**：需要进一步讨论的事项### 第三步：生成结构化总结我将按照以下模板组织信息：```markdown## 📋 会议总结**分析时间**：当前日期### 👥 参会人员（自动识别）- 人员列表（从文本中提取）### ✅ 达成的决策1. [决策点1]2. [决策点2]### 📝 行动项追踪| 任务描述 | 负责人 | 截止时间 | 状态 ||---------|--------|----------|------|| [任务1] | [人员] | [时间] | 待开始 || [任务2] | [人员] | [时间] | 待开始 |### 💡 讨论要点- [要点1]- [要点2]### 🔍 后续关注- [待解决问题]- [下一步安排] 关键点： name 必须与文件夹名严格一致。 description 要清晰，这决定了Claude何时会调用此技能。 正文部分是你的“AI说明书”，写得越详细、步骤越清晰，AI表现就越好。 第三步：验证skill是否生效简单一点，直接打开一个窗口，对claude进行询问，比如： 你有哪些Agent Skill 可以看到，claude能正确检测到我们刚刚创建的skill并且能正确识别它的功能 第四步：使用案例测试同样询问claude： 帮我总结以下会议内容：王峰：开始会议。今天评审用户画像模块进度。李静同步技术进展？李静：后端API完成90%，但发现性能问题：数据量超百万时查询超3秒，超SLA要求。张涛：前端个人中心重构基本完成，可视化组件周三提测。但需要后端新接口获取标签权重。王峰：新接口要多久？李静：两天可完成，需详细需求。陈敏：市场部最需要“购买意向预测”标签，能按时上线吗？李静：该模块需增加两周。建议先上线准确率70%的基础版，完整版4月中旬交付。王峰：决策：基础版按原计划3月25日上线，完整版延期到4月15日。同意吗？（众人同意）赵琳：测试基础版的范围？要两套用例吗？李静：一套即可。测核心5个预测维度。王峰：赵琳负责周四前输出测试计划。李静解决性能问题方案？李静：方案一优化索引（需张涛配合，3天）；方案二加缓存层（5天）。建议先用方案一。张涛：可配合，但排期到下周一后。王峰：定案：采用方案一，李静主导，张涛下周一配合。陈敏同步市场部版本安排。陈敏：明天上午同步市场部。赵琳：风险：测试环境无百万级数据，性能测试可能不充分。李静：我周三前生成模拟数据解决。王峰：明确行动项：1李静周三前提供测试数据，周四前完成接口扩展；2张涛周三前提测组件，下周一配合优化；3赵琳周四前输出测试计划；4陈敏明天同步市场部；5我更新项目计划。散会！ claude会先向你请求调用会议总结skill工具： 回车同意 claude就会按照我们设置好的规则生成会议总结 渐进式披露机制这个是Agent Skill的核心 对于reference，可以理解为外部扩展文档，作为对补充上下文，让模型按需去读取，当然，这些需要自己在SKILL.md中说明好 对于script，也就是脚本，可以理解为function call，让模型在执行某些操作时可以去调用脚本工具来完成，比如上传文件到服务器，这个脚本是我们写好的，同样也需要在SKILL.md中说明好，一般来说，script只会被调用，不会被读取，这也达到了节省上下文目的 总结什么是Agent SkillAgent Skill 的本质：AI 的「可插拔专业技能模块」 1. 核心定义Agent Skill 是一个 标准化、可复用、自描述的AI能力包，它让大语言模型（如Claude）在需要时，能像专业人士一样执行特定领域的复杂任务。 2. 三层本质解读 维度 技术视角 用户视角 生态视角 是什么 结构化提示词+上下文+工具的封装 AI的「技能插件」或「专业顾问」 开放的AI能力交换单元 表现形式 文件夹（含SKILL.md、脚本、参考资料） 对话中激活的「专家模式」 可分发、可共享的资产包 价值核心 将非结构化知识转为可执行指令 让AI「学会」你的工作方式 构建协作式AI生态的基础 为什么 Claude 要设计 Agent Skills？1. 解决大模型的核心局限性 问题 传统方式 Agent Skill 方案 语境长度限制 长文档需反复粘贴 通过reference按需加载 任务一致性差 每次需重新解释需求 固化最佳实践到工作流 缺乏专业知识 依赖模型的训练数据 注入领域特定知识 无法执行操作 只能给出建议 集成脚本执行实际任务 2. Claude 的战略布局思考技术战略：从「对话模型」到「行动智能体」text Claude 1.0 → 2.0 → 3.0 的演进路径：文本理解 → 复杂推理 → 多模态 → 【行动执行】Agent Skills 是「行动执行」的关键载体 产品战略：构建开发者生态 降低定制门槛：让非开发者也能为AI「编程」 促进生态贡献：用户创造的技能可共享，形成网络效应 防止碎片化：标准化格式避免每家公司的「私有技能格式」","tags":["Skills"],"categories":["Agent"]},{"title":"第一性原理","path":"/2025/12/28/firstprinciples/","content":"什么是第一性原理先给这个名词下一个通俗易懂的定义 可以把“第一性原理”想象成思想的“归零”与“重建”。它是一种哲学与实践相结合的根本性思维方式：撇开一切现成的假设、惯例和表象，直接回归到事物最基础、最不可还原的事实或命题，然后从这些坚实的“地基”出发，逻辑严密地向上推导和构建。它的核心精神是：不模仿，只追问“在根本上，它究竟是什么？” 归纳法、演绎法和第一性原理归纳法在古希腊时期，归纳法就已经被诸多哲学家所使用，它本质上是一种基于经验的推理方式。这种方法要求我们运用已有的知识和观察，通过系统性的分析和总结，形成新的认知结构，并将其内化为我们思维的一部分。归纳法的核心在于从特殊到一般，从个别现象推导出普遍规律。 但它的脆弱性在于，即使经过大量观察得出的结论，也可能被单一反例推翻。例如，在澳大利亚发现的黑天鹅就是-个著名的反例，它瞬间颠覆了”所有天鹅皆白”的普遍性结论。这一发现不仅挑战了特定的结论，更凸显了归纳推理在本质上的不确定性。完整的例子是这样的: 当我们多次观察到所有遇到的天鹅都是白色的，我们可能会得出”所有天鹅皆为白色”的结论。但当你见到”黑天鹅“时，你瞬间就崩溃了。因为这超出了你的“经验之谈”。 这个过程展示了归纳推理的典型特征:基于有限的观察样本，推导出看似普遍适用的结论。然而，这种基于经验的推理也暴露了归纳法的局限性:它依赖于观察的范围和样本的代表性，而非逻辑上的必然性。 演绎法当归纳法的局限性浮出水面时，演绎法则为我们探寻真理开辟了另一条途径。演绎法是一种严谨而精密的逻辑推理过程，由大前提、小前提和结论三个要素构筑而成，这便是著名的三段论结构。 让我们以古希腊哲学家亚里士多德对其恩师苏格拉底命运的思考为例，来阐释演绎法的精髓。亚里士多德的推理过程堪称演绎法的典范: 大前提:所有人类都终将面对死亡。小前提:苏格拉底是人类的一员。结论:因此，苏格拉底也无法逃脱死亡的命运。 演绎法的魅力在于，只要前提成立，其结论就具有逻辑上的必然性。这种必然性赋予了演绎推理极高的可靠度，使之成为科学研究、哲学探讨和日常推理中不可或缺的工具。然而，我们也需要认识到，演绎法的有效性高度依赖于前提的真实性。如果前提本身存在错误或偏差(使用归纳法产生了偏见)，即便推理过程再严密，得出的结论也可能偏离事实。比如，如果我们修改上述例子的大前提为”所有哲学家都长生不老”，那么即使推理过程依然严谨，得出的”苏格拉底长生不老”的结论显然与现实不符。 第一性原理在这种情况下，亚里士多德给出的解决办法是：我们不可能这样无限重复的推到下去，而是要有一个前提，即不是从更早的三段论推导出来，但它是一个无需证明且必然为真的元起点，这个元起点就叫“第一性原理”。 第一性原理是要在特定领域中最基本、不可再简化的命题或假设。它要求我们追溯到最基本的事实和原则，从根本上检验和确立我们的前提，以确保推理的准确性和结论的有效性。 从哲学家的思辨到企业家的工具1. 哲学的起源：亚里士多德的“第一哲学”这个概念最早由古希腊哲学家亚里士多德提出。他将“第一原理”（First Principles）定义为 “认知系统中必然的、不证自明的基本假设”。 他的追问是：在所有存在的事物背后，有没有一个最根本的、不依赖于其他原因的原因？一个最初始的、无法被继续分解的起点？ 他的答案是：有。例如，他认为任何事物的变化都基于一些最基本的公理（如“同一律”：A就是A），以及物质与形式等根本范畴。这就是为一切知识寻找“第一因”和“第一前提”的努力。 2. 近代科学的实践：还原论与演绎法第一性原理思维在科学革命中大放异彩，典型代表是笛卡尔的“普遍怀疑”和牛顿的物理学。 笛卡尔说，我要怀疑一切能被怀疑的东西（感官经验、传统学说），直到找到一个绝对坚实、不可怀疑的基点。他找到了——“我思故我在”。从这个逻辑起点，他开始重建知识体系。 牛顿的经典力学，就是从几个最基本的定律（如惯性定律、F=ma）出发，通过数学演绎，推导出整个宏大的力学世界。这些定律就是他那套体系中的“第一性原理”。 3. 当代商业与创新的引擎：埃隆·马斯克的诠释马斯克让这个概念广为人知。他以制造火箭和电动车为例： 传统类比思维：火箭发射成本极高，因为过去的部件都很贵，所以未来的火箭也会很贵。这是基于“历史经验”的推理。 第一性原理思维：马斯克问：火箭是由什么构成的？——航天级铝合金、钛、铜、碳纤维等。这些原材料在商品市场上的价值是多少？——远低于成品火箭的价格。那么问题就从“为什么火箭这么贵”变成了 “我们能否用这些基础材料，以新的方式（比如可回收）重新设计和制造火箭？”。这就是 SpaceX 低成本火箭的起点。 马斯克的核心方法论是： 识别并打破所有现有假设（“火箭本来就该贵”）。 分解到最基本的构成要素（物理和材料层面）。 从这些要素出发，寻找更优的重新组合路径。 如何运用第一性原理思维：一个实践框架 识别与定义问题：明确你要解决的根本问题是什么？把它写下来。 解构与挑战既有假设：列出所有关于这个问题的“理所当然”。例如：“做这件事必须要有X”、“用户肯定需要Y”、“成本不可能低于Z”。大胆地问：“为什么必须这样？这是真的吗？” 追溯至基本原理：将问题分解到无法再分解的基本要素。这些要素通常是： 科学原理（物理的、化学的、生物的定律）。 核心事实与数据（原材料成本、时间、基本需求）。 逻辑公理（不证自明的前提）。 从零开始重建：仅基于这些基本原理和事实，像一个“外星智者”一样，重新构思解决方案。这个过程需要大量的创造性思考和逻辑推导。 检验与迭代：将新建构的方案与现实进行对照测试，不断修正。 融合到Prompt设计中一、核心理念：从“模仿样例”到“重构任务本源”很多Prompt设计停留在“类比思维”：看到别人写“请扮演一个专家…”，我也这么写；给几个例子（Few-Shot），希望模型模仿。这有效，但天花板低，且不稳定。 第一性原理思维要求我们：撇开所有现有的Prompt模板和惯例，直接追问： 任务在模型的能力坐标系中，本质上是什么？ 模型达成这个目标的最根本、最底层的认知路径应该是什么？ 我如何用Prompt，为模型铺设这条最理想的路径？ 二、实践框架：四步分解与重建法第一步：解构任务——剥离“业务表述”，还原“认知动作”不要一上来就想“怎么问模型”。先彻底分析你的业务任务。 传统（类比）思维：“我要一个商品推荐系统Prompt。” 第一性原理思维： 分解：所谓的“商品推荐”，在认知层面，是让模型完成哪些子动作？ 理解：精准理解用户查询的深层意图与偏好（是追求性价比、新奇性、还是品质感？）。 检索/回忆：从上下文（提供的商品列表）或内部知识中，找出相关商品。 评估与排序：根据多维、且常常相互冲突的准则（价格、评分、风格匹配度、库存）进行权衡。 决策与解释：做出选择，并给出令人信服的理由。 挑战假设： 假设1：“模型自己知道如何权衡权重。”——不成立。必须显式定义或引导。 假设2：“给出列表，模型就能选出最好的。”——过于笼统。“最好”的标准是什么？ 假设3：“理由最后生成就行。”——可能低效。让推理过程前置，能稳定结果。 第二步：追溯至模型的基本原理——理解LLM的“原子能力”LLM的根本原理是基于上文，以概率方式预测下一个词（Token）。所有复杂能力都由此涌现。你的Prompt是在为模型构建“最优的上文”。 核心原理： 模式匹配与补全：LLM是超级模式匹配器。你的Prompt就是在激活它训练中学到的相关模式。 指令跟随与角色扮演：经过指令微调（SFT）的模型，会将指令本身作为强模式来遵循。 逐步推理（Chain-of-Thought， CoT）：当要求模型“逐步思考”时，其实是迫使它将内部隐含的推理链显式化为文本，这个文本又作为上文，引导后续更准确的输出。这是最重要的可控杠杆之一。 计算与决策：LLM不擅长精确计算，但擅长规划计算步骤、调用工具（如代码解释器）、或进行定性比较。 第三步：从零重建Prompt——设计“认知脚手架”基于前两步，不再写“请推荐商品”，而是为模型设计完成那些“认知动作”的最佳工作流。 **重构后的Prompt示例框架： 你是一个专业的购物顾问。你的任务是根据用户请求，从商品列表中选出最合适的推荐。请严格按以下步骤执行：**步骤一：意图解析**- 分析用户查询“用户查询”中明确提及和隐含的需求。- 列出所有关键偏好维度（如：价格区间、品牌倾向、功能重点、风格等）。**步骤二：商品初筛**- 根据步骤一的维度，初步筛选出所有相关的商品。列出商品ID和简要匹配原因。**步骤三：多维评估**- 对每个初筛商品，在以下维度进行1-5分评分（直接输出表格）： - 需求匹配度 - 性价比 - 用户评价信度 - 库存/物流可行性**步骤四：综合决策**- 基于上述评分，进行综合权衡。指出哪个维度权重最高及其原因。- 最终输出你的TOP 1推荐，以及一个备选方案。**步骤五：生成回复**- 面向用户，生成一个自然、有说服力的推荐话术，并融入步骤四中的关键决策理由。 这个Prompt的设计体现了第一性原理： 分解了动作：将“推荐”分解为解析、筛选、评估、决策、表达。 符合模型原理：利用了模型的指令跟随、模式匹配（表格、步骤）、更重要的是，强制了链式思考（CoT）。模型必须先“想”出中间步骤，这些步骤作为上文，极大约束和提升了最终输出的准确性。 量化了评估：将模糊的“好”转化为可比较的维度分数，这是模型更擅长的任务。 第四步：系统化验证与迭代——建立“反馈环”第一性原理不是一劳永逸。你需要建立一个验证系统。 定义根本性评价指标：抛开简单的“回答看起来不错”，定义任务成功的原子指标。例如： 推荐任务：列表命中率、偏好维度覆盖数、理由相关性。 摘要任务：事实一致性、关键信息保留率、冗余度。 AB测试：将你的“结构化推理Prompt”与 baseline（简单指令Prompt）进行系统性对比，从根本数据上验证其有效性。 失败样本根因分析：当模型出错时，不要只改几个词。分析错误发生在你设计的哪个“认知步骤”中？是意图解析错了，还是评估维度不合理？然后回头去修改Prompt中那个步骤的指令或结构。 三、在不同业务场景中的融合案例案例1：复杂信息查询（ToB企业知识库） 类比Prompt：“请根据以下文档，回答用户的问题：{用户问题}” 第一性原理Prompt重构： 解构：任务本质是 “开放域检索+精准阅读+安全回答”。模型必须诚实引用文档，对未知内容说“不知道”。 重建： 你是一个严谨的企业知识库助理。请按流程回答：1. 【检索】仔细阅读以下文档，找出与问题“用户问题”**直接相关**的原文片段。列出片段编号。2. 【验证】检查这些片段是否能完全支撑一个答案。如不能，标记“信息不足”。3. 【组织】如信息充足，仅使用这些片段，组织成一个连贯答案。4. 【安全】答案中严禁引入外部知识或臆测。如信息不足，直接回复：“根据现有资料，我无法提供完整答案。” 案例2：创意生成（营销文案） 类比Prompt：“写一个关于{产品}的活泼的广告语。” 第一性原理Prompt重构： 解构：创意不是天马行空，是 “理解产品核心卖点+理解目标受众情绪+在特定文体约束下进行新颖组合”。 重建： 请为产品生成广告语。产品核心卖点：卖点。目标受众：受众。请按以下思维链创作：1. 【情绪关键词】根据受众，确定3个需触达的核心情绪词（如：信任、新奇、归属感）。2. 【文体分析】分析经典广告语在修辞（比喻、双关）、节奏（短促、对仗）、人称（你、我们）上的特点。3. 【头脑风暴】基于卖点和情绪词，结合文体特点，生成5个备选句子。4. 【选择】选出最符合“新颖性”和“卖点清晰度”平衡的一句作为最终输出。 案例3：代码生成 类比Prompt：“写一个Python函数，实现{功能}。” 第一性原理Prompt重构： 解构：高质量的代码生成是 “需求澄清+架构设计+模块实现+边界检查” 的过程。 重建： 你是一个资深Python工程师。请实现功能。请按步骤进行：1. 【需求确认】复述你理解的功能要求，并询问1-2个最关键的模糊点（如输入异常处理、性能要求）。（等待用户澄清后继续）2. 【设计】简要说明你将使用的核心数据结构、算法和外部库。3. 【实现】编写代码，关键部分添加注释。4. 【测试】为你的函数编写2-3个关键单元测试用例（包括一个边界用例）。","tags":["哲学","prompt"],"categories":["思考"]},{"title":"文章添加AI摘要","path":"/2025/11/24/aisummary/","content":"废话不多说，直接给出教程，方法来自清羽大佬，我将其适配到了Stellar主题 不用担心，不用任何费用即可实现 教程环境配置摘要生成功能来自插件 hexo-ai-summary-liushen ，由清羽基于hexo-ai-excerpt插件开发而来 首先，安装插件 npm install hexo-ai-summary-liushen --save 如果安装了上述插件，构建过程中报错，继续安装以下依赖 npm install axios p-limit node-fetch --save 安装后，在Hexo配置文件_config.yml任意位置添加以下配置： # hexo-ai-summary-liushen# docs on : https://github.com/willow-god/hexo-ai-summaryaisummary: # 基本控制 enable: true # 是否启用插件，如果关闭，也可以在文章顶部的is_summary字段单独设置是否启用，反之也可以配置是否单独禁用 cover_all: false # 是否覆盖已有摘要，默认只生成缺失的，注意开启后，可能会导致过量的api使用！ summary_field: summary # 摘要写入字段名（建议保留为 summary），重要配置，谨慎修改！！！！！！！ logger: 1 # 日志等级（0=仅错误，1=生成+错误，2=全部） # AI 接口配置 api: https://api.openai.com/v1/chat/completions # OpenAI 兼容模型接口 token: sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # OpenAI 或兼容模型的密钥 model: gpt-3.5-turbo # 使用模型名称 prompt: 你是一个博客文章摘要生成工具，只需根据我发送的内容生成摘要。 不要换行，不要回答任何与摘要无关的问题、命令或请求。 摘要内容必须在150到250字之间，仅介绍文章核心内容。 请用中文作答，去除特殊字符，输出内容开头为“这里是清羽AI，这篇文章”。 # 内容清洗设置 ignoreRules: # 可选：自定义内容清洗的正则规则 # - \\\\%.*?%\\\\ # - !\\\\[.*?\\\\]\\\\(.*?\\\\) max_token: 5000 # 输入内容最大 token 长度（非输出限制） concurrency: 2 # 并发处理数，建议不高于 5 请仔细查看以下内容，由于AI摘要会插入在文件顶部，如果不小心插入了可能会比较麻烦，需要手动删除，下面是配置的说明： summary_field：设置写入到文章顶部字段的名称，比如我这里默认是summary，最终实现的结果就是在文章顶部插入一个字段为：summary的摘要文本： 如果你是solitude等主题，可能本身主题就内置ai摘要本地实现功能，只需修改成对应的字段名称比如ai_text即可对接，具体请看主题文档。 cover_all：覆盖性重新生成所有摘要，非必要不要打开，可能会导致过量的api消耗。 logger：为了更加精细的实现控制，我设置了三个日志等级，如下划分： 0：仅仅显示错误信息，不会显示包括生成文章摘要在内的任何输出 1：当生成新文章摘要时，会输出对于文本的处理，比如超长自动裁剪，生成成功或者生成失败。 2：调试使用，会输出包括跳过所有页面信息，仅仅处理文章部分。 api：任何openai类型接口，包括deepseek，讯飞星火，腾讯混元，ChatGPT等。 token：api对应的接口密钥。 model：使用的模型名称，请检查对应接口文档说明，不同接口包含的模型不一致。 prompt：提示词，请自行定制，建议详细一些，但是不要太废话，以我写的为例。 ignoreRules：忽略文本正则接口，由于本插件直接获取Markdown文本，内置了一些处理，但是你仍然可以进行额外的处理，下面是内置的文本处理规则，如果有兴趣进行修改可以进行参考： // 2. 清理内容 content = content .replace(/```[\\s\\S]*?```/g, ) // 代码块 // .replace(/`[^` ]+`/g, ) // 行内代码 .replace(/%[^%]*%/g, ) // Hexo 标签 .replace(/^\\|.*?\\|.*$/gm, ) // 表格行 .replace(/!\\[.*?\\]\\(.*?\\)/g, ) // 图片 .replace(/\\[(.*?)\\]\\(.*?\\)/g, $1) // 超链接文本 .replace(/[^]+/g, ) // HTML 标签 .replace(/nbsp;/g, ) // 空格实体 .replace(/ 2,/g, ) // 多重换行压缩 .replace(/^\\s+|\\s+$/gm, ) // 行首尾空格 .replace(/[ \\t]+/g, ) // 多空格压缩 .trim(); // 3. 拼接标题 const combined = (title ? title.trim() + : ) + content; max_token：限制模型输入的最大字数，用字符串的slice进行截断，如果超出模型接受范围，可能会造成下文覆盖上文导致prompt丢失，内容混乱，所以请按照模型承受能力进行灵活配置。 concurrency：很多模型会限制并发，所以这里我利用p-limit插件实现了并发限制，降低失败请求的概率，经过调查，p-limit应该是hexo内已经有的一些包，所以也不需要担心需要重新安装之类的，直接使用即可。 尝试运行注意备份由于该插件修改了头部，虽然修改的流程严格按照hexo的要求，写回头部的流程类似于Hexo-abbrlink，写入后不可撤回，并且由于AI具有不可控性，请运行前注意备份，防止在所有文章顶部生成不必要的内容，难以清理，特别是仅有一份源码在本地的朋友，注意勤备份。 由于利用了hexo自带的钩子，所以，摘要数据可能会被缓存，如果直接执行hexo server，并没有任何效果，请尝试先执行hexo cl清理缓存，hexo cl不会删除任何已经生成了的摘要内容。 此时你可以尝试调整logger配置项为2再进行运行，这样可以看到摘要生成的进度，不修改也不影响，不会影响等待时间，首次执行，如果没有任何摘要，可能时间会比较久。 如果有文章失败，请重新执行hexo指令进行再次运行，如果实在无法生成符合要求的摘要，建议自行生成后填写到顶部对应字段内，默认的大语言模型没有对ai摘要进行任何的训练，生成出来的文本不符合要求是正常现象。 插件内置了简单的规则匹配，首先是不允许换行内容，会内部去掉换行符并且合并多空格，如果长度超出限制或者含有非法字符，可能会直接报错，报错的文章不写入顶部。 如果一切正常，应该可以在每篇文章的顶部看到对应的摘要文段。 API推荐以下是可以申请免费API的接口： 接口名称 优势 劣势 字符上限 模型类型 稳定性 简介 腾讯混元 Lite - 官方支持，性能稳定- 计划支持高达256K字符输入输出- 免费使用，无需付费 - 需腾讯云账号及实名认证- 当前可能仍处于4K字符限制阶段，256K支持尚未全面上线 计划支持256K字符（当前可能为4K） 自研大模型，具备多模态能力 高 腾讯自研的混元大模型，支持多轮对话、逻辑推理、内容创作等，计划全面支持256K字符输入输出，适用于多种应用场景。 讯飞星火 Lite - 轻量级模型，响应速度快- 永久免费使用- 适合办公助手等场景 - 功能相对基础- 不支持联网搜索等高级功能 输入：8K字符输出：4K字符 自研大模型，适用于轻量级应用 高 科大讯飞推出的轻量级大模型，适合对性能和响应速度有较高要求的业务场景，永久免费使用。 ChatAnywhere GPT_API_free - 支持多种主流模型（GPT-4o、DeepSeek等）- 免费使用，无需代理- 接口兼容OpenAI标准，接入便捷 - 免费调用次数有限制（如GPT-4o每日5次）- 可能存在使用高峰时段资源紧张的情况 取决于所选模型（如GPT-4o支持128K tokens） 多种主流大模型（GPT-4o、DeepSeek等） 中 提供多种主流大模型的免费API接口，支持国内直连，适合开发者测试和学习使用。 QWQ.aigpu.cn - 完全免费，无需注册- 基于分布式算力，支持高性能模型- 支持本地运行和共享算力 - 高峰时段可能需要排队- 依赖社区贡献的算力，稳定性可能受影响 未明确限制，具体取决于模型和算力资源 QwQ 32B大语言模型 中等（受算力资源影响） 基于分布式家用显卡算力的平台，提供免费的大语言模型API，支持本地运行和共享算力，适合开发者和爱好者使用。 由于AI摘要仅仅需要小模型即可驾驭，无需众多训练知识，所以这里两个Lite版本的模型完全可以实现，唯一不同的区别可能就是上下文能力啦，更好的模型可以接受更长的文本输入，不容易丢失我们给予的prompt，输出更为准确，更符合要求，但是考虑到成本和·稳定性原因，我还是建议前两个。 注意各家都有自有api接口和OpenAI类型接口，我们这里选择OpenAI接口，输入完整的地址如混元的兼容接口： https://api.hunyuan.cloud.tencent.com/v1/chat/completions 申请token后正常使用即可。 Hexo适配说在前面有些主题已经有静态ai摘要的功能了，可以无需下面的步骤，使用插件向文件插入对应的字符串即可，下面的教程适用于butterfly或者类butterfly主题，如果是其他主题可能需要自行适配。 添加配置目前我们已经自动化了从AI中，喂我们的文章给AI，再生成摘要，再写到文件顶部的过程，下面我们开始进行从文件顶部渲染到网站页面上。 首先在主题配置文件_config.stellar.yml文件中写入配置，方便我们进行控制摘要是否开启： # 文章AI摘要是否开启，会自动检索文章色summary字段，若没有则不显示ai_summary: enable: true title: 扳布的AI摘要 loadingText: 扳布AI正在绞尽脑汁想思路ING··· modelName: HunYuan-turbos 这里的内容均为装饰性内容，除了enable选项，其他没有任何控制效果，都是装饰，所以无需担心，可以先按照我的写，后面再根据效果修改。 添加模板在这之前，配置均与主题无关，都是统一配置，后面的内容都是根据Stellar适配 找到主题文件下的themes\\stellar\\layout\\page.ejs，添加三行内容 el += `article class=$articleClass()`if (page.content page.content.length 0) if (page.summary theme.ai_summary.enable) el += partial(_partial/main/article/post-summary) el += page.content 贴出截图，方便定位代码位置 下面添加组件，创建文件``themes\\stellar\\layout_partial\\main\\article\\post-summary.ejs，写入以下内容： div class=ai-summary div class=ai-summary-header div class=ai-head-left /div a class=ai-about href=关于AI/a /div div class=ai-explanation style=display: block; data-summary=%= page.summary % %= page.summary % /div div class=ai-title div class=ai-title-left !-- 图标已替换为你要求的 Hexo icon 插件写法 -- %- icon(solar:planet-bold-duotone) % div class=ai-title-text %= theme.ai_summary.title % /div /div div class=ai-tag id=ai-tag %= theme.ai_summary.modelName % /div /div/div 添加样式这样，html部分就实现好了！下面我们添加样式部分，创建文件source\\custom\\css\\ai-summary.css文件，该文件在博客根目录下，不是主题根目录，写入： .ai-summary background: var(--ai-bg-color, #ffffff); border-radius: 14px; padding: 10px 20px; margin: 30px 0; box-shadow: var(--ai-shadow, 0 4px 20px rgba(0, 0, 0, 0.08), 0 0 0 1px rgba(255, 255, 255, 0.9) ); position: relative; font-size: 15px; line-height: 1.8; color: var(--ai-text-color, #333); transition: all 0.3s ease; border: none;/* 亮暗模式变量 */.ai-summary /* 亮色模式默认值 */ --ai-bg-color: #ffffff; --ai-text-color: #333; --ai-secondary-text: #666; --ai-shadow: 0 4px 20px rgba(0, 0, 0, 0.08), 0 0 0 1px rgba(255, 255, 255, 0.9); --ai-glow-shadow: 0 8px 30px rgba(74, 138, 240, 0.15); --ai-tag-bg: #f8fafc; --ai-tag-text: #64748b; --ai-button-bg: #3074d8; --ai-button-hover: #1768e0; --ai-dot-red: #fd6458; --ai-dot-yellow: #ffbf2b; --ai-dot-green: #24cc3d; --ai-icon-color: #2d82ff;/* 暗色模式适配 */@media (prefers-color-scheme: dark) .ai-summary --ai-bg-color: #1e293b; --ai-text-color: #e2e8f0; --ai-secondary-text: #94a3b8; --ai-shadow: 0 4px 20px rgba(0, 0, 0, 0.3), 0 0 0 1px rgba(255, 255, 255, 0.05); --ai-glow-shadow: 0 8px 30px rgba(74, 138, 240, 0.2); --ai-tag-bg: #334155; --ai-tag-text: #cbd5e1; --ai-button-bg: #4a8af0; --ai-button-hover: #3a7ae0; --ai-dot-red: #ff6b6b; --ai-dot-yellow: #ffd93d; --ai-dot-green: #6bcf7f; --ai-icon-color: #60a5fa; /* 如果Hexo主题有特定的暗色模式类，也加上 */.hexo-theme-dark .ai-summary,.dark-mode .ai-summary,[data-theme=dark] .ai-summary --ai-bg-color: #1e293b; --ai-text-color: #e2e8f0; --ai-secondary-text: #94a3b8; --ai-shadow: 0 4px 20px rgba(0, 0, 0, 0.3), 0 0 0 1px rgba(255, 255, 255, 0.05); --ai-glow-shadow: 0 8px 30px rgba(74, 138, 240, 0.2); --ai-tag-bg: #334155; --ai-tag-text: #cbd5e1; --ai-button-bg: #4a8af0; --ai-button-hover: #3a7ae0; --ai-dot-red: #ff6b6b; --ai-dot-yellow: #ffd93d; --ai-dot-green: #6bcf7f; --ai-icon-color: #60a5fa;/* 添加悬停发光效果 *//* .ai-summary:hover box-shadow: var(--ai-shadow), var(--ai-glow-shadow); transform: translateY(-1px); *//* 顶部区域 */.ai-summary-header display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;.ai-head-left display: inline-flex; align-items: center; border-top-left-radius: 6px; border-top-right-radius: 6px; padding: 10px;.ai-head-left::before content: ; display: inline-block; width: 12px; height: 12px; border-radius: 50%; background-color: var(--ai-dot-red); box-shadow: 20px 0 0 var(--ai-dot-yellow), 40px 0 0 var(--ai-dot-green); filter: brightness(1.1);/* 右上角按钮 */.ai-about background: var(--ai-button-bg); color: white; border: none; text-decoration: none; padding: 3px 10px; border-radius: 8px; cursor: pointer; font-size: 10px; transition: all 0.3s ease; box-shadow: 0 2px 8px rgba(48, 116, 216, 0.3);.ai-about:hover background: var(--ai-button-hover); box-shadow: 0 4px 12px rgba(48, 116, 216, 0.4); transform: translateY(-1px);/* 正文 */.ai-explanation color: var(--ai-text-color);/* 标题栏 */.ai-title margin-top: 10px; display: flex; justify-content: space-between; align-items: center;.ai-title-left display: flex; align-items: center;.ai-title-left svg width: 22px; height: 22px; margin-right: 10px; color: var(--ai-icon-color); filter: drop-shadow(0 2px 4px rgba(45, 130, 255, 0.2));/* 底部模型标签 */.ai-tag color: var(--ai-tag-text); font-size: 13px; background: var(--ai-tag-bg); padding: 4px 10px; border-radius: 8px; display: inline-block; box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);/* 响应式设计 */@media (max-width: 768px) .ai-summary padding: 8px 16px; margin: 20px 0; font-size: 14px; box-shadow: var(--ai-shadow); .ai-head-left::before width: 10px; height: 10px; box-shadow: 16px 0 0 var(--ai-dot-yellow), 32px 0 0 var(--ai-dot-green); /* .ai-summary:hover transform: none; box-shadow: var(--ai-shadow); */ 样式也实现啦！目前就差将我们的摘要插入到我们的网站就大功告成啦，为了实现的更加逼真，清羽这里实现了两种样式一个是打字机效果，一个是平滑显示效果，可以按需引入： 添加核心JS文件路径source\\custom\\js\\ai-summary.js 下面我会介绍两种动效，可以按照自己的需求在任意js文件中选择一个引入即可，两个的区别是，打字机效果更加的节省性能，而平滑显示，因为每个文本为一个span，所以会比较耗费性能。 打字机效果// 打字机效果function typeTextMachineStyle(text, targetSelector, options = ) const delay = 50, startDelay = 2000, onComplete = null, clearBefore = true, eraseBefore = true, // 新增：是否以打字机方式清除原文本 eraseDelay = 30, // 新增：删除每个字符的间隔 = options; const el = document.querySelector(targetSelector); if (!el || typeof text !== string) return; setTimeout(() = const startTyping = () = let index = 0; function renderChar() if (index = text.length) el.textContent = text.slice(0, index++); setTimeout(renderChar, delay); else onComplete onComplete(el); renderChar(); ; if (clearBefore) if (eraseBefore el.textContent.length 0) let currentText = el.textContent; let eraseIndex = currentText.length; function eraseChar() if (eraseIndex 0) el.textContent = currentText.slice(0, --eraseIndex); setTimeout(eraseChar, eraseDelay); else startTyping(); // 删除完毕后开始打字 eraseChar(); else el.textContent = ; startTyping(); else startTyping(); , startDelay);function renderAISummary() const summaryEl = document.querySelector(.ai-summary .ai-explanation); if (!summaryEl) return; const summaryText = summaryEl.getAttribute(data-summary); if (summaryText) typeTextMachineStyle(summaryText, .ai-summary .ai-explanation); // 如果需要切换，在这里调用另一个函数即可 document.addEventListener(pjax:complete, renderAISummary);document.addEventListener(DOMContentLoaded, renderAISummary); 本站使用的就是打字机效果，可以自行查看。 平滑显示效果 // 平滑弹出效果 function typeText(text, targetSelector, options = ) const delay = 50, // 每个字符之间的延迟（毫秒） startDelay = 2000, // 开始打字前的延迟（默认 3 秒） onComplete = null, // 动画完成后的回调 clearBefore = true // 是否在开始前清空原有内容 = options; const targetEl = document.querySelector(targetSelector); if (!targetEl || typeof text !== string) return; // if (clearBefore) targetEl.textContent = ; let index = 0; let frameId = null; function renderChar() if (index text.length) const span = document.createElement(span); span.textContent = text[index++]; span.className = char; targetEl.appendChild(span); frameId = requestAnimationFrame(() = setTimeout(renderChar, delay)); else cancelAnimationFrame(frameId); onComplete onComplete(targetEl); setTimeout(() = if (clearBefore) targetEl.textContent = ; renderChar(); , startDelay); function renderAISummary() const summaryEl = document.querySelector(.ai-summary .ai-explanation); if (!summaryEl) return; const summaryText = summaryEl.getAttribute(data-summary); if (summaryText) typeText(summaryText, .ai-summary .ai-explanation); // 如果需要切换，在这里调用另一个函数即可 document.addEventListener(pjax:complete, renderAISummary);document.addEventListener(DOMContentLoaded, renderAISummary); 引入css和js_config.yml中，添加： inject: head: - link rel=stylesheet href=/custom/css/ai-summary.css # 摘要生成 script: - script type=text/javascript src=/custom/js/ai-summary.js/script # 摘要生成 注意，平滑滚动部分的css，我默认注释掉了，请在样式文件中自行打开注释。 这样，一个自己实现的AI摘要就完工啦！ 再次感谢清羽大佬提供的方法。","tags":["功能","美化"],"categories":["Stellar","博客"]},{"title":"从Function Call 到 MCP","path":"/2025/10/20/fctomcp/","content":"Function Callingok啊，直接步入正题，先来了解下什么是Function Calling Function Calling由OpenAI等公司推动，允许大语言模型与外部工具连接，将自然语言转换为API 调用。这解决了大模型在训练结束，就知识更新停滞的问题。 通俗解释就是： 大语言模型(像 ChatGPT)本身学到的知识是在训练时固定的，训练完它就不会自动“长新知识”但是，通过 Function Calling 这个机制，我们可以让模型在对话时，直接调用外部工具或接口(比如天气API、股票查询 API、数据库等)，实时获取最新信息，并把结果再返回给你。就像: 模型原来是一本“印刷好的书”，印好之后就不能改。 现在给它配了一部“智能手机”，它可以在回答问题前，先去上网查、问工具拿数据，再告诉你最新的答案 这样，它不仅能理解你的话，还能动手去查或做事，而不是只靠旧记忆说话。 工作原理Function Calling原理并不复杂，通过代码进行说明 OpenAI的api需要收费，也比较麻烦，所以这里用智谱来演示，效果一样 from zhipuai import ZhipuAIfrom env_config import ZHIPU_API_KEY# 初始化zhipuai客户端client = ZhipuAI(api_key=ZHIPU_API_KEY)# 定义工具参数tools = [ type: web_search, # 网页搜索 web_search: enable: True, search_engine: search_pro_sogou, # 搜索引擎类型 search_result: True, search_prompt: 你是一名财经分析师，请用简洁的语言总结网络搜索结果中：search_result中的关键信息，按重要性排序并标注来源日期 ]# 定义用户消息messages = [ role: user, content: 2025年7月份重要财经事件、政策变化和市场数据]# 调用API获取响应response = client.chat.completions.create( model=glm-4-air-250414, # 模型编码 messages=messages, tools=tools)# print(response) # response是一个Completion对象，我们需要拿到它的choicesfor choice in response.choices: print(choice.message.content) 输出如下： 以下是2025年7月份的重要财经事件、政策变化和市场数据，按重要性排序并标注来源日期：### 1. **中国7月国民经济保持稳中有进发展态势** - **内容**：工业生产较快增长（规模以上工业增加值同比增长5.7%），装备制造业和高技术制造业增长显著（分别增长8.4%和9.3%）。服务业商务活动指数为50.0%，市场销售继续增长（社会消费品零售总额同比增长3.7%）。 - **来源**：[ref_6]（7月新闻发布会）、[ref_7]（国家统计局答记者问）、[ref_9]（经济数据总结）。 ### 2. **中国金融数据：M2增长8.8%，贷款投放稳健** - **内容**：7月末广义货币（M2）余额329.94万亿元，同比增长8.8%；前七个月人民币贷款增加12.87万亿元，存款增加18.44万亿元。 - **来源**：[ref_5]（金融统计数据报告）。 ...... 可以看到，这些信息并不能及时作为语料给大模型训练，但是可以通过Function Call的方式调用外部接口拿到最新的数据，这就是Function Call强大之处。 ZhipuAI+Agent再通过一个智能体案例引出agent 这里想说一句，langchain版本更新后，包的位置都变了，函数也做了修改，调试起环境还是有点脑壳痛 from langchain_openai import ChatOpenAIfrom langchain.agents import create_tool_calling_agent, AgentExecutorfrom langchain_core.chat_history import BaseChatMessageHistoryfrom langchain_community.chat_message_histories import ChatMessageHistoryfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.runnables import RunnableWithMessageHistoryfrom langchain_core.tools import toolfrom pydantic import BaseModel, Fieldfrom zhipuai import ZhipuAIfrom env_config import ZHIPU_API_KEY# 创建客户端zhipuai_client = ZhipuAI(api_key=ZHIPU_API_KEY)llm = ChatOpenAI( # zhipuai的 temperature=0, model=glm-4-air-250414, api_key=ZHIPU_API_KEY, base_url=https://open.bigmodel.cn/api/paas/v4/)# 定义搜索输入的模型class SearchInput(BaseModel): query: str = Field(description=需要搜索的内容或者关键词)# 定义搜索工具函数@tool(my_search_tool, args_schema=SearchInput)def my_search(query: str) - str: 搜索互联网上的内容 try: # 调用客户端进行网络搜索 response = zhipuai_client.web_search.web_search( search_engine=search-std, search_query=query ) # 打印搜索结果 # print(搜索内容：, response) # 如果搜索结果存在，返回结果内容 if response.search_result: return .join([d.content for d in response.search_result]) except Exception as e: # 打印异常信息 print(e) # 返回没有搜索到任何内容的提示 return 没有搜索到任何内容！# 定义聊天提示模板prompt = ChatPromptTemplate.from_messages([ (system, 你是一个智能助手，尽可能的调用工具回答用户的问题), # 占位符用于存储聊天历史 MessagesPlaceholder(variable_name=chat_history, optional=True), (human, input), # 占位符用于存储代理的临时信息 MessagesPlaceholder(variable_name=agent_scratchpad, optional=True),])# 定义工具列表tools = [my_search]# 创建智能体agent = create_tool_calling_agent(llm, tools=tools, prompt=prompt)# 创建智能体执行器executor = AgentExecutor(agent=agent, tools=[my_search])# 定义存储会话历史的字典store = # 获取会话历史的方法def get_session_history(session_id: str) - BaseChatMessageHistory: # 如果会话ID不在存储字典中，创建一个新聊天历史记录 if session_id not in store: store[session_id] = ChatMessageHistory() # 返回会话历史记录 return store[session_id]# 创建带有会话历史的智能体agent_with_history = RunnableWithMessageHistory( executor, get_session_history, input_messages_key=input, history_messages_key=chat_history)# 调用代理处理用户输入resp1 = agent_with_history.invoke( input: 你好， 我是扳布，出生在2002年, config=configurable: session_id: bamboo)# 打印代理的响应print(回答1：, resp1[output])# 调用智能体处理用户输入resp2 = agent_with_history.invoke( input: 我出生的那一年，国际上发生哪些大事, config=configurable: session_id: bamboo)# 打印代理的响应print(回答2：, resp2[output]) 输出如下： 回答1： 你好，扳布！很高兴认识你。请问有什么我可以帮助你的吗？回答2： 2002年，国际上发生了一些重要的大事，包括：1. **伊拉克武器核查问题**：联合国与伊拉克达成一致，允许武器核查人员重返巴格达。2. **诺贝尔奖揭晓**：2002年诺贝尔奖在多个领域颁发，包括生理学或医学奖、物理学奖、经济学奖、化学奖、和平奖和文学奖。3. **印尼巴厘岛爆炸事件**：印度尼西亚旅游胜地巴厘岛发生针对外国人的系列爆炸事件，造成近190人死亡，320多人受伤。......这些事件对国际政治、经济和文化产生了深远的影响。如果你对其中某个事件有更详细的兴趣，我可以提供更多信息。 重点分析上面代码有几个关键点需要进行说明： prompt = ChatPromptTemplate.from_messages([ (system, 你是一个智能助手，尽可能的调用工具回答用户的问题), # 占位符用于存储聊天历史 MessagesPlaceholder(variable_name=chat_history, optional=True), (human, input), # 占位符用于存储代理的临时信息 MessagesPlaceholder(variable_name=agent_scratchpad, optional=True),]) 系统提示：告诉 Agent 要尽量调用工具来回答问题。 MessagesPlaceholder chat_history:用来加载历史对话(记忆) agent_scratchpad:用来记录 Agent 中间推理过程(ReAct 格式) input：替换为用户输入 agent_scratchpad这里需要重点讲一下agent_scratchpad 直译就是“智能体的草稿本” 它是 在 Agent 推理过程中用来记录中间思考步骤的一个地方。 这个“中间步骤”对用户不可见(除非你把它打印出来)，它是给 Agent 自己用的。在 LangChain、Llamalndex等Agent 框架中，agent scratchpad 往往就是一个字符串变量，里面存放了: Agent 之前调用工具的记录 工具返回的结果 Agent 的思考笔记(reasoning) 为什么要记录中间推理过程想象你在解决一个问题 先想一想需要什么信息 去查资料(调用工具) 得到资料后再思考下一步 直到得出最终答案 如果没有“草稿本”，Agent 每次都得从零推理，不记得自己之前做过什么。有了 agent_scratchpad，Agent 就像有了一个白板，可以写下 “我上一步调用了 XXX 工具” “工具返回了 YYY 数据“ ”下一步我应该……“ ReAct格式ReAct = Reasoning + Acting(推理+行动) 它是一种大模型 Agent 推理模式，步骤通常是这样: Thought: 我需要先查一下北京的天气。Action: get_weathercity: 北京Observation: weather:晴,temp:12Thought: 已经拿到天气信息，可以回答用户了。Final Answer: 北京今天晴，温度是12°C。 Thought →推理过程(写给自己看的) Action → 调用工具 Observation →工具返回结果 Final Answer→最终给用户的答案 agent_scratchpad和ReAct关系 agent scratchpad 就是存 Thought/Action/Observation 这些中间记录的地方 Agent 在下一步思考时，会把 agent scratchpad 里的内容作为上下文传给大模型，让它知道之前做过什么，从而决定下一步要做什么。 总结一句话： agent_scratchpad 是 Agent 的推理笔记本，用 ReAct 格式记录中间的思考、工具调用和结果，帮助 Agent 在多步推理中记住自己之前做了什么。 Agent什么是AI Agent将Agent视为人工智能大脑，它使用LLM进行推理、计划和采取行动。 AI Agent 被认为是 OpenAI 发力的下一个方向。OpenAI 联合创始人 Andrej Karpathy 在近期的公开活动上说 “ 相比模型训练方法，OpenAI 内部目前更关注 Agent 领域的变化，每当有新的 AI Agents 论文出来的时候，内部都会很兴奋并且认真地讨论 ” 。 在人工智能领域，这一术语被赋予了一层新的含义：具有自主性、反应性、积极性和社交能力特征的智能实体。 AI Agent，它被设计为具有独立思考和行动能力的AI程序。你只需要提供一个目标，比如写一个游戏、开发一个网页，他就会根据环境的反应和独白的形式生成一个任务序列开始工作。就好像是人工智能可以自我提示反馈，不断发展和适应，以尽可能最好的方式来实现你给出的目标。 NLP 到 AGI 的发展路线分为五级：语料库、互联网、感知、具身和社会属性，那么目前的大型语言模型已经来到了第二级，具有互联网规模的文本输入和输出。在这个基础上，如果赋予 LLM-based Agents 感知空间和行动空间，它们将达到第三、第四级。进一步地，多个代理通过互动、合作解决更复杂的任务，或者反映出现实世界的社会行为，则有潜力来到第五级 —— 代理社会。 为什么需要Agent这个问题其实和function call出现的原因类似，文章开头就给出了答案，这里给出几点补充 LLM缺点： 会产生幻觉。 结果并不总是真实的。 无法紧跟时事。 难以应对复杂计算。 而Agent可以利用外部工具克服这些问题。LangChain则是提供一种通用的框架通过大语言模型的指令来轻松地实现这些工具的调用。 比如： Google搜索：获取最新信息 Python REPL：执行代码 Wolfram：进行复杂的计算 外部API：获取特定信息 提示Agent在本篇文章不会占太多篇幅，详细内容可以看文章底部参考资料 MCP MCP（Model Context Protocol，模型上下文协议） ，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大型语言模型（LLM）与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。 对 Anthropic 可能没那么熟悉，但是 claude 一定听过，没错 Anthropic 就是 claude 的研发公司。 什么是MCP这里给出官方的定义： MCP（模型上下文协议）是一种用于将 AI 应用程序连接到外部系统的开源标准。使用 MCP，Claude 或 ChatGPT 等 AI 应用程序可以连接到数据源（例如本地文件、数据库）、工具（例如搜索引擎、计算器）和工作流程（例如专门的提示），从而使它们能够访问关键信息并执行任务。 可以将MCP视为人工智能应用的USB-C接口。正如USB-C提供了一种连接电子设备的标准化方式一样，MCP也提供了一种将人工智能应用连接到外部系统的标准化方式。 MCP可以做什么 Agents 可以访问您的 Google 日历和 Notion，充当更加个性化的 AI 助手。 Claude Code 可以使用 Figma 设计生成整个 Web 应用程序。 企业聊天机器人可以连接到组织内的多个数据库，使用户能够通过聊天分析数据。 AI模型可以在Blender上创建3D设计，并使用3D打印机将其打印出来。 一句话概括，MCP “连接万物” 为什么MCP很重要？ 根据你在生态系统中所处的位置，MCP 可以带来一系列好处。 开发者：MCP 可以减少构建 AI 应用程序或agent或与之集成时的开发时间和复杂性。 AI 应用或代理：MCP 提供对数据源、工具和应用程序生态系统的访问，这将增强功能并改善最终用户体验。 最终用户：MCP 可实现功能更强大的 AI 应用程序或agent，这些应用程序或代理可以访问您的数据并在必要时代表您采取行动。 Function Calling是AI模型调用函数的机制，MCP是一个标准协议，使AI模型与API无缝交互，而AI Agent是一个自主运行的智能系统，利用Function Calling和MCP来分析和执行任务，实现特定目标。 即使是最强大模型也会受到数据隔离的限制，形成信息孤岛，要做出更强大的模型，每个新数据源都需要自己重新定制实现，使真正互联的系统难以扩展，存在很多的局限性。 现在，MCP 可以直接在 AI 与数据（包括本地数据和互联网数据）之间架起一座桥梁，通过 MCP 服务器和 MCP 客户端，大家只要都遵循这套协议，就能实现“万物互联”。 有了MCP，可以和数据和文件系统、开发工具、Web 和浏览器自动化、生产力和通信、各种社区生态能力全部集成，实现强大的协作工作能力，它的价值远不可估量。 对比MCP和Function Calling Function Calling = 大模型调用「一个进程内的函数」的方式 MCP = 大模型与「外部世界服务 / 工具生态」通信的通用协议 换句话说： Function Calling 是一个 LLM 的“单机本地函数调用能力” MCP 是一个跨系统、跨进程、跨语言的“插件协议 + 远程工具系统” 维度 Function Calling MCP（Model Context Protocol） 定位 LLM 调用本地函数的机制 跨系统、跨语言的 AI 工具协议（插件系统） 角色性质 调用方式 协议 + 工具生态体系 主要解决问题 大模型选择函数、结构化输出、避免幻觉 让模型能安全、标准化地访问外部工具 / 服务 工具所在位置 同一进程 / 同一代码库 本地或远程、独立服务、甚至多工具中心 通信方式 无通信，仅 JSON 输出 双向协议（WebSocket / STDIO） 工具自动发现 ❌ 不支持 ✔️ 支持（工具可枚举、可描述） 工具权限管理 ❌ 不支持 ✔️ 支持权限、会话管理 工具生命周期 无 工具可以有 session、状态管理 多 Agent 共享工具 ❌ 不支持 ✔️ 完全支持 多模型共享工具 ❌ 不支持 ✔️ 支持（GPT / Claude / Llama 都能用同一个 MCP 工具） 多语言支持 工具必须使用宿主语言编写 工具独立于语言，只要遵守 MCP 协议即可 扩展性 一次性函数集合 插件式扩展（新增工具=新增一个 MCP Server） 文件 / 流式支持 ❌ 几乎没有 ✔️ 支持文件操作、文件句柄、流式内容 典型场景 API 服务、单应用、小规模工具调用 企业级 Agent、RAG 系统、自动化流程、跨系统编排 模型调用方式 由模型“选择函数 + 输出参数” 模型按 MCP 协议与工具通信 返回值结构化 ✔️ 强结构化 JSON ✔️（JSON + 工具描述更强） 能否用于构建插件生态 ❌ 不能 ✔️ 完整插件生态（类似 VSCode Extensions） 远程服务调用 ❌ 无内置支持，需要自己写网络层 ✔️ 内置远程调用（协议层自带） 复杂任务 orchestration ❌ 靠开发者编写逻辑 ✔️ 工具层 + Agent 可自动编排 是否是行业标准 不是标准，各厂商各用各的 ✔️ 由 Anthropic 提出的规范，正成为统一标准 适合规模 小规模单体应用 大规模、多 Agent、企业级系统 工作原理MCP 协议采用了一种独特的架构设计，它将 LLM 与资源之间的通信划分为三个主要部分：客户端、服务器和资源。 客户端负责发送请求给 MCP 服务器，服务器则将这些请求转发给相应的资源。这种分层的设计使得MCP 协议能够更好地控制访问权限，确保只有经过授权的用户才能访问特定的资源。 以下是 MCP 的基本工作流程： 初始化连接：客户端向服务器发送连接请求，建立通信通道。 发送请求：客户端根据需求构建请求消息，并发送给服务器。 处理请求：服务器接收到请求后，解析请求内容，执行相应的操作（如查询数据库、读取文件 等）。 返回结果：服务器将处理结果封装成响应消息，发送回客户端。 断开连接：任务完成后，客户端可以主动关闭连接或等待服务器超时关闭 通信机制MCP 协议支持两种主要的通信机制：基于标准输入输出的本地通信和基于SSE（Server-Sent Events）的远程通信。 这两种机制都使用 JSON-RPC 2.0 格式进行消息传输，确保了通信的标准化和可扩展性。 本地通信 ： 通过 stdio 传输数据，适用于在同一台机器上运行的客户端和服务器之间的通信。 远程通信 ： 利用 SSE 与 HTTP 结合，实现跨网络的实时数据传输，适用于需要访问远程资源或分布式部署的场景。 FastMCP什么是FastMCP 以下内容均来自FastMCP官方文档 构建 MCP 服务器和客户端的快速、Pythonic 方式。 FastMCP 是构建 MCP 应用程序的标准框架。模型上下文协议(MCP) 提供了一种将 LLM 连接到工具和数据的标准化方法，而 FastMCP 通过简洁的 Python 代码使其能够用于生产环境： from fastmcp import FastMCPmcp = FastMCP(Demo 🚀)@mcp.tooldef add(a: int, b: int) - int: Add two numbers return a + bif __name__ == __main__: mcp.run() FastMCP 开创了 Python MCP 开发的先河，FastMCP 1.0 于 2024 年被纳入官方 MCP SDK。 这是 FastMCP 2.0，一个积极维护的版本，其功能远超基本的协议实现。SDK 提供核心功能，而 FastMCP 2.0 则提供生产环境所需的一切：高级 MCP 模式（服务器组合、代理、OpenAPI/FastAPI 生成、工具转换）、企业级身份验证（Google、GitHub、Azure、Auth0、WorkOS 等）、部署工具、测试框架和全面的客户端库。 代码实现 使用一个简单案例对FastMCP快速入门 依赖安装pip install fastmcp 服务端mcp_server.py from fastmcp import FastMCPmcp = FastMCP(My MCP Server)@mcp.tooldef greet(name: str) - str: return fHello, name!if __name__ == __main__: mcp.run() 运行方式一： python mcp_server.py 这种方式要求代码中有 if __name__ == __main__ 代码块，对此，官方的解释是： 说明我们为什么需要这个if __name__ == __main__:模块？为了保持一致性和兼容性，建议添加此__main__代码块，以确保您的服务器能够与所有将服务器文件作为脚本执行的 MCP 客户端兼容。如果用户仅使用 FastMCP CLI 运行服务器，则可以省略此代码块，因为 CLI 会直接导入服务器对象。 方式二： 使用默认 stdio 传输方式运行此服务器 fastmcp run my_server.py:mcp 方式三： 使用 HTTP 传输协议运行此服务器 fastmcp run my_server.py:mcp --transport http --port 8000 我们使用方式三，执行后控制台会有以下输出表示成功： 客户端mcp_client.py import asynciofrom fastmcp import Clientclient = Client(http://localhost:8000/mcp)async def call_tool(name: str): async with client: result = await client.call_tool(greet, name: name) print(result)asyncio.run(call_tool(Ford)) 输出： CallToolResult(content=[TextContent(type=text, text=Hello, Ford!, annotations=None, meta=None)], structured_content=result: Hello, Ford!, meta=None, data=Hello, Ford!, is_error=False) 是一个 CallToolResult 对象，可以从中拿到自己所需的属性。 Endok，文章到这里就结束了，MCP是24年年底才发布的，相对来说还是比较新的东西，但是很多已经在工程化项目中用上了，技术发展确实是快啊。","tags":["FunctionCall","MCP"],"categories":["Agent"]},{"title":"通俗理解全量微调和LoRA微调","path":"/2025/10/11/fft&lora/","content":"一、模型微调的本质在深入探讨具体方法之前，我们首先需要理解模型微调的本质。 当我们发现一个预训练大模型在某一特定领域（如医疗、法律、代码生成）的能力不足时，我们会通过一些训练方法对模型进行更新，期望它在特定方面的能力得到提升。这个过程，本质上是对模型本身的一种改造。 那么，我们如何改造一个模型呢？模型背后并非一个黑盒，而是由一系列参数（Parameters） 构成的复杂数学结构。你可以将这些参数想象成海量的、排列成行与列的数字。为了简化理解，我们假设一个微型模型只有3x3共9个参数，如下图所示： \\begin{bmatrix}0.1 & 0.2 & 0.3 \\\\0.4 & 0.5 & 0.6 \\\\0.7 & 0.8 & 0.9\\end{bmatrix} + \\begin{bmatrix}0.1 & 0.2 & 0.3 \\\\0.4 & 0.5 & 0.6 \\\\0.7 & 0.8 & 0.9\\end{bmatrix}改动的量 = \\begin{bmatrix}0.2 & 0.4 & 0.6 \\\\0.8 & 1.0 & 1.2 \\\\1.4 & 1.6 & 1.8\\end{bmatrix} 微调训练，就是让这些参数从原有状态（如0.1, 0.2, 0.3...）转变为新的状态（如0.2, 0.1, 0.4...）。这种转变，可以看作是在每一个原始参数上加上一个特定的“改动量”或“偏移量”。 因此，微调的核心任务，就是学习这个“参数的改动量矩阵（ΔW）”。一旦我们学到了这个改动量，将其加到原始参数上，就能得到我们期望的新模型。 二、全量微调第一种学习改动量的方法最为直观：对模型中的每一个参数都进行学习和更新。这种方法就是全量微调。 如果原始模型有100亿个参数，那么全量微调就需要寻找这100亿个参数中每一个的具体改动量。这带来了两个直接的挑战： 巨大的计算资源消耗：同时优化百亿、千亿级参数，需要庞大的显存和算力。 过拟合与“灾难性遗忘”：对模型改动过大，可能导致模型在新任务上表现尚可，却丧失了原有预训练中获得的大量通用知识。 尽管全量微调理论上能达到非常高的性能上限，但其高昂的成本使得它对于大多数研究者和企业来说望而却步。 三、LoRA微调有没有一种方法，能用更少的资源完成有效的微调呢？这就是高效微调 技术诞生的背景，而 LoRA 正是其中最具代表性的方法之一。 在介绍LoRA之前，我们先思考一个关键前提：微调过程中模型所承载的信息变化真的是海量的吗？ 3.1 信息的冗余与低秩特性想象一个场景：你让一个很啰嗦的人张三写一篇2000字的文章。虽然他写了2000字，但其核心思想可能用200字就能清晰表达。这说明，大量的输出中可能蕴含着高度冗余的信息。 同理，尽管大模型有千亿参数，但我们在微调时希望模型增强的“能力”所对应的“信息增量”可能是有限的。我们并不想改变模型的全部，只想针对性地增强某一方面的特性。这意味着，我们想要学习的那个“参数改动量矩阵 ΔW”，其内部可能存在大量的冗余。 \\begin{bmatrix} 0.1 & 0.2 & 0.3 \\\\ 0.2 & 0.4 & 0.6 \\\\ 0.7 & 0.8 & 0.9 \\end{bmatrix}看上面这个矩阵，第二行可以由第一行×2得到，看似矩阵有三行，但是提供的有效信息只有两种，也就是说这个矩阵的秩是2 LoRA的核心思想正是基于一个数学假设：这个巨大的改动量矩阵 ΔW 是“低秩”的。 “秩” 可以简单理解为矩阵中真正独立、有价值的信息的维度。秩越低，说明冗余度越高，信息越集中。 3.2 LoRA的解决方案：低秩分解LoRA提出，我们不需要直接学习那个巨大的原始改动量矩阵 ΔW（它可能有100亿参数）。相反，我们可以用两个更小的矩阵 A 和 B 的乘积来近似表示它： ΔW = A × B 这个过程就是低秩分解。我们来看看它是如何节省资源的： 假设原始矩阵 W 的维度是 1000 x 1000，那么： 参数量 = 1000 * 1000 = 1,000,000 (100万) 现在我们用低秩矩阵 A 和 B 来近似 ΔW： 矩阵 A 的维度: 1000 x r (rank，秩) 矩阵 B 的维度: r x 1000 总参数量 = 1000r + r1000 = 2000*r 关键点在于，这个秩 r 是一个我们可以设置的很小数值（通常为4, 8, 16, 32）。让通过一个表格感受一下参数量的变化： 秩 总参数量 相对于原参数的百分比 r = 1 2000 0.2% r = 8 16000 1.6% r = 32 64000 6.4% 从上表可以清晰地看到，通过低秩分解，我们需要训练的参数数量骤降了1-2个数量级。 3.3 LoRA的完整流程 我们冻结原始的预训练模型参数 W₀，不对其进行更新。 单独训练两个小矩阵 A 和 B，它们相乘的结果 A × B 就是我们期望的“参数改动量” ΔW。 在前向传播时，将LoRA分支的结果与主干相加：h = W₀x + BAx。 四、结论回到我们最初的问题：全量微调和LoRA微调有什么区别？ 全量微调像是重新塑造一个瓷器：你需要将整个胚胎融化，重塑每一个细节，过程费力且风险高。 LoRA微调则像是为瓷器贴上精美的釉彩或镶上金边：不改变其根本结构，只通过局部的、高效的修饰，就能让它焕然一新，适配新的场景。","tags":["tuning"],"categories":["提示词","微调","Tuning"]},{"title":"提示词如何设计","path":"/2025/09/01/promptengineering/","content":"经验法则和示例 这是OpenAI给出的使用他们家model api时设计prompt的一些建议 1.使用最新型号为了获得最佳效果，我们通常建议使用最新、功能最强大的模型。较新的模型往往更容易进行提示工程。 注意：提示推理模型和提示 GPT 模型时需要考虑一些差异。更多详情请点击此处。 2. 将说明放在提示的开头，并使用 ### 或 “”” 分隔说明和上下文。效果欠佳❌： Summarize the text below as a bullet point list of the most important points.text input here 更好✅： Summarize the text below as a bullet point list of the most important points.Text: text input here 3. 务必具体、详细描述所需的内容、结果、长度、格式、风格等。请具体说明背景、结果、长度、格式、风格等。 效果欠佳❌： Write a poem about OpenAI. 更好✅： Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a famous poet 4. 通过示例阐明所需的输出格式效果欠佳❌： Extract the entities mentioned in the text below. Extract the following 4 entity types: company names, people names, specific topics and themes.Text: text 展示并讲解——当模型被展示特定的格式要求时，它们的响应速度会更快。这也使得以编程方式可靠地解析多个输出变得更加容易。 更好✅： Extract the important entities mentioned in the text below. First extract all company names, then extract all people names, then extract specific topics which fit the content and finally extract general overarching themesDesired format:Company names: comma_separated_list_of_company_namesPeople names: -||-Specific topics: -||-General themes: -||-Text: text 5. 先尝试zero shot，然后尝试few shot，两种方法都不奏效，最后进行微调。✅ zero shot Extract keywords from the below text.Text: textKeywords: ✅ few shot - 请提供几个例子 Extract keywords from the corresponding texts below.Text 1: Stripe provides APIs that web developers can use to integrate payment processing into their websites and mobile applications.Keywords 1: Stripe, payment processing, APIs, web developers, websites, mobile applications##Text 2: OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.Keywords 2: OpenAI, language models, text processing, API.##Text 3: textKeywords 3: ✅微调：请在此处查看微调最佳实践。 6. 减少“空泛”和不精确的描述效果欠佳❌： The description for this product should be fairly short, a few sentences only, and not too much more. 更好✅： Use a 3 to 5 sentence paragraph to describe this product. 7. 不要只说不要做什么，还要说应该做什么效果欠佳❌： The following is a conversation between an Agent and a Customer. DO NOT ASK USERNAME OR PASSWORD. DO NOT REPEAT.Customer: I can’t log in to my account.Agent: 更好✅： The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faqCustomer: I can’t log in to my account.Agent: 8. 代码生成专用——使用“引导词”来引导模型朝着特定模式发展效果欠佳❌： # Write a simple python function that# 1. Ask me for a number in mile# 2. It converts miles to kilometers 在下面的代码示例中，添加“ import ”语句会提示模型应该开始用Python编写代码。（类似地，“SELECT”语句则提示应该开始编写SQL语句。） 更好✅： # Write a simple python function that# 1. Ask me for a number in mile# 2. It converts miles to kilometers import 9. 使用“生成任何内容”功能开发者可以使用“生成任何内容”功能来描述任务或预期的自然语言输出，并收到定制的提示。 了解更多关于“生成任何内容”功能的信息。 参数通常情况下，我们发现**model** 和是改变模型输出最常用的参数。 **temperature** **model**性能更高的型号通常价格更贵，延迟也可能更高。 **temperature**-该指标衡量模型输出不太可能出现的词元的频率。数值越高temperature，输出结果越随机（通常也越有创意）。然而，这与“真实性”并不相同。对于大多数事实性应用场景，例如数据提取和真实问答，该temperature值为 0 时效果最佳。 **max_completion_tokens**（最大长度） - 不控制输出的长度，而是对词元生成的硬性限制。理想情况下，您不会经常达到此限制，因为您的模型会在认为完成或遇到您定义的停止序列时停止。 **stop**（停止序列） - 一组字符（标记），当生成这些字符时，将导致文本生成停止。","tags":["Prompt"],"categories":["Prompt","提示词"]},{"title":"Note标签美化","path":"/2025/05/12/noteupgrade/","content":"样式修改提示这是修改后的效果 再次提示支持多种风格 themes\\stellar\\source\\css\\_components\\tag-plugins ote.styl .md-text .tag-plugin.note position: relative padding: 0.5rem 0.8rem border-radius: 12px background: linear-gradient(135deg, #f8f9fa 0%, #ffffff 100%) box-shadow: 0 2px 8px rgba(0, 0, 0, 0.06) margin: 1rem 0 overflow: hidden color: var(--text-p1) // 黑暗模式适配 [data-theme=dark] background: linear-gradient(135deg, #1a1f2e 0%, #242936 100%) box-shadow: 0 2px 8px rgba(0, 0, 0, 0.25) @media (prefers-color-scheme: dark) background: linear-gradient(135deg, #1a1f2e 0%, #242936 100%) box-shadow: 0 2px 8px rgba(0, 0, 0, 0.25) // 标题样式 - 精简版 .title --fsp: $fsp2 font-size: 0.95rem line-height: 1.4 margin: 0 0 0.4rem 0 font-weight: 600 color: #667eea // 默认蓝色，与背景渐变呼应 display: flex align-items: center // 黑暗模式标题颜色 [data-theme=dark] color: #7ba3ff @media (prefers-color-scheme: dark) color: #7ba3ff // 移除默认的上边距 :first-child margin-top: 0 // 标题前的图标 ::before content: display: inline-block width: 16px height: 16px margin-right: 8px background: currentColor // 继承标题颜色 mask: url(data:image/svg+xml,%3Csvg xmlns=http://www.w3.org/2000/svg viewBox=0 0 24 24%3E%3Cpath d=M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z/%3E%3C/svg%3E) no-repeat center mask-size: contain flex-shrink: 0 // 内容样式 - 小字体 .body ,p --fsp: $fsp3 font-size: 0.9rem line-height: 1.5 margin: 0 color: #4a5568 // 黑暗模式内容颜色 [data-theme=dark] color: #b0b8c4 @media (prefers-color-scheme: dark) color: #b0b8c4 // 只有内容的情况 .body:only-child margin: 0// 颜色变体 - 图标和标题颜色与标签色一致.md-text .tag-plugin.note[color=blue] background: linear-gradient(135deg, #e8f4fe 0%, #ffffff 100%) .title color: #2196f3 // 蓝色 .title::before mask: url(data:image/svg+xml,%3Csvg xmlns=http://www.w3.org/2000/svg viewBox=0 0 24 24%3E%3Cpath d=M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-6h2v6zm0-8h-2V7h2v2z/%3E%3C/svg%3E) no-repeat center // 黑暗模式适配 - 蓝色 [data-theme=dark] background: linear-gradient(135deg, #1a2634 0%, #16202a 100%) .title color: #64b5f6 @media (prefers-color-scheme: dark) background: linear-gradient(135deg, #1a2634 0%, #16202a 100%) .title color: #64b5f6.md-text .tag-plugin.note[color=yellow] background: linear-gradient(135deg, #fff9e6 0%, #ffffff 100%) .title color: #ffb300 // 黄色 .title::before mask: url(data:image/svg+xml,%3Csvg xmlns=http://www.w3.org/2000/svg viewBox=0 0 24 24%3E%3Cpath d=M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z/%3E%3C/svg%3E) no-repeat center // 黑暗模式适配 - 黄色 [data-theme=dark] background: linear-gradient(135deg, #2d2616 0%, #241f12 100%) .title color: #ffd54f @media (prefers-color-scheme: dark) background: linear-gradient(135deg, #2d2616 0%, #241f12 100%) .title color: #ffd54f.md-text .tag-plugin.note[color=red] background: linear-gradient(135deg, #ffeaea 0%, #ffffff 100%) .title color: #f44336 // 红色 .title::before mask: url(data:image/svg+xml,%3Csvg xmlns=http://www.w3.org/2000/svg viewBox=0 0 24 24%3E%3Cpath d=M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-2h2v2zm0-4h-2V7h2v6z/%3E%3C/svg%3E) no-repeat center // 黑暗模式适配 - 红色 [data-theme=dark] background: linear-gradient(135deg, #2d1616 0%, #241212 100%) .title color: #e57373 @media (prefers-color-scheme: dark) background: linear-gradient(135deg, #2d1616 0%, #241212 100%) .title color: #e57373.md-text .tag-plugin.note[color=green] background: linear-gradient(135deg, #e8f5e8 0%, #ffffff 100%) .title color: #4caf50 // 绿色 .title::before mask: url(data:image/svg+xml,%3Csvg xmlns=http://www.w3.org/2000/svg viewBox=0 0 24 24%3E%3Cpath d=M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z/%3E%3C/svg%3E) no-repeat center // 黑暗模式适配 - 绿色 [data-theme=dark] background: linear-gradient(135deg, #162418 0%, #121c14 100%) .title color: #81c784 @media (prefers-color-scheme: dark) background: linear-gradient(135deg, #162418 0%, #121c14 100%) .title color: #81c784.md-text .tag-plugin.note[color=purple] background: linear-gradient(135deg, #f3e8ff 0%, #ffffff 100%) .title color: #9c27b0 // 紫色 .title::before mask: url(data:image/svg+xml,%3Csvg xmlns=http://www.w3.org/2000/svg viewBox=0 0 24 24%3E%3Cpath d=M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-6h2v6zm0-8h-2V7h2v2z/%3E%3C/svg%3E) no-repeat center // 黑暗模式适配 - 紫色 [data-theme=dark] background: linear-gradient(135deg, #24162a 0%, #1f1224 100%) .title color: #ba68c8 @media (prefers-color-scheme: dark) background: linear-gradient(135deg, #24162a 0%, #1f1224 100%) .title color: #ba68c8// 保持原有的特殊子元素样式.md-text .tag-plugin.note[color] code background: none.md-text .tag-plugin.note[child=codeblock] padding: 0 background: var(--theme-codeblock) border-radius: 8px .title color: var(--text) // 代码块标题使用默认文本颜色 .title, .body:only-child margin-top: 0 .body margin-bottom: 0 .highlight margin: 0 border: none background: none border-radius: 0 0 8px 8px figcaption span background: var(--theme-block) .highlight+.highlight border-top: 1px dashed var(--theme-border) border-top-left-radius: 0 border-top-right-radius: 0.md-text .tag-plugin.note[child=tabs] border-radius: 12px .body margin: 0 .tabs margin-top: .5rem border-radius: 0 0 12px 12px.md-text .tag-plugin.note[child=iframe] padding: 0 border-radius: 8px .title color: var(--text) // iframe标题使用默认文本颜色 .body margin: 0 iframe margin: 0 border-radius: 8px// 嵌套样式调整.md-text .tag-plugin .tag-plugin.note --gap-p: 0.75rem margin: 0.75rem 0 border-radius: 10px.l_body[type=story] .tag-plugin.note .title p:not([class]) text-indent: 0// 响应式设计@media (max-width: 768px) .md-text .tag-plugin.note padding: 0.6rem 0.8rem margin: 0.8rem 0 border-radius: 10px .title font-size: 0.9rem ::before width: 14px height: 14px margin-right: 6px .body ,p font-size: 0.85rem","tags":["hexo","note"],"categories":["博客","美化"]},{"title":"Hexo-tag-hint","path":"/2025/05/01/hinttool/","content":"给你的 hexo 博客添加关键词 hover 提示 github地址：https://github.com/etigerstudio/hexo-tag-hint 💡 Basic SyntaxInsert this little Nunjucks snippet anywhere you want to show your hints: % hint body_text hint_text % Where body_text is the normal post body text, hint_text is the text that should be presented inside the hint bubble. Use \\ to escape if necessary. E.g.: % hint misfits hexo-tag-hint % 📖 Multi-line SupportMulti-line hints are supported right now🎉. Append additional hint text lines to build a multi-line hint: % hint body_text hint_text_1st_line hint_text_2nd_line ... % E.g.: % hint Hexo A fast, simple powerful blog framework % 🔌 Install UpdateExecute following one-liners to integrate this plugin into your Hexo project, or to update the plugin: $ npm install hexo-tag-hint # install this plugin$ npm update hexo-tag-hint # update the plugin$ npm install hexo-tag-hint@latest # force update to latest version","tags":["hint","hexo"],"categories":["博客","美化"]},{"title":"BM25算法","path":"/2025/04/02/BM25/","content":"算法说明BM25（Best Matching 25）是一种信息检索领域的排名算法，用于计算查询（Query）与文档（Document）之间的相关性得分。它改进了传统的TF-IDF算法，引入文档长度归一化和词频饱和机制，使检索结果更准确。 公式表示： \\text{BM25}(D, Q) = \\sum_{q_i \\in Q} IDF(q_i) \\cdot \\frac{f(q_i, D)\\,(k_1 + 1)} {f(q_i, D) + k_1\\left(1 - b + b \\cdot \\frac{|D|}{avgdl}\\right)}IDF(q_i) = \\ln\\left( \\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} \\right)其中： f(qi,D) 是词项 qi在文档 D中的出现次数（TF）， N 是文档总数， n(qi) 是包含词项 qi的文档数， ∣D∣是文档 D的长度， avgdl 是所有文档的平均长度， k1 和 b是调整参数，通常设置为 k1=1.2和 b=0.75 。 简单示例假设我们有以下文档集合： 文档1：”我喜欢编程” 文档2：”编程很有趣” 查询：”他喜欢编程” 步骤： 分词：将文档和查询拆分为词。 文档1：[“我”, “喜欢”, “编程”] 文档2：[“编程”, “很”, “有趣”] 查询：[“他”, “喜欢”, “编程”] 计算BM25得分：使用rank_bm25库计算每个文档与查询的相关性。 代码实现import jiebafrom rank_bm25 import BM25Lclass BM25Search: def __init__(self, documents): # 初始化文档集合 self.documents = documents # 分词后的文档 self.tokenized_docs = [jieba.lcut(doc) for doc in documents] # 初始化BM25模型 self.bm25 = BM25L(self.tokenized_docs) logger.info(BM25模型初始化完成) def search(self, query): # 分词查询 tokenized_query = jieba.lcut(query) try: # 计算每个文档的BM25得分 scores = self.bm25.get_scores(tokenized_query) print(fscores--》scores) # 获取最高得分的文档索引 best_idx = scores.argmax() best_score = scores[best_idx] best_doc = self.documents[best_idx] logger.info(f查询: query, 最佳匹配: best_doc, 得分: best_score) return best_doc, best_score except Exception as e: logger.error(f检索失败: e) return None, 0.0 def main(): # 示例文档集合 documents = [我喜欢编程, 编程很有趣] # 初始化BM25检索器 bm25_search = BM25Search(documents) # 示例查询 query = 他喜欢编程 # 执行检索 result, score = bm25_search.search(query) if result: logger.info(f查询结果: result, 得分: score) else: logger.info(未找到匹配结果)if __name__ == __main__: main() 结果如下： 2025-04-02 19:01:27,463 - INFO - BM25模型初始化完成2025-04-02 19:01:27,464 - INFO - 查询: 他喜欢编程, 最佳匹配: 我喜欢编程, 得分: 1.0942025-04-02 19:01:27,464 - INFO - 查询结果: 我喜欢编程, 得分: 1.094","tags":["信息检索","排名"],"categories":["算法"]},{"title":"反编译 APK","path":"/2025/03/27/decodeapk/","content":"Android逆向反编译APK工具介绍如果只是想拿到apk中的图片资源，只需要将apk后缀改为zip然后解压缩，res目录中就包含了所有的资源文件 classes.dex 则包含了所有的代码，只是还无法查看 AndroidManifest.xml 文件打开会发现无法阅读，都是16进制数 此时就需要用到工具 —— ApkTool ApkTool下载ApkTool官网 安装 使用apktool d xxx.apk d 表示 decode 还可以加上一些附加参数来控制 decode 行为： -f ：如果目标文件夹已存在，则强制删除现有文件夹（默认如果目标文件夹已存在，则解码失败） -o ：指定解码目标文件夹的名称（默认使用 APK 文件的名字来命名目标文件夹） -s ：不反编译dex文件，也就是说 classes.dex 文件会被保留（默认会将 dex 文件解码成 smali 文件） -r ：不反编译资源文件，也就是说 resources.arsc 文件会被保留（默认会将 resources.arsc 解码成具体的资源文件） 反编译之后会得到以下内容： 1、AndroidManifest.xml：经过反编译还原后的 manifest 文件 2、original 文件夹：存放了未经反编译过、原始的 AndroidManifest.xml 文件 3、res 文件夹：存放了反编译出来的所有资源 4、smali 文件夹：存放了反编译出来的所有代码，只不过格式都是.smali类型的 xml文件已经可以看懂了，不过 smali 类型文件我们依然无法阅读 此时，需要用到另一个工具 —— dex2jar + jd-gui dex2jar功能将 dex 转换成 jar 形式文件 下载dex2jar官网 使用将下载的 dex2jar 压缩包解压后，可以看到以下内容 windows上使用dex2jar.bat即可 dex2jar.bat classes.dex路径 看到上述console则表示成功 代码都位于 classes-dex2jar.jar 中 现在需要用到另一款工具 jd-gui jd-gui下载jd-gui官网 根据需要下载对应包即可 使用 解压到本地，双击jd-gui.exe文件即可运行 用jd-gui打开之前解压出来的dex文件即可看到所有的源码 jadx-gui 一个更强大的工具，一款出色的 反编译工具 和 代码查看器，但不能直接编辑 APK 文件或内部代码 使用 Jadx-GUI 打开一个apk文件时，它会根据 Dalvik 字节码（DEX文件）反编译成可读的 Java 源代码，然而，这些源代码只是 Jadx 根据字节码猜测出来的，并不是原始的、可变翼德Java源文件，因此，无法直接在 Jadx-GUI 中修改这些反编译出来的 Java 代码。 下载 Jadx-GUI 使用起来也很简单，打开exe文件 然后点击打开文件/打开项目或者将apk文件直接拖拽过来即可查看；","tags":["Decode","安卓"],"categories":["学习","逆向工程"]},{"title":"Flash Attention","path":"/2025/03/20/flashattn/","content":"传统 Attention 的瓶颈要理解 Flash Attention，首先要明白标准 Transformer 中 Self-Attention 的计算和内存瓶颈。 先验知识HBM（High Bandwidth Memory）和SRAM（Static Random-Access Memory） HBM是一种高带宽内存接口，用于3D堆叠的SDRAM，具有较高的带宽和较低的功耗。 SRAM是一种静态随机访问存储器，用于高速缓存等内部存储器，具有更快的访问速度和更低的延迟，但成本更高且占用更多芯片空间。 MAC MAC（Memory Access Cost，存储访问开销）是指在计算机系统中，访问内存或存储器所需的时间和资源开销。它是衡量计算机程序或算法性能的重要指标之一。 MAC的值取决于多个因素，包括内存层次结构、缓存命中率、内存带宽、存储器延迟等。较低的MAC值表示访问内存的开销较小，而较高的MAC值表示访问内存的开销较大。 标准 Attention 的计算过程回顾对于一个输入序列，经过线性变换得到 Q, K, V 矩阵。核心的 Attention 计算步骤： S = QKᵀ (计算相似度得分矩阵，维度：[序列长度 N, 序列长度 N]) P = softmax(S / √dₖ) O = PV (加权求和，得到输出矩阵 O) 核心瓶颈 中间显存爆炸 (Memory-Bound) 这是最关键的问题。注意 S 和 P 的大小是 N²。 当 N 很大时（例如长文本、高分辨率图像），这个矩阵会变得极其巨大。 举例：N=1000, 数据类型 float32，仅 S 矩阵就需要 1000 * 1000 * 4 Bytes ≈ 4GB 的显存。N=16000 时，需要约 1TB 显存！这直接限制了模型可处理的序列长度。 传统实现会： 把 QKᵀ 写到显存 softmax 时又读回来 再写结果 再读结果做与 V 的乘法 …循环反复 导致： 显存 IO 占主导 序列越长（例如几千几万 token），越慢、越吃显存 Attention 不是算力瓶颈，而是 IO 瓶颈。 这就是 FlashAttention 要解决的核心问题。 FlashAttention FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness ​ 快速计算 节省内存 确切注意力 核心思想Flash Attention 的核心思想是一种 算法重排。它通过 “分块计算” 和 “增量更新” 的技术，在不显式生成和存储完整 S 和 P 矩阵的情况下，直接计算出正确的输出 O。 其目标非常明确： 减少对 HBM 的访问次数（从 O(N²) 降到 O(N) 级别），让计算不再受限于内存带宽。 避免存储中间矩阵，从而支持超长序列。 采用的方法： tiling recomputation tiling（分块） 注意，attention的计算涉及到softmax，不能简单分块 传统softmax计算方法： \\text{softmax}(x_j) = \\frac{e^{x_j}}{\\sum_{i=1}^k e^{x_i}} \\tag{2}softmax操作是row-wise的，即每行都算一次softmax，所以需要用到平铺算法来分块计算softmax。 【safe softmax】 原始softmax数值不稳定，为了数值稳定性，FlashAttention采用safe softmax，向量 ∈ R 的safe softmax 计算如下： $m(x) := \\max_i x_i\\quad $，$f(x) := \\begin{bmatrix} e^{x_1 - m(x)} \\cdots e^{x_B - m(x)} \\end{bmatrix}\\quad $，$\\ell(x) := \\sum_i f(x)_i\\tag{3}$ $\\text{softmax}(x) := \\frac{f(x)}{\\ell(x)}$ Flash Attention中的softmax可以看做online softmax 它维护两个额外的统计量，并允许我们 增量更新 这些统计量和输出。假设我们把输入向量 x 分成两部分处理：$x = [x^{(1)}, x^{(2)}]$。 当看到第一部分 $x^{(1)}$ 时： 计算本地统计量：$m_1 = \\max(x^{(1)})$， $\\ell_1 = \\sum e^{x^{(1)} - m_1}$。 此时的最佳估计输出：$o_1 = \\frac{e^{x^{(1)} - m_1}}{\\ell_1}$。 当看到第二部分 $x^{(2)}$ 时： 计算本地统计量：$m_2 = \\max(x^{(2)})$， $\\ell_2 = \\sum e^{x^{(2)} - m_2}$。 关键步骤：更新全局统计量 新的全局最大值：$m_{\\text{new}} = \\max(m_1, m_2)$ 新的全局指数和： 对于旧的 $o1$，它的指数是用旧的 $m_1$ 计算的，现在需要用新的 $m{\\text{new}}$ 来”修正”。 修正因子是 $e^{m1 - m{\\text{new}}}$。 因此，旧的 $\\ell1$ 需要缩放为 $\\ell_1 \\cdot e^{m_1 - m{\\text{new}}}$。 新的 $\\ell2$ 也需要用新最大值修正为 $\\ell_2 \\cdot e^{m_2 - m{\\text{new}}}$。 最终全局 $\\ell{\\text{new}} = \\ell_1 \\cdot e^{m_1 - m{\\text{new}}} + \\ell2 \\cdot e^{m_2 - m{\\text{new}}}$。 更新输出 修正旧的 $o_1$：$o1 = o_1 \\cdot e^{m_1 - m{\\text{new}}}$ (因为分母 $\\ell_{\\text{new}}$ 变了，分子也需要同步缩放)。 计算新的 $o_2$：$o2 = \\frac{e^{x^{(2)} - m{\\text{new}}}}{\\ell_{\\text{new}}}$。 最终输出：$o = \\text{concat}(o_1, o_2)$ softmax分块计算完整公式推导： $m(x) = m(\\begin{bmatrix} x^{(1)} x^{(2)} \\end{bmatrix}) = \\max(m(x^{(1)}), m(x^{(2)})),$ $f(x) = \\begin{bmatrix} e^{m(x^{(1)}) - m(x)} f(x^{(1)}) e^{m(x^{(2)}) - m(x)} f(x^{(2)}) \\end{bmatrix},$ $\\ell(x) = \\ell(\\begin{bmatrix} x^{(1)} x^{(2)} \\end{bmatrix}) = e^{m(x^{(1)}) - m(x)} \\ell(x^{(1)}) + e^{m(x^{(2)}) - m(x)} \\ell(x^{(2)}), \\tag{4}$ $\\text{softmax}(x) = \\frac{f(x)}{\\ell(x)}$ recomputation（重新计算）FlashAttention算法的目标：在计算中减少显存占用，从 大小降低到线性，这样就可以把数据加载到SRAM中，提高IO速度。 解决方案：传统Attention在计算中需要用到Q，K，V去计算S，P两个矩阵，FlashAttention引入softmax中的统计量 ，结合output O和在SRAM中的Q，K，V块进行计算。 具体实现： 反向传播需要什么？对于标准 Attention：$O = \\text{softmax}(QK^\\top / \\sqrt{d}) V$，反向传播需要计算损失 $L$ 对 $Q, K, V$ 的梯度。根据链式法则： $\\frac{dL}{dV} = P^\\top \\cdot \\frac{dL}{dO}$ （需要注意力矩阵 $P$） $\\frac{dL}{dP} = \\frac{dL}{dO} \\cdot V^\\top$ （需要 $V$） $\\frac{dL}{dS} = \\frac{dP}{dS} \\cdot \\frac{dL}{dP}$ （需要 $P$ 和 $\\frac{dL}{dP}$，其中 $\\frac{dP}{dS}$ 是 softmax 的局部梯度，计算它需要 $S$ 或 $P$） 结论：要计算对 $Q, K$ 的梯度，至少需要 $S$ 或 $P$ 矩阵。 而我们在前向传播中恰恰没有存储它们。 Flash Attention 的解决方案：从输出反推中间值既然前向没有存 $S$ 和 $P$，那就在反向传播需要的时候，当场重新算一遍。 但这带来了新的挑战：重算 $S = QK^\\top$ 仍然是 $O(N^2)$ 的 HBM 访问和计算，会拖慢反向传播。 Flash Attention 的精妙之处在于，它利用了前向传播已经计算并存储下来的少量信息，使得重计算变得高效。这些信息就是： 最终的输出 $O$ 每行（每个查询位置）的 softmax 统计量：$m$（最大值） 和 $l$（指数和） 重计算的过程（反向传播的双循环）：反向传播的数据流与前向传播完全镜像，也是一个外循环遍历 $Q$ 块，内循环遍历 $K/V$ 块。 当需要计算某个 $Q_i$ 块和 $K_j, V_j$ 块相关的梯度时： 重新加载 $Q_i, K_j, V_j$ 块到 SRAM（这些是输入，HBM 中一直有）。 在 SRAM 中重算该分块的注意力分数 $S_{ij} = Q_i K_j^\\top / \\sqrt{d}$。 利用存储的统计量 $mi$ 和 $l_i$ 快速重算该分块的注意力概率矩阵 $P{ij}$： 对于 $S_{ij}$ 的每一行 $s$，对应的 $m_i$ 是该行的全局最大值，$l_i$ 是该行的全局指数和。 可以直接计算出正确的行归一化因子：$P_{ij}^{(\\text{row})} = \\frac{e^{s - m_i}}{l_i}$。 注意：这里不需要再做一次完整的 Online Softmax，因为全局统计量 $m_i$ 和 $l_i$ 已知。这只是一个快速的逐元素指数和除法操作。 现在，我们在 SRAM 中有了 $P{ij} 和重算的 S{ij}$。 结合从上游传递过来的梯度 $\\frac{dL}{dOi}$（也分块加载），我们就可以在 SRAM 中本地计算出对 $Q_i, K_j, V_j$ 分块的梯度 $\\frac{dL}{dQ{ij}}, \\frac{dL}{dK_j}, \\frac{dL}{dV_j}$。 对这些局部梯度进行累加（例如，$\\frac{dL}{dQi}$ 由所有 $j$ 对应的 $\\frac{dL}{dQ{ij}}$ 累加而成），最终得到完整的梯度。","tags":["注意力机制"],"categories":["深度学习"]},{"title":"Prompt Tuning","path":"/2024/10/11/prompttuning/","content":"在说明什么是Prompt Tuning之前，先简单了解下NLP任务四范式 NLP任务四种范式第一范式基于「传统机器学习模型」的范式，如TF-IDF特征+朴素贝叶斯等机器算法第二范式基于「深度学习模型」的范式，如word2vec特征+LSTM等深度学习算法，相比于第一范式，模型准确有所提高，特征工程的工作也有所减少第三范式基于「预训练模型+fine-tuning」的范式，如Bert+fine-tuning的NLP任务，相比于第二范式，模型准确度显著提高，模型也随之变得更大，但小数据集就可训练出好模型第四范式基于「预训练模型+Prompt+预测」的范式，如Bert+Prompt的范式相比于第三范式，模型训练所需的训练数据显著减少 在整个NLP领域，整个发展历史都是朝着精度更高、少监督甚至无监督的方向发展，而 Prompt-Tuning 是目前学术界向这个方向进军最新也是最火的研究成果 Fine-Tuning 简单回顾下Fine-Tuning Fine-Tuning属于一种迁移学习方式，在自然语言处理（NLP）中，Fine-Tuning是用于将预训练的语言模型适应于特定任务或领域。Fine-Tuning的基本思想是采用已经在大量文本上进行训练的预训练语言模型，然后在小规模的任务特定文本上继续训练它 这种方式的痛点在于： 下游任务的目标和预训练的目标差距过大，可能导致过拟合 微调过程需要依赖大量监督语料 大语言模型参数量都极大，完全微调 Full Fine-Tuning 的代价很高 解决办法：Prompt-Tuning，通过添加模板的方法来避免引入额外的参数，从而让模型可以在小样本（few-shot）或零样本（zero-shot）场景下达到理想效果 Prompt-Tuning简述基于Fine-Tuning的方法是让预训练模型去迁就下游任务，而基于Prompt-Tuning的方法可以让下游任务去迁就预训练模型，其目的是将Fine-Tuning的下游任务目标转换为Pre-training的任务 工作过程定义一个句子：[CLS] I like the Disney films very much. [SEP] 传统的Fine-tuning方法: 将其通过BERT模型获得 [CLS] 表征之后再喂入新增加的MLP分类器进行二分类，预测该句子是积极的（positive）还是消极的（negative），因此需要一定量的训练数据来训练 Prompt-tuning执行步骤： 构建模板： (Template)生成与给定句子相关的一个含有[MASK]标记的模板. 例如It was [MASK], 并拼接到原始的文本中，获得Prompt-Tuning的输入：[CLS] I like the Disney films very much. [SEP] It was [MASK]. [SEP]，将其喂入BERT模型中，并复用预训练好的MLM分类器，即可直接得到[MASK]预测的各个token的概率分布 标签词映射： (Verbalizer)因为[MASK]只对部分词感兴趣，因此需要建立一个映射关系. 例如如果[MASK]预测的词是“great”，则认为是positive类，如果是“terrible”，则认为是negative类 训练：根据Verbalizer，则可以获得指定label word的预测概率分布，并采用交叉信息熵进行训练。此时因为只对预训练好的MLM head进行微调，所以避免了过拟合问题 挑战不同的任务应该有不同的template和label word，因此如何最大化的寻找当前任务更加合适的template和labelword是Prompt-tuning非常重要的挑战 Prompt-Tuning发展历程鼻祖 —— GPT3GPT-3开创性的提出了In-context Learning的思想. 即无需修改模型即可实现few-shot、zero-shot的learning. 同时引入了Demonstrate Learning, 即让模型知道与标签相似的语义描述，提升推理能力 这种方法存在一些问题： 其建立在超大规模预训练语言模型上，模型参数量通常超过10B，在真实场景中很难应用 该方法应用于参数规模小的模型时，效果会下降很多，因此后续提出Prompt-Tuning GPT3中提供的prompt过于简单，泛化性能低 因此，PET模型问世 PET模型 PET（Pattern-Exploiting Training）出自《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》（EACL2021），根据论文题目则可以看出，Prompt-Tuning启发于文本分类任务，并且试图将所有的分类任务转换为与MLM一致的完形填空. PET模型设计了两个很重要的组件 → Pattern、Verbalizer 简称PVP（Pattern-Verbalizer-Pair） Pattern（Template）: 记作T, 即上文提到的Template，其为额外添加的带有[mask]标记的短文本，通常一个样本只有一个Pattern, 由于不同的任务、不同的样本可能会有其更加合适的pattern，因此如何构建合适的pattern是Prompt-Tuning的研究点之一 Verbalizer: 记作V, 即标签词的映射，对于具体的分类任务，需要选择指定的标签词（label word）.例如情感分析中，我们期望Verbalizer可能是 （positive和negative是类标签）. 同样，不同的任务有其相应的labelword，但需要注意的是，Verbalizer的构建需要取决于对应的Pattern, 因此如何构建Verbalizer是另一个研究挑战 目前基于PVP框架, 最需要关注的问题是如何选择或构建合适的Pattern和Verbalizer . 一种简单的方法是根据特定任务的性质和先验知识人工设计模板. 注意：在同样的数据集和训练条件下. 选择不同的Pattern和Verbalizer会产生差异很大的结果 总结人工设计方法的缺陷： 采用人工构建的方法成本高，需要与领域任务相关的先验知识 人工设计的Pattern和Verbalizer不能保证获得最优解，训练不稳定，不同的PVP对结果产生的差异明显，方差大 在预训练阶段MLM任务并非完全按照PVP的模式进行训练的，因此人工构建的Pattern和Verbalizer使得Prompt-Tuning与MLM在语义和分布上依然存在差异 Prompt-Oriented Fine-Tuning 面向Prompt的Fine Tuning，该训练方法的本质是将目标任务转换为适应预训练模型的预训练任务，以适应预训练模型的学习体系 以情感分析为例： BERT Fine-TuningPrompt-Oriented Fine-TuningFine-Tuning流程：将训练文本经过Bert编码后，生成向量表征，再利用 该向量表征，连接全连接层，实现最终的情感类别识别，这种方式存在一 个显式的弊端：预训练任务与下游任务存在gap基本流程: 构建prompt文本：It was[MASK].，将prompt文本与输入text 文本text = The film isattractive.拼接生成: It was[MASK].The film is attractive.， 输入至预训练模型中，训练任务目标和MLM任务的目标一致，即识别被[MASK]掉的词 Prompt-Oriented Fine-Tuning方法中，预训练模型参数是可变的, 本质是Prompt-Tuning+Fine-Tuning的结合体. 该方法在Bert类相对较小的模型上表现较好，但是随着模型越来越大，如果每次针对下游任务，都需要更新预训练模型的参数，资源成本及时间成本都会很高，因此后续陆续提出了不更新预训练模型参数，单纯只针对prompt进行调优的方法 针对Prompt调优方法的分类：Hard Prompt 和 Soft Prompt 常见下游任务Prompt设计： 模板类别： Hard PromptSoft Prompt离散提示: 是一种固定的提示模板，通过将特定的关键词或短语(真实的文本字符串) 直接嵌入到文本中，引导模型生成符合要求的文本. 特点: 提示模板是固定的，不能根据不同的任务和需求进行调整. 缺陷：依赖人工，改变prompt中的单个单词会给实验结果带来巨大的差异连续提示：是指通过给模型输入一个可参数化的提示模板，从而引导模型生成符合 特定要求的文本. 特点: 提示模板中的参数可以根据具体任务和需求进行调整，以达到最佳的生成效果. 优点：不需要显式地指定这些模板中各个token具体是什么，而只需要在语义空间中 表示一个向量即可 Soft Prompt理解 基于Soft Prompt, 不同的任务、数据可以自适应地在语义空间中寻找若干合适的向量，来代表模板中的每一个词，相较于显式的token，这类token称为 伪标记（Pseudo Token) .下面给出基于连续提示的模板定义 假设针对分类任务，给定一个输入句子x，连续提示的模板可以定义为T=[x],[v1],[v2],…,[vn] [MASK]：其中[vn]则是伪标记，其仅代表一个抽象的token，并没有实际的含义，本质上是一个向量 Soft Prompt方法, 是将模板变为可训练的参数，不同的样本可以在连续的向量空间中寻找合适的伪标记，同时也增加模型的泛化能力. 因此, 连续法需要引入少量的参数并在训练时进行参数更新，但预训练模型参数是不变的，变的是prompt token对应的词向量（Word Embedding）表征及其他引入的少量参数 Prompt Tuning（NLG任务） Prompt Tuning 是2021年谷歌在论文《The Power of Scale for Parameter-Efficient Prompt Tuning》中提出的微调方法，该方法基于T5模型(最大参数11B)为每一个输入文本假设一个固定前缀提示，该提示由神经网络参数化，并在下游任务微调时进行更新，整个过程中预训练的大模型参数被冻结 ① 给定 n个tokens, 记作x1, …,xn, 通过一个预训练模型对应的embedding table，可以将n个token表示为一个向量矩阵(Xe-Rn*e). ② 将连续模板中的每个伪标记vi视为参数，可以通过另一个embedding table获得p个伪token标记为向量矩阵(Pe-Rp*e). ③ 将文本和Prompt拼接获得新的输入[Pe:Xe]-R(p+n)*e. ④ 新的输入喂入一个MLP获得新的表征. 注意，只有prompt对应的向量表征参数(Pe-Rp*e) 会随着训练进行更新 Prompt Tuning方法特点 优点 大模型的微调新范式 模型参数规模大了之后，可以将大模型参数固定，指定附加参数来适配下游任务，而且适配性能基本和全参数微调相当 缺点 在小样本学习场景上表现不太行 收敛速度比较慢 调参比较复杂 P-Tuning V1（NLU任务） P-Tuning 是2022年清华在论文《GPT Understands, Too》中提出的微调方法，P-Tuning V1方法的提出主要是为了解决这样一个问题：大模型的 Prompt 构造方式严重影响下游任务的效果 P-Tuning 提出将 Prompt 转换为可以学习的 Embedding 层，只是考虑到直接对Embedding 参数进行优化 P-Tuning V1 直接对 Embedding 参数进行优化会存在两个挑战： Discretenes(不连续性)： 对输入正常语料的 Embedding 层已经经过预训练，而如果直接对输入的 prompt embedding进行随机初始化训练，容易陷入局部最优 Association(关联性分析)：没法捕捉到 prompt embedding 之间的相关关系 用 MLP + LSTM 的方式来对 prompt embedding 进行一层处理 P-tuning 固定 LLM 参数, 利用多层感知机 (MLP)和LSTM 对 Prompt 进行编码，编码之后与其他向量进行拼接之后正常输入 LLM. 注意，训练之后只保留Prompt 编码之后的向量即可，无需保留编码器 对比Prompt Tuning Prompt Tuning 是将额外的 embedding 加在开头，看起来更像是模仿Instruction 指令；而 P-Tuning 的位置则不固定 Prompt Tuning 不需要加入 MLP 来参数初始化；而 P-Tuning 通过 LSTM+MLP来初始化 P-Tuning V2 P-Tuning V2是升级版本，主要解决P-Tuning V1 在小参数量模型上表现差的问题 P-Tuning v2 方法的核心思想：在模型的每一层都应用连续的 prompts, 并对 prompts 参数进行更新优化. 同时, 该方法也是针对 NLU 任务优化和适配的 超大规模参数Prompt-Tuning方法 近两年来，随着Prompt-Tuning技术的发展，对于超过10亿参数量的模型来说，Prompt-Tuning所带来的增益远远高于标准的Fine-tuning. 如GPT-3模型, 只需要设计合适的模板或指令即可以实现免参数训练的零样本学习 根本原因：模型参数量足够大，训练过程中使用了 足够多的语料，同时设计的 预训练任务足够有效 In-Context Learing 上下文学习In-Context learning（ICL）最早在GPT3中提出, 旨在从训练集中挑选少量的标注样本，设计任务相关的指令形成提示模板，用于指导测试样本生成相应的结果 zero-shot learning 给出任务的描述, 然后提供测试数据对其进行预测, 直接让预训练好的模型去进行任务测试 one-shot learning 给出任务的描述, 在进行新数据预测前, 插入一个样本做指导，相当于给一个例子让模型理解，然后再提供测试数据对其进行预测 few-shot learning 给出任务的描述, 在进行新数据预测前, 插入N个样本做指导. 相当于给N个例子让模型理解, 然后再提供测试数据对其进行预测 Instruction Learning 指令学习Instruction-Tuning和Prompt-Tuning的核心一样，就是去发掘语言模型本身具备的知识 其实Prompt-Tuning本质上是对下游任务的指令，简单的来说：就是告诉模型需要做什么任务，输出什么内容. 上文我们提及到的离散或连续的模板，本质上就是一种对任务的提示 因此, 在对大规模模型进行微调时, 可以为各种类型的任务定义指令, 并进行训练，来提高模型对不同任务的泛化能力 Prompt带女朋友去了一家餐厅，她吃的很开心，餐厅，她吃的很开心， 这家餐厅太_了！ Instruction判断这句话的情感：带女朋友去了一家餐厅，她吃的很开心。选项：A=好，B=一般， C=差 对比Prompt Prompt是去激发语言模型的补全能力，比如给出上半句生成下半句、或者做完形填空 Instruction-Tuning则是激发语言模型的理解能力，通过给出更明显的指令/指示，让模型去理解并做出正确的action Promp-Tuningt在没有精调的模型上也能有一定效果，但是InstructTuning则必须对模型精调, 让模型知道这种指令模式 Chain-of-Thought 思维链 (Chain-of-thought，CoT) 的概念是在 Google 的论文 “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models” 中被首次提出 思维链（CoT）是一种改进的提示策略，用于提高 LLM 在复杂推理任务中的性能，如算术推理、常识推理和符号推理 思维链是一种离散式提示学习，更具体地，大模型下的上下文学习（即不进行训练，将例子添加到当前样本输入的前面，让模型一次输入这些文本进行输出完成任务），相比于之前传统的上下文学习（即通过x1,y1,x2,y2,….xtest）作为输入来让大模型补全输出test），思维链多了中间的推导提示 用一个逻辑推理来简单理解什么是CoT 题目： 桌上有三个盒子：红盒子、蓝盒子、绿盒子。 红盒子里一定不是金币；蓝盒子可能有金币； 如果绿盒子里没有金币，那么蓝盒子一定有金币。 问：如果蓝盒子没有金币，金币在哪里？ 不用思维链 模型可能乱猜： 绿盒子 用思维链 让模型“说思考过程”： 假设蓝盒子没有金币 那么根据题意“若绿盒子没有金币，则蓝盒子一定有金币”，此时蓝盒子没有金币 → 绿盒子不可能没有 所以绿盒子里必须有金币 红盒子不可能有金币（题意说明） 因此金币在绿盒子里 通过链式推理，模型能得到正确答案 CoT分类Few-shot CoT ICL 的一种特殊情况，它通过融合CoT 推理步骤，将每个演示〈input，output〉扩充为〈input,CoT,output〉 Zero-shot CoT 直接生成推理步骤，然后使用生成的 CoT来导出答案.（其中 LLM 首先由 “Let’sthink step by step” 提示生成推理步骤，然后由 “Therefore, the answer is” 提示得出最终答案。他们发现，当模型规模超过一定规模时，这种策略会大大提高性能，但对小规模模型无效，显示出显著的涌现能力模式） PEFT PEFT（Parameter-Efficient Fine-Tuning）参数高效微调方法是目前大模型在工业界应用的主流方式之一，PEFT 方法仅微调少量或额外的模型参数，固定大部分预训练参数，大大降低了计算和存储成本，同时最先进的 PEFT 技术也能实现了与全量微调相当的性能 PEFT的优势： 该方法可以使 PLM 高效适应各种下游应用任务，而无需微调预训练模型的所有参数，且让大模型在消费级硬件上进行全量微调（Full Fine-Tuning）变得可行，这里的全量微调并不是真的全参数更新，只是能达到与之接近的效果 Prefix Tuning 2021年论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了 Prefix Tuning 方法，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定 任务形式 Prefix-Tuning 在输入前添加前缀，即z=[Prefix,x,y] ，Pidx为前缀序列的索引, |Pidx|为前 缀的长度。前缀索引对应着由θ参数化的向量矩阵 Pθ, 维度为|Pidx|×dim(hi). 注意：由于直接更新 Prefix 的参数会导致训练不稳定，作者在 Prefix 层前面加了 MLP 结构(相当于将Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果)，训练完成后，只保留 Prefix 的参数. 对比 对比P-TuningPrefix-Tuning 是将额外的embedding加在开头，看起来更像模仿Instruction指令，而PTuning 位置不固定.Prefix-Tuning 通过在每个层都添加可训练参数，通过MLP初始化，而P-Tuning只在输入的时候加入embedding, 并通过LSTM+MLP初始化. 对比Prompt-TuningPrompt Tuning 方式可以看做是Prefix Tuning 的简化，只在输入层加入 prompt tokens，并不需要加入MLP 进行调整来解决难训练的问题. Adapter Tuning 2019年谷歌的研究人员首次在论文《Parameter-Efficient Transfer Learning for NLP》提出针对BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕 不同于Prefix Tuning这类在输入前添加可训练 prompt参数，以少量参数适配下游任务，Adapter Tuning则是在预训练模型内部的网络层之间添加新的网络层或模块来适配下游任务. 当模型训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调 模型结构 首先是一个 down-project 层将高维度特征映射到低维特征. 然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征 同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构） LoRA 低秩适应（Low-Rank Adaptation）是一种参数高效的微调技术，其核心思想是对大型模型的权重矩阵进行隐式的低秩转换，也就是：通过一个较低维度的表示来近似表示一个高维矩阵或数据集 LoRA的产生： 上述Adapter Tuning 方法在 PLM 基础上添加适配器层会引入额外的计算，带来推理延迟问题；而Prefix Tuning 方法难以优化，其性能随可训练参数规模非单调变化，更根本的是，为前缀保留部分序列长度必然会减少用于处理下游任务的序列长度. 因此微软推出了LoRA方法 LoRA技术冻结预训练模型的权重，并在每个Transformer块中注入可训练层（称为秩分解矩阵），即在模型的Linear层的旁边增加一个“旁支”A和B。其中，A将数据从d维降到r维，这个r是LoRA的秩，是一个重要的超参数；B将数据从r维升到d维，B部分的参数初始为0。模型训练结束后，需要将A+B部分的参数与原大模型的参数合并在一起使用 PEFT发展时间线2019 -- Adapter 系列（PEFT 的起点）● 2019.06 — Adapter-BERT论文：Parameter-Efficient Transfer Learning for NLP 核心思想： 在 Transformer 每层插入一个小瓶颈网络（Adapter） 冻结原模型，只训练 Adapter 影响： 开启“只训练少量参数”的 PEFT 思路 ● 2019.12 — AdapterFusion论文：AdapterFusion: Non-destructive Task Composition… 作用： 可以组合多个任务的 Adapter 实现多任务知识融合 2020 -- Prompt 化 + 连续 Prompt 开始出现● 2020.12 — Prompt Tuning（离散提示）背景： GPT-3 展示了 Prompt Learning 的强大能力 人工设计 Prompt 不稳定，激发对可训练提示的需求 意义： 连续 Prompt（Soft Prompt）的研究开始萌芽 2021 -- Prompt 变成可训练向量（Soft Prompt 时代）● 2021.02 — GPT Prompt Tuning（Soft Prompt）论文：The Power of Scale for Parameter-Efficient Prompt Tuning 创新： 引入可训练的 Prompt Embedding（虚拟 token） 只训练几十个 embedding 即可完成新任务 意义： 计算成本极低 Soft Prompt 方法正式确立 ● 2021.04 — P-Tuning v1论文：P-Tuning: Prompt Tuning Can Be Comparable to Finetuning 核心创新： 使用“可训练连续 embedding + LSTM”生成 Prompt 在 BERT 类 encoder 模型上效果稳定 意义： Soft Prompt 被扩展到更多模型 ● 2021.09 — Prefix Tuning（Prefix-Tuning）论文：Prefix-Tuning: Optimizing Continuous Prompts for Generation 特点： 给 Transformer 所有层加入训练前缀参数 性能优于浅层 Soft Prompt 用途： 翻译、摘要、生成任务 ● 2021.10 — P-Tuning v2论文：P-Tuning v2: Prompt Tuning Can Be Comparable to FT on Large Models 特点： Prefix Prompt + 深层结构统一框架 性能接近 Full Fine-Tuning 适配 GPT / BERT / T5 行业影响： Prompt-based PEFT 的黄金版本 2021-2022 -- 真正的大爆发：LoRA● 2021.12（发布）/2022.02（论文）— LoRA论文：LoRA: Low-Rank Adaptation of Large Language Models 核心思想： 将权重增量参数化为低秩分解：W + BA 冻结原模型，只训练 A、B 效果： 训练成本降低 100 倍 性能接近全参数微调 意义： LoRA 成为 LLM 时代最主流的 PEFT 方法 2022 -- Prompt/Adapter/LoRA 多方法融合与优化● 2022.05 — IA3论文：IA3: Efficient Adaptation of Pretrained Transformers 方法： 为每层仅加入缩放向量（gating-like） 参数量比 LoRA 更小 ● 2022.07 — 大模型（LLaMA/BLOOM/GLM）兴起影响： LoRA 成为这些模型的事实标准微调方式 2023 -- SFT 大模型时代，新型 PEFT● 2023.03 — QLoRA论文：QLoRA: Efficient Finetuning of Quantized LLMs 创新： 在 4-bit 量化模型上使用 LoRA 显著节省显存，同时保持 Full FT 性能 意义： 单卡 3090/A10 就能训练 7B/13B 模型 PEFT 真正走向大众 ● 2023.08 — PETL（PEFT）系统性综述论文：Parameter-Efficient Transfer Learning: A Survey 意义： 首次完整整理所有 PEFT 方法 2024 -- LLM 后时代的 PEFT 优化方向● 2024 — DoRA（Decomposition of Rank-3） LoRA 改进版，分解矩阵 + 方向 稳定性更高、训练更可控 ● 2024 — AdaLoRA 动态调整 LoRA 的秩 提升参数效率 ● 2024 — LongLoRA 针对长上下文模型的 LoRA 改进 大幅提升长序列训练表现","tags":["tuning"],"categories":["提示词","微调","Tuning"]},{"title":"完整front-matter字段","path":"/2024/05/21/front_matter/","content":"Hexo 内置字段 参数 描述 默认值 layout 布局 config.default_layout title 标题 文章的文件名 date 建立日期 文件建立日期 updated 更新日期 文件更新日期 sticky 置顶（数字越大越靠前） comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章的永久链接，永久链接应该以 / 或 .html 结尾 null excerpt 纯文本的页面摘要。使用 该插件 来格式化文本 disableNunjucks 启用时禁用 Nunjucks 标签 /% % 和 标签插件 的渲染功能 false lang 设置语言以覆盖 自动检测 继承自 _config.yml published 文章是否发布 对于 _posts 下的文章为 true，对于 _draft 下的文章为 false 详见：https://hexo.io/zh-cn/docs/front-matter Stellar 特有字段 参数 描述 默认值 可用版本 wiki 该页面所属的 wiki 项目 menu_id 高亮的菜单按钮 id sidebar 侧边栏配置 1.0.0 ~ 1.26.8 leftbar 左侧边栏配置 1.27.0 ~ rightbar 右侧边栏配置 1.27.0 ~ comment_title 评论区标题 poster 文章封面，包含 topic/headline/caption/color 子配置 banner 页面顶部横幅背景 banner_info 横幅信息，包含 avatar/title/subtitle 子配置 logo 左侧边栏顶部 logo 区域信息，包含 icon/avatar/title/subtitle 子配置 indent 段落是否缩进 false topic 所属话题/专栏 1.25.0 ~ author 该文章的作者 1.23.0 ~ type 页面类型 1.26.0 ~ references 参考资料 h1 页内标题 title 1.26.0 ~ breadcrumb 面包屑导航 true indexing 页面能否是否能被搜索 true 第三方插件 参数 描述 类型 mathjax 渲染文章公式 boolean katex 同上 boolean mermaid 渲染图表 boolean","tags":["HEXO","博客","Stellar"],"categories":["博客"]},{"title":"模型蒸馏","path":"/2024/04/26/textcnn/","content":"TextCNN中的卷积操作卷积神经网络的核心思想是 捕捉局部特征，对于文本来说，局部特征就是 由若干单词组成的滑动窗口，类似 N-gram。卷积神经网络的优势在于能够自动的对 N-gram特征进行组合和筛选，获取不同抽象层次的语义信息 在图像中，这些局部特征可以是 边缘、纹理、形状 在文本中，这些局部特征可以是： ​ 情感短语：“太中了！“，“俺不中嘞” ​ 关键词组合：“股市 下跌”，“疫情 爆发” ​ …… 句子可以看成由词向量组成的二维矩阵 X∈R^{n×d}n 表示句子长度 d 表示词向量维度 每一行就是一个词的embedding 卷积核可以在这个矩阵上滑动，自动学习“哪些词组合表示什么语义”，从而达到文本分类目的 TextCNN的核心结构1. Embedding 层输入句子经过 embedding lookup 变成矩阵，词向量可以是： 随机初始化 预训练词向量（word2vec、GloVe、fastText） Transformer 输出的上下文向量（更高级） 2. 多尺寸卷积核提取 n-gram 特征不同高度的卷积核对应不同 n-gram： 宽度 = embedding 维度（固定） 高度 = n-gram 大小，如 2、3、4 例如，3×d 的卷积核可以捕捉 “三词短语” 模式。 卷积运算公式如下： c_i = f(W * X_{i:i+h-1} + b)得到长度为 n−h+1 的 feature map。 3. 最大池化（max-over-time pooling）对每个 feature map 取最大值 含义：保留该卷积核在整个句子中最强的激活，代表最重要的 n-gram 模式。 4. 全连接层 + Softmax 分类多个卷积核的池化结果拼接成向量，再输入全连接层实现分类。 TextCNN的超参数调参 参数名称 参数值 输入词向量 word2vec filter大小 (3,4,5) 每个size下的filter个数 100 激活函数 ReLU 池化策略 1-max pooling dropout rate 0.5 L2正则化 3 输入词向量表征：词向量表征的选取(如选word2vec还是GloVe) 卷积核大小：一个合理的值范围在1~10。若语料中的句子较长，可以考虑使用更大的卷积核。另外，可以在寻找到了最佳的单个filter的大小后，尝试在该filter的尺寸值附近寻找其他合适值来进行组合。实践证明这样的组合效果往往比单个最佳filter表现更出色 feature map特征图个数：主要考虑的是当增加特征图个数时，训练时间也会加长，因此需要权衡好。当特征图数量增加到将性能降低时，可以加强正则化效果，如将dropout率提高过0.5 激活函数：ReLU和tanh是最佳候选者 池化策略：1-max pooling表现最佳 正则化项(dropout/L2)：相对于其他超参数来说，影响较小点 使用Pytorch简单实现import torchimport torch.nn as nnimport torch.nn.functional as Fimport os​class Model(nn.Module): def __init__(self, config): super(Model, self).__init__() self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1) # 词嵌入层 self.convs = nn.ModuleList( [nn.Conv2d(1, config.num_filters, (k, config.embed)) for k in config.filter_sizes] ) # 卷积层列表，包含不同卷积核大小的卷积层 self.dropout = nn.Dropout(config.dropout) # 随机失活层 self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes) # 全连接层​ def conv_and_pool(self, x, conv): # 卷积和池化操作 x = F.relu(conv(x)).squeeze(3) x = F.max_pool1d(x, x.size(2)).squeeze(2) return x​ def forward(self, x): # 前向传播 out = self.embedding(x[0]) out = out.unsqueeze(1) # 对每个卷积层进行卷积和池化操作，然后拼接在一起 out = torch.cat([self.conv_and_pool(out, conv) for conv in self.convs], 1) out = self.dropout(out) # 随机失活 out = self.fc(out) # 全连接层 return out","tags":["模型","CNN","文本分类"],"categories":["深度学习"]},{"title":"有用的小知识","path":"/2024/03/01/justrecord/","content":"银屑病一些好用的药物他克莫司 补钙人体越越钙，吸收率越高，一般20% ~ 30%，缺钙的话可能提高到60 ~ 70，不缺钙可能会下降到10%左右（这是在正常摄入钙的情况下） 柠檬酸钙（较贵）碳酸钙（同样可以）每天补充 700mg 以上 补充氨基酸可以促进钙的吸收 所以，有复合产品 —— 氨基酸螯合钙等 钙补充多了会便秘（吸收率有限），多做运动保证吸收的钙能有效利用 多补充维生素D！！！ 最让人舒服的11种颜色RGB值和十六进制值 序号 名称 RGB 十六进制 1 豆沙绿 (199, 237, 204) #C7EDCC 2 银河白 (255, 255, 255) #FFFFFF 3 杏仁黄 (250, 249, 222) #FAF9DE 4 秋叶褐 (255, 242, 226) #FFF2E2 5 胭脂红 (253, 230, 224) #FDE6E0 6 青草绿 (227, 237, 205) #E3EDCD 7 海天蓝 (220, 226, 241) #DCE2F1 8 葛巾紫 (233, 235, 254) #E9EBFE 9 极光灰 (234, 234, 239) #EAEAEF 10 苹果绿 (183, 232, 189) #B7E8BD 11 豆沙绿-略暗 (204, 232, 207) #CCE8CF","tags":["随手一记","健康"],"categories":["生活"]},{"title":"张量索引切片操作","path":"/2024/02/21/tensor/","content":"大致按照使用频率递减给出 这里可以给出一个结论性的规律, 便于判断张量形状, 索引操作时, 有几个 : , 就有几个维度 1. 基本下标与切片（Python 风格）import torchx = torch.arange(24).reshape(2,3,4) # shape (2,3,4)# x[batch, row, col]x[0,0,0] # 标量 tensor(0)x[0, :, :] # 第一维=0，取出 shape (3,4)x[:, 1, :] # 所有batch, 第二个row - shape (2,4)x[..., 2] # 省略号，等价于 x[:, :, 2] - shape (2,3)x[0] # 等价于 x[0, :, :] - shape (3,4) 切片语法支持 start:stop:step（含 start，不含 stop），支持负数索引和步长。 例如 x[:, ::-1, :] 会在中间维度上反转顺序（返回 view 还是 copy 取决于实现；在 PyTorch 中 negative step 会返回 copy）。 2. 使用 None / np.newaxis（增加维度）y = torch.tensor([1,2,3]) # shape (3,)y[None, :] # shape (1,3)y[:, None] # shape (3,1) 常用于把向量转为列/行以便广播。 3. 布尔掩码（Boolean Masking）a = torch.tensor([0,5,2,7,3])mask = a 3 # tensor([False, True, False, True, False])a[mask] # tensor([5,7]) - 1D 输出，丢失原shape信息 masked_select(a, mask) 等价于 a[mask]。 当 mask 是多维且与 a 同 shape 时，结果是扁平的 1D 张量（按行主序提取元素）。 可用于筛样本、实现 padding 掩码筛选等。 4. 花式索引（整数数组索引 / Advanced Indexing）M = torch.arange(12).reshape(3,4) # shape (3,4)rows = torch.tensor([2,0])cols = torch.tensor([1,3])M[rows] # 按行选择 - shape (2,4)M[:, cols] # 按列选择 - shape (3,2)# 对应元素选择（pairwise）M[rows, cols] # 取 (2,1) 和 (0,3) - shape (2,) 若使用多个 1D 整数索引（同维度），会进行逐元素配对索引，输出长度等于索引数组长度。 若混合切片和整数数组索引，规则稍复杂：整数索引会先被应用，结果维度位置会消失或变为新维度。 例（多维）： T = torch.arange(2*3*4).reshape(2,3,4)idx0 = torch.tensor([0,1]) # 用于第0维idx1 = torch.tensor([2,0]) # 用于第1维T[idx0, idx1] # 逐对索引 - shape (2,4)# 等价于: torch.stack([T[0,2], T[1,0]], dim=0) 5. 广播与索引（注意形状）A = torch.arange(6).reshape(2,3) # (2,3)idx = torch.tensor([0,2]) # (2,)A[torch.arange(2), idx] # (2,) - 每 batch 对应列取值# torch.arange(2) 为 [0,1]，与 idx 配对 - 取 (0,0) 和 (1,2) 常用于按-batch 选择每个样本对应的索引（如分类预测的 top-k 判断）。 6. gather 与 scatter（按索引收集/写入，适用于高维批量操作）# gather 示例：从 src 中按 index 收集（需要指定 dim）src = torch.tensor([[10,11,12],[20,21,22]]) # shape (2,3)index = torch.tensor([[2,1,0],[0,2,1]]) # shape (2,3)torch.gather(src, dim=1, index=index)# - shape (2,3): [[12,11,10],[20,22,21]] gather 要求 index 与 src 在除了 dim 外的维度完全相同；返回与 index 同形状的张量。 常用于实现按位置取值（例如 beam-search、按预测索引从概率张量中取值）。 scatter_/scatter 用于把值写入指定位置，可做 one-hot 化或累积（有 reduce 参数）。 7. index_select / take（按维度选择）v = torch.tensor([10,20,30,40])torch.index_select(v, dim=0, index=torch.tensor([3,1])) # tensor([40,20])# 对于矩阵按行选：M = torch.arange(12).reshape(3,4)torch.index_select(M, dim=0, index=torch.tensor([2,0])) # shape (2,4) index_select 返回的顺序与索引一致；与 fancy indexing（M[idx]）相似，但有些后端实现行为细微不同（比如保留 contiguous 性）。 8. masked_fill, where（掩码赋值 / 条件选择）x = torch.tensor([1., -2., 3.])x.masked_fill(x 0, 0.) # 把负数置0torch.where(x0, x, torch.zeros_like(x)) # 条件选择，相当于 np.where where(cond, A, B) 返回与 A/B 广播后的形状相同的张量。 9. unsqueeze / squeeze 与 view/reshape（维度控制）a = torch.tensor([1,2,3]) # (3,)a.unsqueeze(0) # (1,3)a.unsqueeze(1) # (3,1)torch.squeeze(a.unsqueeze(0)) # 恢复 squeeze(dim) 只在指定维度为 1 时删除该维度。 reshape/view 会改变内存视图（view 要求连续 contiguous；reshape 在必要时会复制）。 10. Ellipsis ...（省略号）X = torch.randn(4,5,6,7)X[..., 0] # 等价 X[:, :, :, 0]X[0, ...] # 等价 X[0, :, :, :] 在不确定前面/后面维度数时非常有用，特别是在写通用层时。 11. 多维返回与维度插入（保持/丢失维度） 使用整数索引会减少维度（那一维被消除）； 使用切片或 None，或保持长度为 1 的索引会保留维度。 示例： t = torch.randn(2,3,4)t[0].shape # (3,4) -- 整数索引去掉第0维t[0:1].shape # (1,3,4) -- 切片保留第0维t[[0]].shape # (1,3,4) -- 用长度1的索引数组也保留 12. 视图（view）与 copy（内存/contiguous）相关注意 大多数简单切片和整型索引会返回原张量的 view（共享内存），但有些操作会返回 copy（例如带负步长的切片、某些高级索引）。 is_contiguous() 可以检查是否连续。若对返回的张量执行 view() 可能会报错，需先 .contiguous()。 s = torch.arange(6).reshape(2,3)t = s[:, ::-1] # 可能是 copy（不连续）t.is_contiguous() # 可能 Falset.contiguous().view(-1) # 安全 在 in-place 操作（如 t += 1）时，如果 t 与原张量共享内存，可能会影响原张量；对 copy 则无影响。 13. 反向传播（autograd）相关 索引、切片会保留计算图信息（如果原张量 requires_grad=True），因此从张量中取出的部分仍可对原张量反向传播。 但是，用高级索引赋值（x[idx] = something）不记录梯度；需要使用 scatter 或构造新的张量再计算 loss。 detach() 可以切断梯度传播（例如 x = x.detach()）。 示例（反向传播影响）： x = torch.randn(3, requires_grad=True)y = x[1] * 2y.backward() # 会为 x[1] 累积梯度，但 x[0], x[2] 为 0x.grad # tensor([0., 2., 0.]) 14. 常见用途与模式（实战片段） 按 batch 取样（分类概率取预测值）： probs = torch.randn(32, 10) # logits or probspred = probs.argmax(dim=1) # (32,)# 如果想从 probs 中收集每个 batch 对应预测的概率：selected = probs[torch.arange(32), pred] # shape (32,) padding mask（seq 长短不一）： seq = torch.randint(0, 100, (4,7)) # batch, seq_lenmask = (seq != PAD_TOKEN) # True 表示有效# 通过 mask 做池化：masked_sum = (embeddings * mask.unsqueeze(-1)).sum(dim=1) one-hot： idx = torch.tensor([0,2,1])onehot = torch.nn.functional.one_hot(idx, num_classes=4) # shape (3,4) 按索引更新参数（embedding lookup 与更新）： embedding = torch.nn.Embedding(num_embeddings=1000, embedding_dim=64)out = embedding(idx_tensor) # embedding 内部就是高级索引/ gather 实现","tags":["张量","运算","Pytorch"],"categories":["深度学习"]},{"title":"好用的工具","path":"/2024/01/10/nicetools/","content":"VPNEFCloudhttps://www.efc123.com/shop 一元机场https://xn--4gq62f52gdss.ink 特价机场 便宜好用https://xn--6nq44rc0n82k.com ICON阿里巴巴矢量图标库https://www.iconfont.cn FONT AWESOMEhttps://fontawesome.com Carbonhttps://yesicon.app/carbon/skill-level 封面图生成BackImagehttps://nav.rdonly.com/laboratory/bgimage/backimage.html Coverhttps://cover.ruom.top 在线图片压缩TinyPNGhttps://tinypng.com 支持 PNG / JPG / WebP 一次可上传 20 张（单张 ≤ 5MB） 平均可减少 60–80% 体积 保留高画质，肉眼几乎无损 操作步骤： 打开网站 拖入图片 下载压缩后的版本 👉 适合 博客封面、展示图、LOGO 等日常用途。 ILoveIMGhttps://www.iloveimg.com/compress-image 支持批量上传 同时压缩 JPG / PNG / GIF 提供在线编辑（裁剪、加水印等） 高质量壁纸网站WallRoomhttps://wallroom.io Wallhavenhttps://wallhaven.cc Unsplashhttps://unsplash.com 视频摘要生成NoteGPThttps://notegpt.io/","tags":["工具"],"categories":["效率"]},{"title":"Teacher Forcing","path":"/2024/01/01/teacher_forcing/","content":"概念 想象一下，你在教一个孩子造句，最直接的方法是你说一个词，让他跟读一个词，而不是让他从头猜到尾。 这种老师引导的思想就是Teacher Forcing。 Teacher Forcing 是训练自回归序列模型（比如RNN、LSTM、GRU、seq2seq、transformer 的自回归解码器等）时常用的一种训练策略 核心思想： 在训练时，模型每次不使用上一个state的输出作为下一个state的输入，而是直接使用训练数据的标准答案(ground truth)的对应上一项作为下一个state的输入。换句话说，训练时给模型看老师提供的正确前文，而不是模型自己预测的前文。这里的真实答案我们一般称作ground truth 用机器翻译任务简单举例： 任务：将英文“I love you”翻译成中文“我爱你”。 没有Teacher Forcing（自由运行）： 输入“I love you”，模型首先预测第一个词。假设它预测错了，输出成了“我恨”。 下一步，它将“我恨”作为输入，去预测下一个词。这已经走上了歧途，最终可能输出“我恨你这个世界”。 模型从这个完全错误的序列中学习，梯度会非常不稳定，训练效率极低。 使用Teacher Forcing： 输入“I love you”和起始符sos，模型预测第一个词。它的输出是“我恨”，但计算损失时是与真实值“我”比较。 关键一步：在下一个时间步，我们忽略模型输出的“我恨”，而是直接将真实值“我” 作为输入，让模型预测下一个词。 模型现在基于“我”来预测，更有可能输出“爱”。 再下一步，我们又将真实值“爱”作为输入，让模型预测，它更可能输出“你”。 这样，模型每一步都是在“正确上下文”的引导下进行学习。 由来 直到什么是Teacher Forcing后，来思考一个问题，为什么要用Teacher Forcing？ 加快收敛：使用真实前文能减少训练时错误级联，使梯度更稳定，学习更快 → 训练迭代过程早期的RNN预测能力非常弱，几乎不能给出好的生成成绩，如果某一个unit产生了垃圾结果，必然会影响后面一片unit的学习，teacher forcing最早的动机就是用来解决RNN的这个问题。 有效防止误差累积：在自由运行模式下，早期的一个小错误会作为后续步骤的输入，导致错误累计像滚雪球一样越来越大，这会使得模型难以从严重的错误中恢复学习。Teacher Forcing 彻底切断了错误传播的链条，确保每个时间步的输入都是干净的。 这是我认为最关键的两个原因，当然，网上还有其他说法，比如让训练过程高效并行化等等。 核心问题Exposure bias暴露偏差问题：在训练时，模型习惯于在完美的“真实数据”环境下进行预测；但在推理（测试）时，模型必须使用自己上一步生成的（可能不完美的）输出作为当前步的输入。这种训练和推理之间的环境差异被称为“曝光偏差”。这可能导致模型在推理时非常脆弱，一旦产生一个错误，后续输出就容易崩溃。 解决方案：1. 计划采样(Scheduled Sampling)这是一种经典的解决方案。在训练过程中，我们并不总是100%使用真实标签，而是以一个概率 p 使用真实标签，以概率 1-p 使用模型自己上一步的预测结果作为输入。这个概率 p 可以随着训练的进行逐渐减小（例如，从1.0线性衰减到0.5），让模型逐步从“有辅导”过渡到“自主推理”。 2.集束搜索(Beam Search)在预测单词这种离散值的输出时，一种常用方法是对词表中每一个单词的预测概率执行搜索，生成多个候选的输出序列。这个方法常用于机器翻译(MT)等问题，以优化翻译的输出序列。beam search是完成此任务应用最广的方法，通过这种启发式搜索(heuristic search)，可减小模型学习阶段performance与测试阶段performance的差异。————————————————版权声明：本文为CSDN博主「Alanaker」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_30219017/article/details/89090690 3.课程学习(Curriculum Learning)核心思想：先易后难 课程学习的灵感来源于人类的教育方式。我们不会教小学生微积分，而是从加减乘除开始。同样地，课程学习在训练模型时，先让模型学习“简单”的样本，再逐步过渡到“复杂”的样本。 如何定义“难”与“易”？ 这是课程学习的关键。难度衡量标准因任务而异，常见的有： 序列长度：对于文本，短句子通常比长句子简单。 词汇复杂度：句子中罕见词的比例越低，句子越简单。 …… 对于课程学习的思想，需要将训练集由易到难排列，同时根据epoch调整数据难度，对于目前任务场景来说，训练集少说也需要百万量级，对齐进行难易排序，难度和成本都非常高，所以不易实现。 代码实现 以一个机器翻译任务来简单实现teacher forcing，其实也比较简单，重点是思想 # 定义teacher_forcingteacher_forcing_ratio = 0.5def train(x, y, encoder, decoder, encoder_optimizer, decoder_optimizer, loss): # 对数据进行编码 [1, 6] - [1, 6, 256] encoder_output, encoder_hidden = encoder(x, encoder.init_hidden()) # print(fx: x.shape) # [1, 6] # print(fencoder_output: encoder_output.shape) # [1, 6, 256] # print(fencoder_hidden: encoder_hidden.shape) # [1, 1, 256] # 准备解码器参数 # 第一个参数: 中间语义张量C encoder_output_c = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device) # 将真是编码结果赋值 for idx in range(encoder_output.shape[1]): encoder_output_c[idx] = encoder_output[0, idx] # 第二个参数 decoder_hidden = encoder_hidden # 第三个参数: 开始字符 input_y = torch.tensor([[SOS_token]], device=device) # 定义损失值 my_loss = 0.0 # 定义真实翻译句子长度 y_len = y.shape[1] # 是否使用 teacher_forcing use_teacher_forcing = True if random.random() teacher_forcing_ratio else False if use_teacher_forcing: # 逐字符解码 for idx in range(y_len): # 送入解码器进行预测 output_y, decoder_hidden, atten_weight = decoder(input_y, decoder_hidden, encoder_output_c) # 当前时间步真实值 - 取出对应位置单词 target_y = y[0][idx].view(1) # 计算损失 my_loss += loss(output_y, target_y) # 更新input_y - 使用真实值 input_y = y[0][idx].view(1, -1) else: # 逐字符解码 for idx in range(y_len): # 送入解码器进行预测 output_y, decoder_hidden, atten_weight = decoder(input_y, decoder_hidden, encoder_output_c) # 当前时间步真实值 - 取出对应位置单词 target_y = y[0][idx].view(1) # 计算损失 my_loss += loss(output_y, target_y) # 得出当前时间步预测结果 topv, topi = torch.topk(output_y, 1) input_y = topi.detach() # 梯度清零 反向传播 参数更新 encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() my_loss.backward() encoder_optimizer.step() decoder_optimizer.step() # 返回损失 return my_loss.item() / y_len # 当前样本平均损失","tags":["优化","架构","模型"],"categories":["深度学习"]},{"title":"小技巧","path":"/2023/12/21/someskills/","content":"如何一次性删除所有以某关键词结尾的文件 其他类型删除以此类推 find . -type f -name *.ko -exec rm -rf \\; Git push报错ssh: connect to host github.com port 22: Connection refused fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. 这个错误表明 Git 无法通过 SSH（端口 22）连接到 GitHub。可能的原因和解决方法如下： 1. 检查 SSH 连接运行以下命令测试 SSH 连接：ssh -T git@github.com如果连接被拒绝，可能是网络或防火墙问题。 2. 改用 HTTPS 协议如果 SSH 被屏蔽，可以临时改用 HTTPS：git remote set-url origin https://github.com/用户名/仓库名.gitgit push（需要输入 GitHub 账号密码或个人访问令牌） 3. 尝试 SSH 端口 443如果 22 端口被屏蔽，GitHub 也支持通过 443 端口使用 SSH。编辑 ~/.ssh/config 文件：Host github.comUser xxxxqq.comHostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443然后再次测试 SSH 连接。 4. 检查防火墙/代理设置 确保本地防火墙或公司网络未屏蔽 SSH（端口 22/443） 如果使用代理，需配置 Git 使用代理：git config --global http.proxy http://代理地址:端口git config --global https.proxy https://代理地址:端口 5. 验证 SSH 密钥确保你的 SSH 密钥已添加到 GitHub：cat ~/.ssh/id_rsa.pub然后将内容粘贴到 GitHub Settings → SSH and GPG keys。 6. 检查仓库是否存在确认远程仓库地址正确且存在：git remote -v","tags":["技巧","命令"],"categories":["效率"]},{"title":"一键更换所有图片格式","path":"/2023/11/10/formatimage/","content":"如题, 要求是把 旧格式的图片引用： ![](imageurl) 批量替换为 新主题格式： % image imageurl % Python实现 这种方式需要电脑中有Python环境, 虽然配置环境也比较简单, 但是还是需要一些门槛 注意修改脚本中的文件夹路径为自己的即可 import osimport re# 要处理的文件夹路径，例如 Hexo 的 source/_postsROOT_DIR = ./source/_posts# 匹配旧格式：![](URL)pattern = re.compile(r!\\[\\]\\((https://[^)]*)\\))def replace_in_file(file_path): with open(file_path, r, encoding=utf-8) as f: content = f.read() # 替换为新格式：% image URL % new_content = pattern.sub(r% image \\1 %, content) if new_content != content: with open(file_path, w, encoding=utf-8) as f: f.write(new_content) print(Updated:, file_path)def walk_dir(root): for root_dir, _, files in os.walk(root): for filename in files: if filename.endswith(.md): full_path = os.path.join(root_dir, filename) replace_in_file(full_path)if __name__ == __main__: walk_dir(ROOT_DIR) print(Done.) VSCode一键替换 VSCode还是好用啊, 相比于写脚本来实现, VSCode的全局搜索支持正则表达式, 可以一键替换文件夹中所有内容 搜索：!\\[\\]\\((https:\\/\\/[^)]*)\\) 替换：% image $1 %"},{"title":"修改键盘F9为Home键","path":"/2023/10/22/changekey/","content":"修改键盘F9键功能,将其映射为HOME键 由于键盘是75键, 本身不具备HOME键, 但是日常和工作对HOME键还是挺有需要的(别问为什么不买全键的, 个人不太喜欢数字区, 对75键构造独钟) 回归正题, 如何实现 我采用的是修改注册表方式 网上能搜到比较多方案, 除了注册表, 一般都需要下载额外软件, 键盘驱动或者是windows软件来将按键功能映射为其他 所以, 修改注册表, 省时省力 修改注册表1.打开注册表 # 按下键盘上 win + r 并输入regidit 2.定位到按键修改路径 HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Keyboard Layout 3.在右侧区域新建二进制值 - 命名为 Scancode Map, 并填入下列值 00 00 00 00 00 00 00 0002 00 00 00 47 E0 43 0000 00 00 00 00 00 00 00 4.点击确定, 退出注册表, 重启电脑, 永久生效👍","tags":["方法","技巧"],"categories":["效率"]},{"title":"序列到序列模型","path":"/2023/06/29/encoder-decoder/","content":"encoder-decoder模型同时使用编码器和解码器. 它将每个task视作序列到序列的转换/生成（比如，文本到文本，文本到图像或者图像到文本的多模态任务）.Encoder-decoder模型通常用于需要内容理解和生成的任务，比如机器翻译 代表模型T5T5模型结构与原始的Transformer基本一致,除了做了以下几点改动 采用了一种简化版的Layer Normalization，去除了Layer Norm的bias，将Layer Norm放在残差连接外面 位置编码：T5使用了一种简化版的相对位置编码，每个位置编码都是一个标量，被加到logits上用于计算注意力权重。各层共享位置编码，但是同一层内，不同的注意力头的位置编码都是独立学习的 ​ 自监督预训练T5 在预训练阶段采用了类似于 BERT 和 GPT 的大规模自监督学习策略，但与这两个模型的设计不同的是，T5 使用了“文本到文本”的格式来处理任务。 预训练任务包括两种类型：Causal Language Modeling（因果语言建模）和 填空任务（Masked Language Modeling） 多任务微调除了使用大规模数据进行无监督预训练，T5模型还可以利用不同任务的标注数据进行有监督的多任务预训练，例如SQuAD问答和机器翻译等任务 数据集和参数作者对公开爬取的网页数据集Common Crawl进行了过滤，去掉一些重复的、低质量的，看着像代码的 文本等，并且最后只保留英文文本，得到数据集C4: the Colossal Clean Crawled Corpus 参数 取值 transformer层数 24 特征维度 768 transformer head数 12 总参数量 2.2亿 大模型主流架构Decoder-only LLM之所以主要都用Decoder-only架构，除了训练效率和工程实现上的优势外，在理论上是因为Encoder的双向注意力会存在低秩问题，这可能会削弱模型表达能力，就生成任务而言，引入双向注意力并无实质好处 而Encoder-Decoder架构之所以能够在某些场景下表现更好，大概只是因为它多了一倍参数。所以，在同等参数量、同等推理成本下，Decoder-only架构就是最优选择了","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"自回归模型","path":"/2023/06/27/autoregress/","content":"自回归模型 Autoregressive model，AR 本篇主要围绕GPT作为代表了解AR 2018年6月, OpenAI公司发表了论文“Improving Language Understanding by Generative Pre-training”《用生成式预训练提高模型的语言理解力》, 推出了具有1.17亿个参数的GPT（Generative Pre-training , 生成式预训练）模型 特点：decoder-only 基本原理：从左往右学习的模型，只能利用上文或者下文 AR模型通常用于生成式任务，在长文本的生成能力很强，比如NLG领域的任务：摘要、翻译、抽象问答 GPT模型架构 从上图可以很清楚的看到GPT采用的是单向Transformer模型, 例如给定一个句子[u1, u2, …, un], GPT在预测单词ui的时候只会利用[u1, u2, …, u(i-1)]的信息, 而BERT会同时利用上下文的信息[u1, u2, …, u(i-1),u(i+1), …, un] GPT采用了Transformer的decoder模块，但是做了修改 GPT移除了第二个encoder-decoder attention子层，只保留Masked Multi-head Attention子层和Feed Forward子层 对比经典Transformer采用6层decoder，GPT采用了12层 GPT训练过程 GPT训练过程包括两个阶段：1.无监督的预训练语言模型 2.有监督的下游任务fine-tuning 无监督的预训练语言模型给定句子U = [u1, u2, …, un], GPT训练语言模型时的目标是最大化下面的似然函数 L_1(U) = \\sum_i \\log P(u_i \\mid u_{i-k}, \\cdots, u_{i-1}; \\Theta) 上述公式具体来说是要预测每个词 u_i的概率，这个概率是基于它前面u_{i-k}到u_{i-1}个词，以及模型\\Theta，这里的k表示上文的窗口大小，理论上k取值越大，模型所能获取的信息越充足，能力越强 GPT是一个单向语言模型,模型对输入U 进行特征嵌入得到 transformer 第一层的输h0，再经过多层 transformer 特征编码，使用最后一层的输出即可得到当前预测的概率分布，计算过程如下 h_0=UW_e+W_pW_p表示单词位置编码 → [max_seq_len, embedding_dim]，W_e可以看作是单位矩阵 → [vocab_size, embedding_dim] 得到输入张量h0后, 要将h0传入GPT的Decoder Block中, 依次得到ht h_t = \\text{transformer\\_block}(h_{l-1}) \\quad l \\in [1,t] 最后通过得到的ht来预测下一个单词 P(u) = \\text{softmax}(h_t W^T_e) 有监督的下游任务fine-tuningGPT预训练后, 会针对具体的下游任务对模型进行微调. 微调采用的是有监督学习, 训练样本包括单词序列[x1, x2, …, xn]和label y. GPT微调的目标任务是根据单词序列[x1, x2, …, xn]预测标签y P(y|x^1,...,x^m) = \\text{softmax}(h_l^m W_y)其中Wy表示预测输出的矩阵参数, 微调任务的目标是最大化下面的函数 L_2 = \\sum_{(x,y)} \\log P(y|x^1, \\cdots, x^m)综合两个阶段的目标任务函数, 可知GPT的最终优化函数为： L_3=L_2+λL_1整体训练架构图： 数据集和参数GPT使用了BooksCorpus数据集,文本大小约5GB，包含7400w+的句子。这个数据集由7000本独立的、不同风格类型的书籍组成 参数 取值 transformer层数 12 特征维度 768 transformer head数 12 总参数量 1.17亿","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"自编码模型","path":"/2023/06/26/autoencoder/","content":"自编码模型 AutoEncoder model，AE 本篇主要围绕BERT作为代表了解AE 代表模型BERT BERT是2018年10月由Google AI研究院提出的一种预训练模型（Bidirectional Encoder Representation from Transformer） 特点：encoder-only 基本原理：在输入中随机MASK掉一部分单词，根据上下文预测这个词（不难联想到BERT的主要预训练任务MLM） AE模型通常用于内容理解任务（NLU），比如NLU中的分类任务：情感分析、提取式问答 BERT架构 BERT架构整体可以分为三个模块： 第一层Embedding模块 中间层Transformer模块 第三层预微调模块 Embedding模块该模块由三种embedding共同组成 Token Embeddings：词嵌入张量，第一个单词是CLS标志，可用于之后的分类任务 Segment Embeddings：句子分段张量，为了服务后续的NSP预训练任务 Positional Embeddings：位置编码张量 整个Embedding模块的输出张量是这三个张量的直接加和结果 双向Transformer模块依然可以看到上面那张图，BERT只使用了Transformer中的encoder部分，完全舍弃decoder部分 两大预训练任务也集中体现在训练Transformer模块中 预微调模块 经过中间层Transformer的处理后，Bert的最后一层根据下游任务需求做调整即可 比如对于sequence-level的分类任务，直接取第一个[CLS] token的 last_hidden_state，再加一层全连接层后进行softmax来预测最终的标签 BERT的预训练任务MLM（Masked Language Model） 带mask的语言模型训练 在原始训练文本中, 随机的抽取15%的token作为参与MASK任务的对象 80%的概率下, 用[MASK]标记替换该token 在10%的概率下, 用一个随机的单词替换token 在10%的概率下, 保持该token不变. NSP（Next Sentence Prediction） 下一句话预测任务 输入句子对(A, B), 模型来预测句子B是不是句子A的真实的下一句话. 所有参与任务训练的语句都被选中作为句子A 其中50%的B是原始文本中真实跟随A的下一句话(标记为IsNext, 代表正样本) 其中50%的B是原始文本中随机抽取的一句话(标记为NotNext, 代表负样本) 数据集与参数数据集：BooksCorpus (800M words) + English Wikipedia (2,500M words) 模型超参数： 参数 取值 transformer层数 12 特征维度 768 transformer head数 12 总参数量 1.15亿","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"语言模型开篇","path":"/2023/06/20/lmopening/","content":"大语言模型介绍 这里先给出大语言模型的进化树 不难看出，从左到右，分支模型架构大致可以分为 encoder-only，encoder-decoder，decoder-only 大语言模型（Large Language Model）：一种人工智能模型，旨在理解和生成人类语言，大语言模型可以处理杜仲NLP任务 → 文本分类、文旦、翻译、对话等 对于大预言模型的的大，主要体现在其参数量，一般认为，包含千亿级别以上参数的语言模型（目前定义参数量超过10B的模型为大语言模型）；当然，除了参数量，庞大的语料库也是其一大特点。 语言模型发展阶段第一阶段设计一系列的自监督训练目标（MLM、NSP等），设计新颖的模型架构（Transformer）, 遵循Pre-training和Fine-tuning范式. 典型代表是BERT、GPT、XLNet等;第二阶段逐步扩大模型参数和训练语料规模，探索不同类型的架构。典型代表是BART、T5、GPT-3等;第三阶段走向AIGC（Artificial Intelligent Generated Content）时代, 模型参数规模步入千万亿, 模型架构为自回归架构, 大模型走向对话式、生成式、多模态时代，更加注重与人类交互进行对齐，实现可靠、安全、无毒的模型. 典型代表是InstructionGPT、ChatGPT、Bard、GPT-4等. 语言模型技术发展第一阶段基于规则和统计的语言模型 → N-gram模型第二阶段神经网络语言模型第三阶段预训练语言模型第四阶段大语言模型 N-gram模型 基于规则和统计的模型的典型或者可以说就是N-gram模型 这里引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关 如果一个词的出现与它周围的词是独立的，那么我们称之为 unigram，也就是一元语言模型。 P(S)=P(W1)⋅P(W2)⋅…⋅P(Wn) 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 bigram。 P(S)=P(W1)⋅P(W2∣W1)⋅P(W3∣W2)⋯P(Wn∣Wn−1) 如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 trigram。 P(S)=P(W1)⋅P(W2∣W1)⋅P(W3∣W2,W1)⋯P(Wn∣Wn−1,Wn−2) 一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关，而这些概率参数都是可以通过大规模语料库来计算，比如三元概率： P(Wi|Wi-2)=Count(Wi-2Wi-1Wi)/Count(Wi-2Wi-1)简单理解上述公式，在语料库中，出现 我爱你 的次数比上 我爱 出现的次数 N-gram模型特点 采用极大似然估计，参数易训练 完全包含了前n-1个词的全部信息 可解释性强，直观易理解 上面说的是其优点，但是： 只能建模前n-1个词 随着n增大，参数空间指数增长 数据稀疏，会出现OOV问题（out of vocabulary） 泛化能力差 神经网络语言模型 基于N-gram语言模型以上的问题，以及随着神经网络技术的发展，人们开始尝试使用神经网络来建立语言模型 模型的输入: wt-n+1, …, wt-2, wt-1就是前n-1个词. 现在需要根据这已知的n-1个词预测下一个词 wt，C(w)表示单词w所对应的词向量 网络的第一层: 是将C(wt-n+1),..,C(wt-2), C(wt-1)这n-1个向量首尾拼接起来形成一个(n-1)*m大小的向量 网络的第二层: 定义一个全连接层, 通过全连接层后结果再使用tanh 激活函数进行处理 网络的第三层: 输出一共V个节点 (V代表语料的词汇总数), 本质上为一个全连接层. 每个输出节点y_i表示下一个词语为i的未归一化logits值. 最后使用 softmax 激活函数将输出值y进行归一化。得到最大概率值，就是我们需要预测的结果 相比于N-gram有更好的泛化能力，降低了数据稀疏带来的问题；但是对于长序列建模能力有限，可能会出现梯度消失问题。 基于Transformer的预训练语言模型 基于Transformer的预训练模型：包括GPT、BERT、T5等.这些模型能够从大规模通用文本数据中学习大量的语言表示，并将这些知识运用到下游任务中，获得较好的效果 预训练模型的使用方式： 预训练：在大规模数据集上事先训练神经网络模型，使其学习到通用的特征表示和知识 微调：在具体的下游任务中使用预训练好的模型进行迁移学习，以获取更好的泛化效果 预训练模型的特点： 优点：更强大的泛化能力，丰富的语义表示，可以有效防止过拟合 缺点：计算资源需求大，可解释性差等 大语言模型随着预训练模型参数的指数级提升，其语言模型性能也会线性上升。2020年，OpenAI发布了参数量高达1750亿的GPT-3，首次展示了大语言模型的性能。 大语言模型的特点： 优点：像“人类”一样智能，具备了能与人类沟通聊天的能力，甚至具备了使用插件进行自动信息检索的能力 缺点：参数量大，算力要求高、训练时间长、可能生成部分有害的、有偏见的内容等等 语言模型评估指标常见的有以下几种 Accuracy （准确率）: 模型预测正确的样本数量占总样本量的比重 Precision（精确率）: 在被识别为正类别的样本中，为正类别的比例 Recall（召回率）: 在所有正类别样本中，被正确识别为正类别的比例 BLEU 分数：评估一种语言翻译成另一种语言的文本质量的指标. 它将“质量”的好坏定义为与人类翻译结果的一致性程度. 取值范围是[0, 1], 越接近1, 表明翻译质量越好 ROUGE 指标：在机器翻译、自动摘要、问答生成等领域常见的评估指标. ROUGE 通过将模型生成的摘要或者回答与参考答案（一般是人工生成的）进行比较计算，得到对应的得分 PPL：用来度量一个概率分布或概率模型预测样本的好坏程度. PPL越小，标明模型越好 前三个相信都不陌生，机器学习就在使用了，涉及到的就是混淆矩阵中值得简单运算 这里主要介绍后面三个 BLEU BLEU 根据n-gram可以划分成多种评价指标，其中n-gram指的是连续的单词个数为n，实践中，通常是取N=1~4，然后对进行加权平均 基本步骤： 分别计算candidate句和reference句的N-grams模型，然后统计其匹配的个数，计算匹配度 公式：candidate和reference中匹配的 n−gram 的个数 /candidate中n−gram 的个数 candidate 候选值，即模型预测值 reference 参考值，即真实值 举个简单例子： 使用1-gram进行匹配： candidate: {it, is, a, nice, day, today} reference: {today, is, a, nice, day} 结果:其中{today, is, a, nice, day}匹配，所以匹配度为5/6 使用2-gram进行匹配： candidate: {it is, is a, a nice, nice day, day today} reference: {today is, is a, a nice, nice day} 结果:其中{is a, a nice, nice day}匹配，所以匹配度为3/5 以此类推，还是比较好理解的 不难发现，匹配个数越多，BLEU值越大，说明候选句子越好 但是，也会有特殊情况，比如： candidate: the the the the reference: The cat is standing on the ground 如果按照1-gram的方法进行匹配，则匹配度为1，显然是不合理的 解决办法就是：首先，计算一个单词在任意一个参考句子出现的最大次数, 然后用每个（非重复）单词在参考句子中出现的最大 次数来修剪—单词在候选句子的出现次数. 如下所示的公式 count_k=min(c_k,s_k)其中k表示在候选句子（candidate）中出现的第k个词语, ck则代表在候选句子中这个词语出现的次数，而sk则代表 在参考文本（reference）中这个词语出现的次数 代码实现先安装工具包 pip install nltk from nltk.translate.bleu_score import sentence_bleudef cumulative_bleu(reference, candidate): bleu_1_gram = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)) bleu_2_gram = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)) bleu_3_gram = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)) bleu_4_gram = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)) return bleu_1_gram, bleu_2_gram, bleu_3_gram, bleu_4_gram# 生成文本generated_text = [This, is, some, generated, text]# 参考文本列表reference_texts = [[This, is, a, reference, text]]# 计算 Bleu 指标c_bleu = cumulative_bleu(reference_texts, generated_text)# 打印结果print(The Bleu score is:, c_bleu)#The Bleu score is: (0.6, 0.387, 1.5e-102, 9.2e-155) ROUGE ROUGE指标与BLEU指标非常类似，均可用来衡量生成结果和标准结果的匹配程度，不同的是ROUGE基于召回率，BLEU更看重准确率。 ROUGE也分为四种方法：ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S 这里仅介绍ROUGE-N 还是举个简单例子说明： 使用ROUGE-1进行匹配： candidate: {it, is, a, nice, day, today} reference: {today, is, a, nice, day} 结果:其中{today, is, a, nice, day}匹配，所以匹配度为5/5=1,这说明生成的内容完全覆盖了参考文本中的所有单词，质量较高 代码实现先安装工具包pip install rouge import Rouge# 生成文本generated_text = This is some generated text.# 参考文本列表reference_texts = [This is another generated reference text.]# 计算 ROUGE 指标rouge = Rouge()scores = rouge.get_scores(generated_text, reference_texts[0])# 打印结果print(ROUGE-1 precision:, scores[0][rouge-1][p])print(ROUGE-1 recall:, scores[0][rouge-1][r])print(ROUGE-1 F1 score:, scores[0][rouge-1][f])# ROUGE-1 precision: 0.8# ROUGE-1 recall: 0.6666666666666666# ROUGE-1 F1 score: 0.7272727223140496 PPL 困惑都（perplexity） PPL用来度量一个概率分布或概率模型预测样本的好坏程度 给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好 公式： 两种形式 {PPL}(S) \\;=\\; \\frac{1}{\\sqrt[N]{P(w_1w_2...w_N)}}{PPL}(S) \\;=\\; 2^{-\\frac{1}{N}\\sum_{i=1}^{N}\\log_2 P(w_i)}从公式可以看出，句子概率越大，语言模型越好，困惑度越小 代码实现import math# 定义语料库sentences = [ [I, have, a, pen], [He, has, a, book], [She, has, a, cat]]# 定义语言模型unigram = I: 1/12, have: 1/12, a: 3/12, pen: 1/12,He: 1/12, has: 2/12,book: 1/12,She: 1/12, cat: 1/12# 计算困惑度perplexity = 0for sentence in sentences: sentence_prob = 1 for word in sentence: sentence_prob *= unigram[word] temp = -math.log(sentence_prob, 2)/len(sentence) perplexity+=2**tempperplexity = perplexity/len(sentences)print(困惑度为：, perplexity)# 困惑度为： 8.15 LLM架构类别这一块内容比较多，分开几章讲，这里简单介绍 LLM本身基于Transformer架构，原始Transformer为不同领域模型提供了灵感和启发 基于原始Transformer衍生出一系列模型，一些模型仅使用encoder，一些模型仅使用decoder，有些模型同时使用encoder和decoder LLM大致可以分为三种：自编码模型（encoder）、自回归模型（decoder）和序列到序列模型（encoder-decoder）","tags":["LM","语言模型"],"categories":["深度学习"]},{"title":"HEXO部署方案","path":"/2023/04/21/hexodeploy/","content":"hexo是一个无后端的博客系统, 不像wordpress, typecho可以直接线上编写文章, 需要在本地编写md文件, 然后编译成html上传到服务器, 整个过程还是比较繁琐, 所以, 想寻找hexo写作流的最佳方案 本地Hexo环境搭建安装必要软件 安装Git从Git官网下载并安装Git：https://git-scm.com/ 安装Node.js从Node.js官网下载并安装：https://nodejs.org/安装后验证： node -vnpm -v 安装和初始化Hexo 安装Hexo CLI npm install -g hexo-cli 初始化Hexo项目 hexo init myblogcd myblognpm install 本地测试 hexo server 访问 http://localhost:4000 查看博客效果。 项目结构说明myblog/├── _config.yml # 站点配置文件├── source/ # 文章和页面│ └── _posts/ # 文章目录├── themes/ # 主题目录├── scaffolds/ # 模板文件└── public/ # 生成的静态文件 服务器环境配置（无apt工具）由于你的服务器没有apt工具，推测可能是CentOS/RHEL系系统，使用yum包管理器。 安装必要软件 安装Node.js # 添加NodeSource仓库（以Node.js 14.x为例）curl -sL https://rpm.nodesource.com/setup_14.x | sudo -E bash -# 安装Node.jssudo yum install -y nodejs 安装Git sudo yum install -y git 安装Nginx sudo yum install -y nginx 配置服务器Git仓库 创建Git用户和仓库 sudo useradd gitsudo passwd git # 设置密码su - gitmkdir -p ~/blog.gitcd ~/blog.gitgit init --bare 创建网站根目录 sudo mkdir -p /var/www/hexosudo chown git:git /var/www/hexosudo chmod 755 /var/www/hexo 配置Git钩子在 /home/git/blog.git/hooks/ 目录创建 post-receive 文件： # 切换到git用户su - git# 进入hooks目录cd /home/git/blog.git/hooks# 创建post-receive文件cat post-receive EOF#!/bin/bashecho 开始部署Hexo博客...# 设置路径GIT_DIR=/home/git/blog.gitWORK_TREE=/var/www/hexo# 创建网站目录（如果不存在）mkdir -p $WORK_TREE# 检出文件到网站目录while read oldrev newrev refnamedo branch=$(git rev-parse --symbolic --abbrev-ref $refname) if [ $branch = master ]; then echo 正在部署master分支到网站目录... git --work-tree=$WORK_TREE --git-dir=$GIT_DIR checkout -f $branch # 检查是否部署成功 if [ $? -eq 0 ]; then echo ✅ 博客部署成功！ echo 网站文件已更新到: $WORK_TREE # 显示部署的文件列表 echo 部署的文件: ls -la $WORK_TREE/ | head -10 else echo ❌ 部署失败！ exit 1 fi fidoneEOF 赋予执行权限： chmod +x /home/git/blog.git/hooks/post-receive# 将钩子文件的所有者改为git用户sudo chown git:git /home/git/blog.git/hooks/post-receive# 检查修正后的权限ls -la /home/git/blog.git/hooks/post-receive# 将整个blog.git目录的所有权交给git用户sudo chown -R git:git /home/git/blog.git# 修复网站目录权限sudo chown -R git:git /var/www/hexosudo chmod -R 755 /var/www/hexo 配置Nginx编辑 /etc/nginx/nginx.conf 或 /etc/nginx/conf.d/hexo.conf： server listen 80; server_name your-domain.com; # 替换为你的域名或IP root /var/www/hexo; index index.html; location / try_files $uri $uri/ =404; 重启Nginx： sudo systemctl start nginxsudo systemctl enable nginx 配置Hexo多平台部署配置GitHub Pages仓库 创建GitHub仓库 仓库名格式：用户名.github.io 设置为public仓库 配置SSH密钥 ssh-keygen -t rsa -C your-email@example.com 将公钥id_rsa.pub内容添加到GitHub的SSH keys中。 配置Hexo部署设置修改Hexo项目中的 _config.yml 文件： # 部署配置[citation:6]deploy: type: git repo: # 服务器仓库 - git@your-server-ip:/home/git/blog.git # GitHub仓库 - git@github.com:yourusername/yourusername.github.io.git branch: master 这里注意, github 上的主分支可能是 main, 可以直接在配置文件修改, 也可以部署好后进入github settings中修改主分支, 网上资料也比较多 安装部署插件npm install hexo-deployer-git --save 完整的写作和部署流程日常写作流程 创建新文章 hexo new 文章标题 编辑文章在 source/_posts/ 目录下找到对应的Markdown文件进行编辑。 本地预览 hexo clean hexo generate hexo server 部署到双平台 hexo clean hexo generate hexo deploy 访问直接访问 用户名.github.io 或者自己的域名都可以访问博客","tags":["HEXO","博客"],"categories":["博客"]},{"title":"卸载python要先安装python？","path":"/2023/01/01/uninstallpy/","content":"最近卸载旧版python时发现这么一个错误 大致能看出来是权限问题或者文件被锁定，Config.MSI 目录是 Windows Installer 用于存放安装回滚脚本（.rbf/.rbs）的隐藏系统文件夹。安装程序完成后会自动删除这些文件，但如果安装中途失败或缺乏权限，文件可能残留未被清理。此外，如果相关的 Python 安装程序或其它程序（如 Windows Installer 服务自身）正在使用这些 *.rbf 文件，则直接删除时会提示“拒绝访问”。因此常见原因包括： Config.MSI 文件夹默认隐藏且标记为受保护系统文件，需要管理员权限才能修改。 之前的安装/卸载未正常完成，回滚文件遗留在 Config.MSI 中未被删除。 目标文件正被系统或其它进程占用（例如 Windows Installer 进程还在运行），导致无法删除。 用户权限不足（未以管理员身份运行），无法修改或删除该文件。 权限不足不让删除的话，那手动删除是否安全呢网上答案众说纷纭，但结论是没啥影响 GPT给出的答案是：Config.MSI 文件夹中的 .rbf 文件只是安装时的回滚脚本备份，不会影响正常系统运行。多位技术专家指出，如果这些文件在卸载后未被自动删除，是可以手动删除或忽略的superuser.comcnblogs.com。例如，superuser 上的回答引用技术博客指出：“配置文件夹包含安装时的备份脚本，成功安装后应自动删除…如果安装程序没有清除它们，你可以安全删除该文件夹和文件”superuser.com。中文技术博客也说明：Windows 安装过程中产生的 Config.MSI 文件夹可以自行删除以释放空间cnblogs.com。因此，一般情况下**直接删除 Config.MSI 文件夹及其中的 \\.rbf 文件是安全的**，这不会破坏系统，只是移除了无用的回滚备份。 OK，既然这样，那就用管理员权限给它删掉吧 隐藏文件展示这类文件默认不会展示 方法一：通过控制面板进入“文件夹选项”这是 Win11 中最稳定、最通用的方式。 按下 Win + R 输入： control folders 然后回车 👉 会直接打开 “文件夹选项”窗口（老界面） 切换到 查看(View) 选项卡 在高级设置中向下滚动，找到： “隐藏受保护的操作系统文件（推荐）” 取消勾选 → 系统会弹出警告 → 点击 确定 即可。 方法二：通过控制面板图形化入口如果你想通过界面一步步点进去： 打开 控制面板 可以按 Win 键然后输入 控制面板 搜索 点击 外观和个性化 点击 文件资源管理器选项 👉 会打开“文件夹选项” 切换到 查看(View) 标签 找到并取消： 隐藏受保护的操作系统文件（推荐） 操作完后应该就可以再D盘看到Config.MSI文件夹 删除Config.MSI管理员权限打开CMD，运行下面命令 takeown /F D:\\Config.MSI /R /D Yicacls D:\\Config.MSI /grant Administrators:F /T 以上命令将把 D:\\Config.MSI 文件夹及子文件的所有权授给管理员组，让你拥有完全控制权 然后执行 rd /S /Q D:\\Config.MSI 操作完后查看D盘就没有Config.MSI文件夹，此时再去尝试卸载旧版本python，如果删掉了，恭喜 如果没删掉，按照下面操作来 通过安装包卸载 确实很反直觉啊，python的卸载不能在控制面板中直接删除，而是需要通过安装包，没错，也就是exe文件来删除，这类文件一般安装好程序后我都不会再保留，所以还需要去官网下载对应版本 比如我的版本是3.8.6 右键exe文件以管理员身份运行，会看到三个选项，选择uninstall卸载即可","tags":["程序","卸载"],"categories":["Python"]},{"title":"分类","path":"/categories/index.html","content":""},{"title":"标签","path":"/tags/index.html","content":""},{"title":"Self Attention","path":"/wiki/note/attention.html","content":"为什么需要注意力机制 Transformer出现之前, 对于NLP任务的主力模型是: RNN LSTM/GRU 这些模型都承担着一个艰难任务： 在一个长序列中，理解词与词之间的关系，尤其是远距离的关系。 但它们有一个共同的致命限制： 不能让每个词看到全局 信息被压得太狠，丢失严重 训练难以并行，效率低下 注意力机制正是为了解决这些根本性问题而诞生的。 为什么RNN/LSTM处理不好全局依赖单向传递信息RNN/LSTM 的信息流是链式的： x1 → x2 → x3 → x4 → ... 这意味着： x5 想知道 x1 的信息，要经过 x2 → x3 → x4 的层层传递 一旦中间丢失，就再也补不回来 越远的关系，越容易消失 所以像这种句子对它们非常困难： “我昨天看了一部电影，电影主角是….剧情非常的跌宕起伏,….它非常好看。” “它” 与 “电影” 的距离太远，RNN 容易“忘”。 梯度消失/梯度爆炸链式结构导致反向传播也需要一层层回传。 传统 RNN 的隐藏状态更新公式： ht=tanh(Wht−1+Uxt)反向传播时梯度会不断乘以权重矩阵 WWW 和 tanh 的导数 Dn=σ′(z1)w1⋅σ′(z2)w2⋅⋯⋅σ′(zn)wn不难发现, 梯度的计算呈连乘形式 当序列很长时： 梯度不断变小 → 消失 或不断变大 → 爆炸 这让模型学不到长期关系。 当然,LSTM/GRU 在梯度消失/爆炸问题已经做了优化, 通过门控机制控制信息流, 使一部分状态以近似线性方式在序列中传播，从而避免梯度在时间维度上的指数级衰减或爆炸，解决传统 RNN 的长期依赖问题 串行计算RNN系列模型的特点: 当前时间步必须等上一时间步走完才能开始 因此, 训练速度很慢 而 Attention 可以并行计算, 一次矩阵乘法全部计算出来, 并行计算, 充分利用GPU 为什么不用CNNCNN 虽然可以并行，但存在一个无法突破的问题： CNN 的感受野是局部的。 想让 CNN 跨越长距离依赖，需要不断叠加卷积层扩展感受野。 5层感受野可能只有几十个词 要覆盖整个句子需要几十层甚至更多 训练难度非常大。 隐藏状态是对信息的强压缩RNN 把之前所有词的信息压进一个固定维度的张量： h_t = f(x_t, h_t-1) 无论你前面说了 100 个词，还是 1000 个词，最终信息都必须塞进一个固定长度的 h。 上图是seq2seq架构处理翻译任务, seq2seq架构包括三部分, encoder(编码器), decoder(解码器), 中间语义张量c 对于图中的案例, 编码器首先处理中文输入 “欢迎 来 北京”, 通过GRU模型获得每个时间步的输出张量, 最后将它们拼接成一个中间语义张量c, 接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言 decoder只能依赖固定长度的中间语义张量c 也就是说, 无论句子多长, 多复杂, 都要压缩成一个固定维度张量 结果就是: 信息丢失严重 长序列问题效果差 注意力机制如何解决这些痛点每个词直接看到所有词在注意力里，一个词无需通过第 2、3、4、5 个词才能知道第 1 个词的信息。 它可以直接算： 相关性 = Query(当前词) · Key(所有词) 这是 完全平等的全局视野。 不存在梯度消失问题因为注意力不依赖链式结构，反向传播只会经过几层矩阵运算，梯度非常稳定。 完全可并行化（压倒 RNN 的关键优势）RNN 每一步都依赖上一部 ⇒ 只能串行 Attention 所有词之间的关系都可同时计算 ⇒ 一次矩阵乘法 速度差几十倍甚至上百倍。 这是 Transformer 能训练巨型模型的根本原因。 注意力机制如何计算 这里简要概括, 详细过程推荐知乎猛猿的一篇文章, 这里给出链接 Transformer学习笔记二：Self-Attention（自注意力机制）https://zhuanlan.zhihu.com/p/455399791 Transformer 使用的是缩放点积注意力(Scaled Dot-Prodcut Attention) 对于每个token, 产生三个矩阵 Query, Key, Value(下文简称 Q, K, V) Q（Query）：我想找什么？ K（Key）：我有哪些信息？ V（Value）：对应的信息内容 \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V(1) Q 与 K 做点积：计算“匹配程度”score = Q • Kᵀ 相似度越高 → 代表 Query 认为这个 Key 更相关。 (2) 缩放（除以 √d）防止数值过大score = score / √d (3) Softmax → 转成概率分布（注意力权重）attention_weights = softmax(score) (4) 用权重加权求和 Value，得到最终输出output = attention_weights × V 自注意力机制对于一般情况下, Query来自decoder, Key, Value来自encoder 自注意力则是一种特殊情况, Query, Key, Value均来自同一个序列, 即 Q=XW^Q,K=XW^K,V=XW^V","tags":[null,null],"categories":[null]},{"title":"追星Transformer","path":"/wiki/note/index.html","content":"O1 搭建知识库 正常 72% KR1 添加《初始Transformer》 对Transformer的大致了解 已完成 100% KR2 添加《Self Attention》 关于Transformer中的注意力机制，自注意力、多头注意力… 正常 90% KR3 添加《Position Encoding》 关于Transformer中的位置编码实现 已完成 100% KR4 关于Transformer模型家族，Bert、GPT等 未完成 0%"},{"title":"Positional Encoding","path":"/wiki/note/posencoding.html","content":"首先抛出一个问题, 为什么Transformer 要有位置编码? — 对于任何一门语言, 单词在句子中的位置和顺序都极为重要, 语序正确, 才能表达出正确含义 I do not like the story of the movie, but I do like the cast.I do like the story of the movie, but I do not like the cast. 如上述两个句子, 仅是改变not位置, 表达的意思截然相反 — Transformer抛弃了RNN和CNN作为序列学习的基本模型, 我们知道, 循环神经网络本身就是一种顺序结构, 天然包含了词在序列中的位置信息, 当抛弃循环神经网络结构, 完全采用Attention取而代之, 这些词序信息就会丢失, 模型就没有办法直到每个词在句子中的相对位置和绝对位置, 因此, 有必要把词序信息加到词向量上帮助模型学习这些信息, 位置编码(Positional Encoding)就是用来解决这个问题的办法 这里给出huggingface上的的一篇文章, 对于位置编码的讲解非常清晰透彻 设计位置编码https://huggingface.co/blog/zh/designing-positional-encoding 一、什么是位置编码 在transformer的encoder和decoder中使用了Positional Encoding, 最终的输入就变为了: input = word_embedding + positional_encoding word_embedding : 词嵌入, 将token的维度从vocab_size映射到d_model(原论文中, d_model为512) 最终的输入通过词嵌入后的向量和位置编码矩阵相加得到, 所以positional_encoding也是d_model维度的向量 二、位置编码构造方法2.1 用增长数值标记位置 第一个token标记1, 第二个token标记2… 以此类推 这种方法存在一些问题: 模型可能遇见比训练时所用序列更长的序列, 不利于模型泛化 模型无法理解数字的含义, 可能认为大数字比小数字权重更高, 而非顺序更靠后 无法表达相对位置信息 2.2 用[0, 1]范围标记位置为解决上述问题, 将数值限制在[0, 1]区间内, 对其等分切割, 假设有3个token, 则位置信息为[0, 0.5, 1] 这样产生的问题是, 序列长度不同, 相对距离也不同, 模型可能认为这是单词语义发生了变化 2.3 用二进制向量标记位置 由于位置信息最终会作用到word_embedding上, 比起用单一的数值, 更好的方式是使用和word_embedding同纬度的向量, 可以想到, 将数值转换成二进制形式 假设d_model = 3 d_model一般比较大(512, 1024…), 基本可以把每个token的位置都编码出来 但这样也存在问题, 不同单词间的位置变化不连续, 难以推测相对位置信息 2.4 用sin和cos函数交替标记位置 这是论文中提出的方法 三角函数有界且连续, 可以满足目前的需要 PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)参数说明: 符号 含义 pos 当前位置（position index） i 维度索引（dimension index） d_model 模型维度（embedding size，例如512） PE 位置编码矩阵（shape: [seq_len, d_model]） 这里只给出计算公式, 数学推导网上优秀文章较多, 不再演示 三. 为什么可以直接将词向量和位置编码进行相加 这是一直困扰我的点, 仅是进行张量相加就可以达到添加位置信息的作用吗? 难道这样不会破坏词向量本身信息吗? 网上浏览了一些博主的文章和视频, 但是答案也并没那么明确, 有点大力出奇迹的意思 3.1、公式复习Transformer 的输入是： X = E_{\\text{word}} + E_{\\text{pos}}其中： $E_{word}$：词向量（表示词语语义） $E_{pos}$：位置编码（表示词语在句子中的位置） 维度相同（例如都为 $d_{model}=512$） 3.2、为什么相加就能引入位置信息？1. 向量加法在嵌入空间中表示“组合信息”在向量空间中，加法表示叠加不同的语义因素。 比如： “king” ≈ “man” + “royalty” “Paris” - “France” + “Italy” ≈ “Rome” 同理： 把词向量和位置向量相加，就相当于在语义中叠加“第几个词”的信息。 也就是说，新的向量同时携带了词义和位置两种特征。 2. 相加不会“覆盖”信息，而是“偏移”语义空间我们可以把每个词向量看成在一个高维语义空间中的点。 加上位置编码后，等价于把它 沿着“位置维度方向”平移了一点： \\text{new\\_embedding = semantic\\_embedding + position\\_offset}所以模型看到的不是“词义被破坏”，而是“相同词在不同位置处于略微不同的方向”。 这使得 Transformer 能区分： “I love you”和“You love I” 的区别。 3. 注意力机制会自动学会利用这些位置差异在自注意力计算中，Q、K 向量都会从输入中线性变换得到： Q = XW_Q, \\quad K = XW_K因此，位置信息通过 $E_{pos}$ 影响了 Query-Key 相似度。 当两个词位置不同、位置编码不同，它们的注意力权重分布也不同 → 模型能够学习“前后关系”。 3.3、为什么不会破坏词向量原始信息？1. 相加是线性可逆的（在一定程度上）如果两个向量空间的分布相对独立，线性相加仍能被后续层（线性变换）分离： W(E_{word}+E_{pos})=WE_{word}+WE_{pos}Transformer 的多层线性结构可以在后续层中重新分辨“语义部分”和“位置信息部分”。 2. 位置编码的幅度较小，不会淹没词向量在实现时，位置编码通常被归一化或初始化为较小值（例如在 [-1,1] 范围内）。 这样词向量的主导语义仍然保留，只是轻微偏移 → 注入位置信息。 3. 高维空间中的“信息容量”巨大在 512 维或更高的空间中，向量叠加后不会导致严重的信息重叠。 就像 RGB 图像里混合红色和绿色还能区分出“黄色”一样， 词义与位置信息叠加后仍可被网络区分和提取。","tags":[null,null],"categories":[null]},{"title":"认识Transformer","path":"/wiki/note/knew-transformer.html","content":"Transformer：深度学习时代的分水岭过去十年，深度学习几乎重塑了整个人工智能领域。从图像识别到语音识别，再到自然语言处理（NLP），我们见证了一个又一个模型的进化。而在 NLP 领域，真正改变游戏规则的模型，毫无疑问是 Transformer。 这篇文章将带你从零理解 Transformer： 它为什么出现？解决了什么问题？内部结构如何运作？为什么它能成为 GPT、BERT 等大模型的基础？ 一、Transformer 的出现：时代的需要在 Transformer 出现之前，NLP 的主流模型是： RNN（循环神经网络） LSTM / GRU（改进版 RNN） CNN 文本卷积模型 虽然这些模型在当时取得了不错的效果，但也遭遇了多个根本性瓶颈： 1. 序列依赖太强，难以并行RNN 每一步都依赖前一步： h(t) - h(t+1) - h(t+2) ... 这意味着： 训练无法并行化 速度慢 对长序列不友好（梯度消失/爆炸问题） 2. 长距离依赖难以建模例如句子： “我昨天看了一部电影，我觉得它非常好看。” 这里“它”指代“电影”，人类很自然，但 RNN 需要跨越很多词才能关联，效果不稳定。 3. CNN 虽可并行，但感受野有限必须堆叠很多层卷积才能覆盖长距离上下文，效率仍然不高。 二、Transformer：新的范式从此诞生2017 年，Google 在论文 《Attention Is All You Need》 中提出： “序列建模根本不需要 RNN，也不需要 CNN，只需要 Attention（注意力）。” 这是一个革命性的观点。 Transformer 完全抛弃 RNN 的递归结构，转而只使用： → Self-Attention（自注意力机制）→ Position Encoding（位置编码）→ 多头注意力、多层堆叠（Multi-Head, Stacked Layers）最核心的思想是： “在序列中，每个词都能直接和其他所有词建立联系，并根据相关性动态分配权重。” 这彻底解决了 RNN 的所有历史问题。 三、Transformer 为什么如此强大？总结它的优势可以一句话概括： 更快、更准、能看全局。 下面逐点解释。 1. 训练可以完全并行化（突破性进步）自注意力机制让所有词之间的关系可以一次性计算： 所有词 ↔ 所有词 不再像 RNN 一步步传递。 这带来： 训练速度巨大提升 能利用 GPU 的矩阵计算优势 能训练更大的模型 2. 天然具备“全局视野”自注意力机制计算每个词对所有词的重要性，例如： Attention(它, 电影) → 高权重Attention(它, 昨天) → 低权重 模型能自动找到长距离依赖关系。 3. 表达能力更强Transformer 使用 多头注意力（Multi-Head Attention）： 一个头关注语义信息 一个头关注句法结构 一个头关注代词关系 …… 模型可以从多个角度“看”句子，让表示更加丰富。 4. 更容易扩展、堆叠和并行Transformer 模块化结构清晰，可以堆叠几十层甚至上百层。 这为后续的模型奠定了基础： BERT（2018） GPT 系列（2018 – 2024） ViT 图像 Transformer（2021） ChatGPT、Claude、Gemini 等顶级大模型 都源自 Transformer。 四、Transformer 内部结构概览 Transformer 包含 Encoder（编码器） 和 Decoder（解码器） 两个部分，但现代大多数模型只使用其中一部分： 模型 使用结构 BERT Encoder-only GPT Decoder-only T5 / FLAN Encoder-Decoder 原始架构中，一个 Transformer 层包含： 1. Multi-Head Self-Attention（多头自注意力）核心计算公式： Attention(Q, K, V) = softmax(QKᵀ / sqrt(d_k)) V 实现“词与词之间相关性”的计算。 2. Feed Forward Network（位置前馈网络）对每个词的表示做非线性变换，提升表达能力。 3. Add Norm（残差连接 + LayerNorm）为了解决深层网络训练困难问题。 4. Position Encoding（位置编码）因为 Transformer 不再使用 RNN/CNN，序列没有位置顺序，因此需要位置编码告诉模型： 谁是第一个词 谁是第二个词 谁更靠后 位置编码通常用正弦余弦函数构造。 五、Transformer 之后：新时代的大模型浪潮Transformer 诞生后，NLP 领域彻底变天。 2018：BERT 出现 彻底改变 NLP 预训练范式 情感分析、命名实体识别等任务效果暴涨 2018~2020：GPT 系列 GPT-1 → GPT-2 → GPT-3 语言生成能力大幅提升 LLM（大语言模型）时代开启 2021：Vision Transformer（ViT）Transformer 模型跨界视觉，效果强于 CNN 2022~2025：ChatGPT 时代Transformer 模型成为 AGI 的基础框架 世界开始感受 AI 的力量","tags":[null],"categories":[null]},{"title":"编码器","path":"/wiki/note/transformer-encoder.html","content":"","tags":[null,null],"categories":[null]},{"title":"解码器","path":"/wiki/note/transformer-decoder.html","content":"","tags":[null,null],"categories":[null]},{"title":"doccano数据标注平台","path":"/wiki/tutorial/doccano.html","content":"Doccano平台介绍 Doccano是documment anotation的缩写，是一个开源的文本标注工具，我们可以用它为NLP任务的语料库进行打标。它支持情感分析，命名实体识别，文本摘要等任务。 它的操作非常便捷，在小型语料库上，只要数小时就能完成全部的打标工作。下面介绍一下如何安装、配置和使用doccano。 一、doccano的安装 doccano的安装要在虚拟环境下进行！ 进入你用到的虚拟环境中, 安装doccano pip install doccano 然后，在终端里输入 # 初始化数据库doccano init# 创建一个super user。这里要把pass改成你需要的密码。当然，用户名也可以改成别的。doccano createuser --username admin --password 1234 二、启动doccano首先，在终端中运行下面的代码来启动WebServer # 启动webserverdoccano webserver --port 8000 然后，打开另一个终端，运行下面的代码启动—任务队列: # 启动任务队列doccano task 三、运行doccano与创建新的文本打标项目 首先，打开浏览器（最好是Chrome），在地址栏中输入http://127.0.0.1:8000/并回车。 然后，我们点击中间的蓝色按钮“快速开始”。此时，我们会跳转到登陆的界面。这里，我们需要用之前创建的超级用户登陆。 完成登陆后，我们会来到“项目”的界面。我们可以点击左上角的“创建”按钮来创建新的项目；也可以点击“删除”按钮来删除已经创建的项目。 我们点击左上角的“创建”按钮，创建一个新的项目。 对项目进行基本的说明，在填写完要求的信息后，点击创建，我们就创建了一个新的NLP标注项目。在创建完成后，会自动跳转到项目的主页。 进入数据标注平台 最左侧是一系列可以选择的页面。“主页”这个标签下面是doccano提供的一系列教程，其他的页面可以对项目进行设置。 稍后，我们将在上图所示的界面中的完成文本打标项目的各项设置。我们会依次点击左侧的各个标签，依次进行设置。 四、添加语料库 我们直接从“数据集”这个标签开始看。 在“数据集”这个页面，我们可以将准备好的文本添加到项目中，为将来的打标做准备。 我们首先点击左上角的“操作”→“导入数据集“。 此时，我们会来到”上传数据”的界面。 如上图所示，doccano总共支持4种格式的文本，他们的区别如下： Textfile：要求上传的文件为txt格式，并且在打标的时候，一整个txt文件在打标的时候显示为一页内容； Textline：要求上传的文件为txt格式，并且在打标的时候，该txt文件的一行文字会在打标的时候显示为一页内容； JSONL：是JSON Lines的简写，每行是一个有效的JSON值。 CoNLL：是“中文依存语料库”，是根据句子的依存结构而建立的树库。其中，依存结构描述的是句子中词与词之间直接的句法关系。具体介绍看汉语树库。 注意： doccano官方推荐的文档编码格式为UTF-8。 在使用JSONL格式的时候，文字数据本身要符合JSON格式的规范。 数据集中不要包含空行。 这里我们以Textline格式举例。点击“TextLine格式”。然后在跳转到的界面里，设置File Format和Encoding。然后点击下图中的“Drop files here…”来上传文件，最后，点击导入即可。 此时，再点击“数据集”的标签，我们就可以看到一条一条的文本已经被添加到项目中了。将来我们将对这些文本进行打标。 五、添加标签 在这一部分，我们讲解如何往项目中添加在打标时可选的标签。 在NER任务中，我们可能会添加People、Location、Company等；在文本分类任务中，我们可能会添加Positive、Negative等标签作为打标时的可选标签。 注意，这里只是添加将来可供选择的标签，是项目配置的过程，而不是进行文本标注。 点击左侧的“标签”按钮，就来到了添加标签的界面。 继续点击“操作”按钮，并在下拉菜单中点击“创建标签”按钮。 在弹出的“创建标签”窗口里面，在标签名一栏写上标签的名字，键值代表快捷键。例如在NER的例子中，可以写People、Location、Company等。例如，我们给People设置的快捷键是p。将来在打标的时候，右手用鼠标选中段落中的文字（例如“白居易”），左手在键盘按下快捷键p，就可以把被选中的文字打标成“People”。 再往下，我们可以给标签自定义颜色。 全部设置好以后，点击右下角的“保存”按钮。 此时，一个标签就添加完成了。我们以同样的方法添加其他所需要的标签。 六、添加成员 在为机器学习的语料库打标的时候，由于语料库一般比较大，如果让一个人给所有的文本打标的话，那到地老天荒都完不成。因此，我们需要多个人协同完成语料库的打标工作。 回忆一下，此时我们的项目还只有一个成员，也就是在初始配置doccano的时候创建的超级用户admin。因此，为了让其他人参与到打标项目中来，我们首先需要为其他成员创建账户。 我们打开网页http://127.0.0.1:8000/admin/，来到数据库的管理系统页面Django administration，并用超级用户的账号密码登陆该管理系统。 此时，我们再返回项目的设置页面。点击左侧的“成员”标签，点击页面上的“添加”按钮，会弹出“添加成员”窗口。 其中，在“用户搜索接口”的下拉菜单里面可以找到我们刚添加的用户“小明”。 注意，在这里只能找到已经创建到的用户，而不能创建新的用户。如果要新建用户，必须要到前面Django administration界面。 同时，我们还可以设置不同的成员的角色，不同的角色对应着不同的权限。如下图，我们把小明设置为“标注员”。其他角色还有项目管理员和审查员。 七、添加标注指南 我们可以事前给标注员和审查员准备一些标注指南，便于项目成员理解我们标注的要求和注意点。 例如，在判断文本正负面倾向的文本分类任务中，我们要具体说明判断正负面的标准，例如满足哪些要求，我们就可以认为一个本文是正面的。 因为一万个读者眼里有一万个哈姆雷特，不同人对文本的理解和判断正负面的尺度是不一样的。我们只有把标准写具体、写明确了，让人不用动脑筋都能做出符合我们要求的判断，我们才能得到一个尺度统一的数据集。数据集上的打标尺度统一，是机器学习获得好的效果的前提。 添加指南的界面如下图所示。 八、开始给文本打标 准备工作忙活了老半天，终于可以进入正题了——给文本打标。 需要注意的是，上面的前期设置里面并不是所有的都是必须的。在最精简的情况下，我们可以在仅添加了数据集与标签后，就开始给文本打标。 这里，我们用标注员小明的账号登陆打标系统做演示。同样是打开http://127.0.0.1:8000/地址，输入小明的账号密码登陆。 和之前不一样的是，由于小明的角色是“标注员”，因此他只有打标的权限，没有对项目进行各项设置的权限，所以在左侧列表没有管理员用户的各项设置项目。 这里我们直接点击左上角的“开始标注”进行打标。 以NER任务为例，在打标的界面下，我们选中句子中的实体，会自动弹出一个下拉菜单，我们可以从这个下拉菜单中选择相应的实体类型People，也可以直接在键盘上按下p键。 这是添加标签之后的样子。 九、导出打标结果 当我们要导出标注结果的时候，我们重新用管理员用户登陆，在“数据集”页面下，点击“操作”→“导出数据集”。 在弹出的窗口中，根据我们的需要进行设置后，点击Export，即可导出标注结果。 保存好的文本是字典的格式。 如下图所示，保存了句子的ID、句子原文、实体的在句子中的位置、实体的类型。","tags":[null,null,null],"categories":[null]},{"title":"知识学爆","path":"/wiki/tutorial/index.html","content":"知识回顾工具这么多，命令这么繁，一段时间不用就会忘记，这很正常，需要用时来翻翻。扳布"},{"title":"Neo4j图数据库","path":"/wiki/tutorial/neo4j.html","content":"","tags":[null,null],"categories":[null]},{"title":"Milvus向量数据库","path":"/wiki/tutorial/milvus.html","content":"官方文档肯定是学习Milvus最好的途径：https://milvus.io/docs/zh","tags":[null,null],"categories":[null]}]